# Thesis-PEFT: Parameter-Efficient Domain Adaptation for Entity Matching

This repository is part of my master's thesis on Entity Matching (EM). In my project, a parameter-efficient approach - Low-Rank Adaptation (LoRA) - is used to adapt pre-trained transformers for EM tasks. By updating only a small subset of parameters, this method achieves strong performance on unlabeled target datasets by aligning feature distributions between labeled source and unlabeled target datasets.

---

## Project Overview

- **ğŸ” Problem:** Entity Matching (EM) requires labeled training data, which is expensive to obtain. This project explores **domain adaptation** to transfer knowledge from labeled source datasets to unlabeled target datasets.
- **ğŸ’¡ Solution:** Uses **LoRA** (Low-Rank Adaptation) for efficient fine-tuning and **MK-MMD** (Multiple Kernel Maximum Mean Discrepancy) for domain alignment.
- **âœ¨ Key Features:**
  - âœ… Efficient fine-tuning with **LoRA** (only 1% additional parameters).
  - âœ… **Domain adaptation** to align source and target feature distributions.
  - âœ… **Task learning and domain adaptation trade-off**, controlled by a dynamic **Î±** function.

---

## ğŸ“‚ Project Structure
Thesis-PEFT/
â”‚â”€â”€ config/                # YAML config files for training & hyperparameter tuning
â”‚â”€â”€ dataset/               # Data processing and dataloaders
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ models/            # LoRA and Fine-Tuning models
â”‚   â”œâ”€â”€ divergences/       # Domain adaptation losses (MK-MMD, Gaussian Kernels)
â”‚   â”œâ”€â”€ training/          # Training scripts
â”‚   â”œâ”€â”€ evaluation/        # Evaluation metrics and scripts
â”‚â”€â”€ scripts/
â”‚   â”œâ”€â”€ train.sh           # Training script
â”‚   â”œâ”€â”€ run_experiments.sh # Batch execution for different datasets
â”‚â”€â”€ Dockerfile             # Docker setup for containerized execution
â”‚â”€â”€ requirements.txt       # Required Python dependencies
â”‚â”€â”€ setup.sh               # Setup script for dependencies (CUDA support included)
â”‚â”€â”€ README.md              # Project documentation
The script:
1. Loops through source and target dataset experimented in the paper, using 3 random seeds 
4. Use the `--hparam_tuning` flag to enable hyperparameter tuning.

---

## ğŸ—ï¸ Setup & Installation

### **1ï¸âƒ£ Using Docker (Recommended)**
This project is containerized using **Docker** for consistency and reproducibility.  
To set up and run the model inside a Docker container:

#### **ğŸ”¹ Build the Docker Image**
```bash
docker build -t thesis-peft .
```

#### **ğŸ”¹ Run Training Inside a Container**
```docker run --gpus all --rm -v $(pwd):/app thesis-peft bash train.sh --src <source_dataset> --tgt <target_dataset> --model lora```

#### **ğŸ”¹ Run Hyperparameter Tuning**

```docker run --gpus all --rm -v $(pwd):/app thesis-peft bash train.sh --src <source_dataset> --tgt <target_dataset> --model lora --hparam-tuning```

