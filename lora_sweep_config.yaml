program: train.py
project: lora
method: random
metric: 
    name: target_test/f1
    goal: maximize

# These are the hyparameters you selected with Matteo
# parameters:
#     learning_rate:
#         values: [1e-3, 1e-4, 1e-5]
#     weight_decay:
#         values: [0.1]
#     # out of memory :(
#     batch_size:
#         values: [16]

#     lora_alpha:
#         values: [8, 32]
#     lora_r: 
#         values: [8, 32]
#     lora_dropout:
#         values: [0.1, 0.3]

# boh prova con ancora piu parametri
parameters:
    learning_rate:
        values: [1e-3]
    weight_decay:
        values: [0.01]
    # out of memory :(
    batch_size:
        values: [16]

    lora_alpha:
        values: [4, 64]
    lora_r: 
        values: [4, 64]
    lora_dropout:
        values: [0.01, 0.1]



# Try with the hparams on Sebastian's project
# parameters:
#     learning_rate:
#         values: [1e-3, 1e-4, 1e-5]
#     lora_alpha:
#         values: [8]
#     lora_r: 
#         values: [8]
#     lora_dropout:
#         values: [0.3, 0.5]
#     weight_decay:
#         values: [0.01]
#     # out of memory :(
#     batch_size:
#         values: [16]


#     weight_decay:
#         values: [0.001, 0.01, 0.1]


