=== Argument Setting ===
src: computers
tgt: watches
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 115
tp 113
fp 195
recall 0.9826086871077506
precision 0.36688311569193793
f1 0.5342785613737981
tgt_res:
===== RES ====
p 461
tp 430
fp 793
recall 0.9327548786708137
precision 0.35159443961439535
f1 0.5106884378737416
save pretrained model to: checkpoint/computers/bert/3000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/computers/bert/3000/source-classifier.ptwatchesbest
===== RES ====
p 116
tp 103
fp 53
recall 0.8879310268281808
precision 0.6602564060239974
f1 0.7573524464211637
tgt_res:
===== RES ====
p 462
tp 398
fp 252
recall 0.8614718596072037
precision 0.6123076913656805
f1 0.7158268511338114
save pretrained model to: checkpoint/computers/bert/3000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/computers/bert/3000/source-classifier.ptwatchesbest
===== RES ====
p 115
tp 106
fp 56
recall 0.9217391224196598
precision 0.6543209836153026
f1 0.7653424691579929
tgt_res:
===== RES ====
p 462
tp 412
fp 264
recall 0.8917748898446431
precision 0.6094674547197227
f1 0.7240768450557789
save pretrained model to: checkpoint/computers/bert/3000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/computers/bert/3000/source-classifier.ptwatchesbest
===== RES ====
p 116
tp 110
fp 98
recall 0.9482758538941737
precision 0.5288461513036243
f1 0.6790118818018654
===== RES ====
p 115
tp 107
fp 80
recall 0.9304347745179585
precision 0.572192510309131
f1 0.7086087952505219
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/3000/source-encoder.ptwatchesbest
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/3000/source-classifier.ptwatchesbest
Pretraining time:  86.79248833656311
=== Training F' and A ===
===== RES ====
p 115
tp 76
fp 20
recall 0.6608695594706995
precision 0.791666658420139
f1 0.7203786441458008
======== tgt result =======
===== RES ====
p 461
tp 308
fp 87
recall 0.6681127968153735
precision 0.7797468334689953
f1 0.719625669515694
===== RES ====
p 116
tp 85
fp 24
recall 0.7327586143727706
precision 0.779816506607188
f1 0.7555550493237873
======== tgt result =======
===== RES ====
p 462
tp 335
fp 104
recall 0.7251082235387268
precision 0.7630979481478407
f1 0.743617700673284
===== RES ====
p 114
tp 87
fp 28
recall 0.7631578880424746
precision 0.756521732551985
f1 0.7598248208847517
======== tgt result =======
===== RES ====
p 462
tp 346
fp 116
recall 0.7489177472967148
precision 0.7489177472967148
f1 0.7489172472970487
===== RES ====
p 116
tp 81
fp 21
recall 0.698275856049346
precision 0.7941176392733565
f1 0.7431187612998875
===== RES ====
p 115
tp 87
fp 27
recall 0.756521732551985
precision 0.7631578880424746
f1 0.7598248208847517
===== RES ====
p 115
tp 78
fp 22
recall 0.6782608636672969
precision 0.7799999922
f1 0.7255808910333321
===== RES ====
p 114
tp 83
fp 25
recall 0.728070169052016
precision 0.7685185114026064
f1 0.7477472413768456
===== RES ====
p 115
tp 84
fp 25
recall 0.730434776257089
precision 0.7706421947647505
f1 0.7499994936626416
===== RES ====
p 114
tp 85
fp 27
recall 0.7456140285472453
precision 0.7589285646524235
f1 0.7522118827632739
===== RES ====
p 113
tp 77
fp 20
recall 0.6814159231733105
precision 0.7938144248060369
f1 0.7333328292520378
===== RES ====
p 116
tp 90
fp 28
recall 0.7758620622770512
precision 0.7627118579431199
f1 0.769230262692998
======== tgt result =======
===== RES ====
p 461
tp 349
fp 136
recall 0.7570498898979395
precision 0.7195876273822935
f1 0.7378430505592736
===== RES ====
p 116
tp 84
fp 24
recall 0.7241379247919144
precision 0.7777777705761317
f1 0.7499994939416591
===== RES ====
p 114
tp 84
fp 24
recall 0.7368420987996307
precision 0.7777777705761317
f1 0.7567562503046883
===== RES ====
p 115
tp 86
fp 26
recall 0.7478260804536863
precision 0.7678571360012756
f1 0.7577087445131285
===== RES ====
p 115
tp 86
fp 25
recall 0.7478260804536863
precision 0.7747747677948219
f1 0.7610614403245511
===== RES ====
p 115
tp 87
fp 27
recall 0.756521732551985
precision 0.7631578880424746
f1 0.7598248208847517
===== RES ====
p 116
tp 78
fp 21
recall 0.6724137873067777
precision 0.7878787799204164
f1 0.7255808917255973
===== RES ====
p 113
tp 85
fp 28
recall 0.7522123827237842
precision 0.7522123827237842
f1 0.7522118827241167
===== RES ====
p 114
tp 76
fp 21
recall 0.6666666608187135
precision 0.7835051465618026
f1 0.7203786433371948
===== RES ====
p 116
tp 73
fp 20
recall 0.6293103394024971
precision 0.7849462281188578
f1 0.6985640926722203
===== RES ====
p 115
tp 86
fp 25
recall 0.7478260804536863
precision 0.7747747677948219
f1 0.7610614403245511
===== RES ====
p 116
tp 86
fp 28
recall 0.7413793039536267
precision 0.75438595829486
f1 0.7478255804918277
===== RES ====
p 115
tp 83
fp 22
recall 0.7217391241587903
precision 0.7904761829478459
f1 0.7545449487193383
===== RES ====
p 114
tp 86
fp 25
recall 0.75438595829486
precision 0.7747747677948219
f1 0.7644439377385986
===== RES ====
p 116
tp 86
fp 24
recall 0.7413793039536267
precision 0.7818181747107439
f1 0.7610614405203376
===== RES ====
p 115
tp 84
fp 24
recall 0.730434776257089
precision 0.7777777705761317
f1 0.7533627224359323
===== RES ====
p 115
tp 84
fp 22
recall 0.730434776257089
precision 0.7924528227127092
f1 0.7601804894251963
===== RES ====
p 115
tp 84
fp 22
recall 0.730434776257089
precision 0.7924528227127092
f1 0.7601804894251963
===== RES ====
p 116
tp 82
fp 23
recall 0.7068965456302022
precision 0.7809523735147393
f1 0.7420809424871804
===== RES ====
p 116
tp 85
fp 22
recall 0.7327586143727706
precision 0.7943925159402568
f1 0.7623313325427066
===== RES ====
p 115
tp 85
fp 26
recall 0.7391304283553876
precision 0.7657657588669752
f1 0.7522118828807457
===== RES ====
p 116
tp 83
fp 21
recall 0.7155172352110584
precision 0.7980769154031067
f1 0.7545449491738833
===== RES ====
p 116
tp 86
fp 27
recall 0.7413793039536267
precision 0.7610619401675934
f1 0.7510911965831593
===== RES ====
p 114
tp 82
fp 23
recall 0.7192982393044014
precision 0.7809523735147393
f1 0.7488579414944636
===== RES ====
p 114
tp 82
fp 21
recall 0.7192982393044014
precision 0.7961164971250826
f1 0.7557598629831915
===== RES ====
p 116
tp 85
fp 24
recall 0.7327586143727706
precision 0.779816506607188
f1 0.7555550493237873
===== RES ====
p 115
tp 82
fp 23
recall 0.7130434720604916
precision 0.7809523735147393
f1 0.7454540397110779
===== RES ====
p 115
tp 85
fp 22
recall 0.7391304283553876
precision 0.7943925159402568
f1 0.7657652595165987
===== RES ====
p 116
tp 85
fp 21
recall 0.7327586143727706
precision 0.8018867848878606
f1 0.7657652598818284
===== RES ====
p 114
tp 84
fp 23
recall 0.7368420987996307
precision 0.7850467216350774
f1 0.7601804890976032
===== RES ====
p 300
tp 252
fp 92
recall 0.8399999972000001
precision 0.7325581374053542
f1 0.7826081955560436
=== Result of InvGAN+KD: ===
0.7826081955560436
The source-target datasets are: computers_watches with seed 3000
The F1 score is: 0.7826081955560436
The training time is: 588.0855846405029
The inference time is: 0.0002667158842086792
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: watches
tgt: computers
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 140
tp 107
fp 34
recall 0.7642857088265307
precision 0.7588652428449274
f1 0.7615653308851956
tgt_res:
===== RES ====
p 572
tp 444
fp 135
recall 0.7762237748667417
precision 0.7668393769139217
f1 0.7715025395122962
save pretrained model to: checkpoint/watches/bert/3000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/watches/bert/3000/source-classifier.ptcomputersbest
===== RES ====
p 137
tp 81
fp 6
recall 0.591240871596782
precision 0.931034472057075
f1 0.7232138041696357
===== RES ====
p 136
tp 105
fp 40
recall 0.7720588178525087
precision 0.7241379260404281
f1 0.7473304560482564
===== RES ====
p 141
tp 119
fp 75
recall 0.8439716252200594
precision 0.6134020586938038
f1 0.7104472694679218
===== RES ====
p 136
tp 112
fp 49
recall 0.8235294057093426
precision 0.6956521695922225
f1 0.7542082526729565
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/3000/source-encoder.ptcomputersbest
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/3000/source-classifier.ptcomputersbest
Pretraining time:  73.41442584991455
=== Training F' and A ===
===== RES ====
p 142
tp 105
fp 17
recall 0.7394366145110097
precision 0.8606557306503628
f1 0.7954540422982906
======== tgt result =======
===== RES ====
p 572
tp 419
fp 73
recall 0.7325174812368576
precision 0.8516260145292154
f1 0.7875934863088955
===== RES ====
p 138
tp 105
fp 21
recall 0.7608695597038437
precision 0.8333333267195768
f1 0.7954540404617456
===== RES ====
p 133
tp 97
fp 15
recall 0.7293233027870428
precision 0.8660714208386481
f1 0.7918362319036847
===== RES ====
p 139
tp 102
fp 21
recall 0.7338129443610579
precision 0.8292682859409083
f1 0.7786254501197756
===== RES ====
p 140
tp 106
fp 29
recall 0.757142851734694
precision 0.7851851793689987
f1 0.7709085854680928
===== RES ====
p 138
tp 99
fp 15
recall 0.7173912991493384
precision 0.8684210450138505
f1 0.785713784013918
===== RES ====
p 138
tp 110
fp 45
recall 0.7971014434992649
precision 0.7096774147762748
f1 0.7508527388790598
===== RES ====
p 138
tp 99
fp 20
recall 0.7173912991493384
precision 0.8319327661182121
f1 0.7704275123017942
===== RES ====
p 141
tp 103
fp 17
recall 0.7304964487198834
precision 0.8583333261805556
f1 0.7892715278404795
===== RES ====
p 137
tp 104
fp 28
recall 0.7591240820501892
precision 0.7878787819100093
f1 0.7732336951676095
===== RES ====
p 141
tp 105
fp 23
recall 0.7446808457824053
precision 0.8203124935913086
f1 0.780668640345256
===== RES ====
p 139
tp 104
fp 23
recall 0.7482014334661767
precision 0.8188976313472628
f1 0.7819543823565851
===== RES ====
p 138
tp 105
fp 26
recall 0.7608695597038437
precision 0.8015267114387274
f1 0.7806686395160811
===== RES ====
p 139
tp 105
fp 24
recall 0.7553956780187362
precision 0.8139534820623762
f1 0.7835815844010757
===== RES ====
p 140
tp 106
fp 24
recall 0.757142851734694
precision 0.815384609112426
f1 0.7851846800551874
===== RES ====
p 140
tp 103
fp 18
recall 0.7357142804591837
precision 0.8512396623864491
f1 0.7892715272532896
===== RES ====
p 136
tp 100
fp 18
recall 0.7352941122404845
precision 0.8474576199367998
f1 0.7874010711144567
===== RES ====
p 136
tp 98
fp 21
recall 0.7205882299956748
precision 0.8235294048442907
f1 0.7686269471744863
===== RES ====
p 138
tp 104
fp 23
recall 0.7536231829447596
precision 0.8188976313472628
f1 0.7849051553153762
===== RES ====
p 138
tp 99
fp 19
recall 0.7173912991493384
precision 0.8389830437374318
f1 0.7734369970095967
===== RES ====
p 139
tp 102
fp 22
recall 0.7338129443610579
precision 0.8225806385275755
f1 0.7756648949677191
===== RES ====
p 137
tp 105
fp 23
recall 0.7664233520699025
precision 0.8203124935913086
f1 0.7924523247849357
===== RES ====
p 138
tp 105
fp 22
recall 0.7608695597038437
precision 0.8267716470332941
f1 0.7924523250697342
===== RES ====
p 141
tp 105
fp 28
recall 0.7446808457824053
precision 0.789473678274634
f1 0.7664228524964629
===== RES ====
p 138
tp 99
fp 26
recall 0.7173912991493384
precision 0.7919999936640001
f1 0.7528512065234839
===== RES ====
p 139
tp 104
fp 22
recall 0.7482014334661767
precision 0.825396818846057
f1 0.7849051556571344
===== RES ====
p 142
tp 106
fp 26
recall 0.7464788679825431
precision 0.8030302969467402
f1 0.77372212275593
===== RES ====
p 139
tp 102
fp 21
recall 0.7338129443610579
precision 0.8292682859409083
f1 0.7786254501197756
===== RES ====
p 138
tp 103
fp 24
recall 0.7463768061856754
precision 0.8110236156612314
f1 0.7773579855610183
===== RES ====
p 136
tp 98
fp 18
recall 0.7205882299956748
precision 0.8448275789239001
f1 0.7777772747546636
===== RES ====
p 142
tp 106
fp 23
recall 0.7464788679825431
precision 0.8217054199867797
f1 0.7822873182557886
===== RES ====
p 141
tp 101
fp 20
recall 0.7163120516573613
precision 0.8347107369032171
f1 0.7709918634406796
===== RES ====
p 133
tp 94
fp 18
recall 0.7067669119791962
precision 0.8392857067920919
f1 0.7673464361852441
===== RES ====
p 143
tp 106
fp 23
recall 0.7412587360751137
precision 0.8217054199867797
f1 0.7794112602998434
===== RES ====
p 138
tp 102
fp 19
recall 0.7391304294265911
precision 0.842975199644833
f1 0.7876442837170154
===== RES ====
p 139
tp 105
fp 24
recall 0.7553956780187362
precision 0.8139534820623762
f1 0.7835815844010757
===== RES ====
p 134
tp 97
fp 24
recall 0.7238805916128315
precision 0.801652885936753
f1 0.7607838090583816
===== RES ====
p 142
tp 102
fp 18
recall 0.7183098540964095
precision 0.8499999929166667
f1 0.7786254517805159
===== RES ====
p 137
tp 101
fp 22
recall 0.7372262719910492
precision 0.8211382047061935
f1 0.7769225723967698
===== RES ====
p 140
tp 101
fp 20
recall 0.7214285662755102
precision 0.8347107369032171
f1 0.7739458568726497
===== RES ====
p 300
tp 243
fp 93
recall 0.8099999973
precision 0.7232142835618622
f1 0.7641504425955548
=== Result of InvGAN+KD: ===
0.7641504425955548
The source-target datasets are: watches_computers with seed 3000
The F1 score is: 0.7641504425955548
The training time is: 726.7016820907593
The inference time is: 0.0002690926194190979
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: cameras
tgt: watches
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 116
tp 93
fp 64
recall 0.8017241310196196
precision 0.5923566841251167
f1 0.6813181876051754
tgt_res:
===== RES ====
p 462
tp 353
fp 276
recall 0.7640692624154345
precision 0.561208266198397
f1 0.6471122511343443
save pretrained model to: checkpoint/cameras/bert/3000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/cameras/bert/3000/source-classifier.ptwatchesbest
===== RES ====
p 116
tp 80
fp 28
recall 0.68965516646849
precision 0.7407407338820302
f1 0.7142852085462675
tgt_res:
===== RES ====
p 462
tp 298
fp 124
recall 0.6450216436254943
precision 0.7061611357673907
f1 0.6742076442951217
save pretrained model to: checkpoint/cameras/bert/3000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/cameras/bert/3000/source-classifier.ptwatchesbest
===== RES ====
p 115
tp 100
fp 79
recall 0.8695652098298677
precision 0.5586592147560938
f1 0.6802716279100005
===== RES ====
p 114
tp 93
fp 55
recall 0.8157894665281626
precision 0.6283783741325786
f1 0.7099231671234684
===== RES ====
p 115
tp 92
fp 48
recall 0.7999999930434784
precision 0.6571428524489796
f1 0.7215681265978023
tgt_res:
===== RES ====
p 462
tp 363
fp 183
recall 0.7857142840136054
precision 0.664835163617518
f1 0.7202375972816161
save pretrained model to: checkpoint/cameras/bert/3000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/cameras/bert/3000/source-classifier.ptwatchesbest
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/3000/source-encoder.ptwatchesbest
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/3000/source-classifier.ptwatchesbest
Pretraining time:  67.53611040115356
=== Training F' and A ===
===== RES ====
p 115
tp 90
fp 36
recall 0.782608688846881
precision 0.7142857086167801
f1 0.746887461648721
======== tgt result =======
===== RES ====
p 460
tp 340
fp 140
recall 0.7391304331758034
precision 0.7083333318576389
f1 0.723403754006683
===== RES ====
p 116
tp 89
fp 31
recall 0.7672413726961951
precision 0.7416666604861112
f1 0.75423678188772
======== tgt result =======
===== RES ====
p 461
tp 336
fp 123
recall 0.7288503237985893
precision 0.7320261421960215
f1 0.730434281023499
===== RES ====
p 115
tp 89
fp 28
recall 0.7739130367485824
precision 0.7606837541821901
f1 0.767240872733679
======== tgt result =======
===== RES ====
p 462
tp 336
fp 131
recall 0.7272727256985438
precision 0.7194860798297943
f1 0.7233579484037238
===== RES ====
p 114
tp 85
fp 25
recall 0.7456140285472453
precision 0.7727272657024794
f1 0.7589280648121915
===== RES ====
p 116
tp 89
fp 27
recall 0.7672413726961951
precision 0.7672413726961951
f1 0.767240872696521
===== RES ====
p 116
tp 83
fp 21
recall 0.7155172352110584
precision 0.7980769154031067
f1 0.7545449491738833
===== RES ====
p 116
tp 87
fp 27
recall 0.7499999935344829
precision 0.7631578880424746
f1 0.7565212325901226
===== RES ====
p 116
tp 90
fp 23
recall 0.7758620622770512
precision 0.7964601699428304
f1 0.7860256940946336
======== tgt result =======
===== RES ====
p 460
tp 336
fp 126
recall 0.730434781020794
precision 0.7272727256985438
f1 0.728849823801285
===== RES ====
p 116
tp 84
fp 21
recall 0.7241379247919144
precision 0.7999999923809524
f1 0.7601804898346878
===== RES ====
p 114
tp 90
fp 27
recall 0.7894736772853186
precision 0.7692307626561473
f1 0.7792202725589309
===== RES ====
p 116
tp 84
fp 25
recall 0.7241379247919144
precision 0.7706421947647505
f1 0.7466661605139143
===== RES ====
p 116
tp 90
fp 26
recall 0.7758620622770512
precision 0.7758620622770512
f1 0.7758615622773735
===== RES ====
p 115
tp 87
fp 25
recall 0.756521732551985
precision 0.7767857073501276
f1 0.7665193171227233
===== RES ====
p 114
tp 88
fp 28
recall 0.7719298177900893
precision 0.7586206831153389
f1 0.7652168846884174
===== RES ====
p 116
tp 88
fp 27
recall 0.7586206831153389
precision 0.7652173846502837
f1 0.7619042553178823
===== RES ====
p 116
tp 89
fp 26
recall 0.7672413726961951
precision 0.7739130367485824
f1 0.7705622639009262
===== RES ====
p 116
tp 89
fp 30
recall 0.7672413726961951
precision 0.7478991533789987
f1 0.7574463021460973
===== RES ====
p 116
tp 86
fp 23
recall 0.7413793039536267
precision 0.7889908184496255
f1 0.7644439381336599
===== RES ====
p 115
tp 90
fp 31
recall 0.782608688846881
precision 0.7438016467454409
f1 0.7627113582666303
===== RES ====
p 115
tp 86
fp 29
recall 0.7478260804536863
precision 0.7478260804536863
f1 0.7478255804540206
===== RES ====
p 115
tp 88
fp 26
recall 0.7652173846502837
precision 0.7719298177900893
f1 0.7685584452626202
===== RES ====
p 115
tp 89
fp 27
recall 0.7739130367485824
precision 0.7672413726961951
f1 0.7705622639009262
===== RES ====
p 115
tp 88
fp 25
recall 0.7652173846502837
precision 0.7787610550552119
f1 0.7719293178288865
===== RES ====
p 116
tp 90
fp 29
recall 0.7758620622770512
precision 0.7563025146529201
f1 0.7659569403715353
===== RES ====
p 115
tp 86
fp 25
recall 0.7478260804536863
precision 0.7747747677948219
f1 0.7610614403245511
===== RES ====
p 116
tp 91
fp 35
recall 0.7844827518579073
precision 0.7222222164902998
f1 0.7520656103411552
===== RES ====
p 116
tp 85
fp 26
recall 0.7327586143727706
precision 0.7657657588669752
f1 0.7488981720587862
===== RES ====
p 116
tp 89
fp 30
recall 0.7672413726961951
precision 0.7478991533789987
f1 0.7574463021460973
===== RES ====
p 115
tp 91
fp 34
recall 0.7913043409451797
precision 0.727999994176
f1 0.7583328278822731
===== RES ====
p 116
tp 90
fp 26
recall 0.7758620622770512
precision 0.7758620622770512
f1 0.7758615622773735
===== RES ====
p 114
tp 90
fp 29
recall 0.7894736772853186
precision 0.7563025146529201
f1 0.7725316824405969
===== RES ====
p 116
tp 90
fp 28
recall 0.7758620622770512
precision 0.7627118579431199
f1 0.769230262692998
===== RES ====
p 115
tp 79
fp 16
recall 0.6869565157655956
precision 0.8315789386149586
f1 0.7523804497508934
===== RES ====
p 115
tp 88
fp 29
recall 0.7652173846502837
precision 0.752136745708233
f1 0.7586201831528268
===== RES ====
p 116
tp 87
fp 29
recall 0.7499999935344829
precision 0.7499999935344829
f1 0.7499994935348163
===== RES ====
p 114
tp 84
fp 27
recall 0.7368420987996307
precision 0.7567567499391284
f1 0.7466661601188533
===== RES ====
p 115
tp 88
fp 27
recall 0.7652173846502837
precision 0.7652173846502837
f1 0.7652168846506104
===== RES ====
p 116
tp 90
fp 26
recall 0.7758620622770512
precision 0.7758620622770512
f1 0.7758615622773735
===== RES ====
p 116
tp 90
fp 30
recall 0.7758620622770512
precision 0.74999999375
f1 0.7627113580870845
===== RES ====
p 116
tp 84
fp 22
recall 0.7241379247919144
precision 0.7924528227127092
f1 0.7567562509539855
===== RES ====
p 300
tp 264
fp 113
recall 0.8799999970666667
precision 0.7002652501319224
f1 0.779910877871885
=== Result of InvGAN+KD: ===
0.779910877871885
The source-target datasets are: cameras_watches with seed 3000
The F1 score is: 0.779910877871885
The training time is: 586.7104935646057
The inference time is: 0.0002666860818862915
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: watches
tgt: cameras
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 93
tp 62
fp 9
recall 0.666666659498208
precision 0.8732394243205716
f1 0.7560970607528471
tgt_res:
===== RES ====
p 388
tp 262
fp 60
recall 0.675257730218408
precision 0.8136645937463832
f1 0.7380276712560363
save pretrained model to: checkpoint/watches/bert/3000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/watches/bert/3000/source-classifier.ptcamerasbest
===== RES ====
p 85
tp 67
fp 36
recall 0.7882352848442908
precision 0.6504854305778114
f1 0.7127654544480583
===== RES ====
p 92
tp 70
fp 22
recall 0.76086955694707
precision 0.76086955694707
f1 0.7608690569473986
tgt_res:
===== RES ====
p 389
tp 266
fp 103
recall 0.6838046254915048
precision 0.720867206718517
f1 0.7018464641958152
save pretrained model to: checkpoint/watches/bert/3000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/watches/bert/3000/source-classifier.ptcamerasbest
===== RES ====
p 92
tp 75
fp 29
recall 0.8152173824432893
precision 0.7211538392196747
f1 0.765305616514277
tgt_res:
===== RES ====
p 387
tp 286
fp 156
recall 0.7390180859456897
precision 0.6470588220654778
f1 0.6899874378103851
save pretrained model to: checkpoint/watches/bert/3000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/watches/bert/3000/source-classifier.ptcamerasbest
===== RES ====
p 91
tp 73
fp 33
recall 0.8021977933824419
precision 0.688679238786045
f1 0.7411162466441533
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/3000/source-encoder.ptcamerasbest
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/3000/source-classifier.ptcamerasbest
Pretraining time:  76.27655982971191
=== Training F' and A ===
===== RES ====
p 91
tp 65
fp 20
recall 0.7142857064364209
precision 0.7647058733564015
f1 0.7386358558242015
======== tgt result =======
===== RES ====
p 389
tp 257
fp 65
recall 0.6606683787643487
precision 0.7981366434840477
f1 0.7229249595094402
===== RES ====
p 92
tp 70
fp 18
recall 0.76086955694707
precision 0.7954545364152894
f1 0.7777772693830374
======== tgt result =======
===== RES ====
p 388
tp 263
fp 80
recall 0.6778350497993942
precision 0.7667638461610383
f1 0.7195617434284767
===== RES ====
p 89
tp 64
fp 15
recall 0.7191011155157179
precision 0.8101265720237143
f1 0.761904254606335
===== RES ====
p 90
tp 64
fp 20
recall 0.7111111032098767
precision 0.7619047528344672
f1 0.7356316760473731
===== RES ====
p 90
tp 68
fp 16
recall 0.7555555471604939
precision 0.8095237998866215
f1 0.7816086870131347
======== tgt result =======
===== RES ====
p 389
tp 272
fp 92
recall 0.6992287899762756
precision 0.7472527451998551
f1 0.7224430577295876
===== RES ====
p 88
tp 63
fp 23
recall 0.7159090827737604
precision 0.7325581310167659
f1 0.7241374227774624
===== RES ====
p 88
tp 64
fp 18
recall 0.7272727190082646
precision 0.780487795359905
f1 0.7529406682356256
===== RES ====
p 91
tp 65
fp 13
recall 0.7142857064364209
precision 0.8333333226495728
f1 0.7692302630863477
===== RES ====
p 90
tp 73
fp 42
recall 0.8111111020987656
precision 0.6347826031758035
f1 0.712194622439365
===== RES ====
p 93
tp 67
fp 17
recall 0.7204300997803216
precision 0.7976190381235829
f1 0.7570616396313414
===== RES ====
p 87
tp 63
fp 14
recall 0.7241379227110584
precision 0.8181818075560805
f1 0.768292175416742
===== RES ====
p 90
tp 63
fp 16
recall 0.6999999922222223
precision 0.7974683443358438
f1 0.7455616234729
===== RES ====
p 89
tp 66
fp 18
recall 0.741573025375584
precision 0.7857142763605444
f1 0.7630052719439271
===== RES ====
p 86
tp 56
fp 12
recall 0.6511627831260142
precision 0.8235293996539794
f1 0.7272722246587928
===== RES ====
p 90
tp 67
fp 15
recall 0.7444444361728396
precision 0.8170731607674006
f1 0.7790692594648951
===== RES ====
p 89
tp 66
fp 24
recall 0.741573025375584
precision 0.7333333251851853
f1 0.7374296593742663
===== RES ====
p 92
tp 67
fp 20
recall 0.7282608616493385
precision 0.7701149336768399
f1 0.7486028439814827
===== RES ====
p 93
tp 70
fp 30
recall 0.7526881639495897
precision 0.699999993
f1 0.7253880941773687
===== RES ====
p 90
tp 63
fp 24
recall 0.6999999922222223
precision 0.7241379227110584
f1 0.7118638988799835
===== RES ====
p 88
tp 64
fp 26
recall 0.7272727190082646
precision 0.7111111032098767
f1 0.7191006155791888
===== RES ====
p 89
tp 65
fp 19
recall 0.7303370704456509
precision 0.7738095145975058
f1 0.7514445784359632
===== RES ====
p 94
tp 69
fp 19
recall 0.7340425453825261
precision 0.7840909001807852
f1 0.7582412504531729
===== RES ====
p 90
tp 66
fp 23
recall 0.7333333251851853
precision 0.741573025375584
f1 0.7374296593742663
===== RES ====
p 93
tp 68
fp 21
recall 0.7311827878367443
precision 0.7640449352354501
f1 0.7472522392830298
===== RES ====
p 89
tp 63
fp 18
recall 0.7078651605857847
precision 0.7777777681755831
f1 0.7411759629761144
===== RES ====
p 92
tp 67
fp 22
recall 0.7282608616493385
precision 0.7528089803055171
f1 0.7403309836699449
===== RES ====
p 89
tp 65
fp 19
recall 0.7303370704456509
precision 0.7738095145975058
f1 0.7514445784359632
===== RES ====
p 91
tp 66
fp 20
recall 0.7252747173046734
precision 0.7674418515413738
f1 0.7457622038370356
===== RES ====
p 92
tp 67
fp 15
recall 0.7282608616493385
precision 0.8170731607674006
f1 0.7701144353286354
===== RES ====
p 89
tp 64
fp 19
recall 0.7191011155157179
precision 0.771084328059225
f1 0.744185538467074
===== RES ====
p 87
tp 61
fp 20
recall 0.7011494172281676
precision 0.7530864104557233
f1 0.7261899681834502
===== RES ====
p 92
tp 65
fp 18
recall 0.7065217314508507
precision 0.7831325206851504
f1 0.7428566356901307
===== RES ====
p 86
tp 63
fp 25
recall 0.7325581310167659
precision 0.7159090827737604
f1 0.7241374227774624
===== RES ====
p 90
tp 60
fp 18
recall 0.6666666592592594
precision 0.7692307593688364
f1 0.7142852083336798
===== RES ====
p 91
tp 64
fp 22
recall 0.7032966955681682
precision 0.744186037858302
f1 0.7231633340359034
===== RES ====
p 90
tp 69
fp 21
recall 0.7666666581481483
precision 0.7666666581481483
f1 0.7666661581484745
===== RES ====
p 87
tp 62
fp 17
recall 0.712643669969613
precision 0.7848101166479732
f1 0.7469874439689789
===== RES ====
p 87
tp 63
fp 17
recall 0.7241379227110584
precision 0.7874999901562502
f1 0.7544905098070642
===== RES ====
p 91
tp 66
fp 19
recall 0.7252747173046734
precision 0.7764705791003461
f1 0.7499994920587005
===== RES ====
p 88
tp 69
fp 29
recall 0.7840909001807852
precision 0.7040816254685549
f1 0.7419349773387558
===== RES ====
p 300
tp 265
fp 93
recall 0.883333330388889
precision 0.7402234616194875
f1 0.8054706260569754
=== Result of InvGAN+KD: ===
0.8054706260569754
The source-target datasets are: watches_cameras with seed 3000
The F1 score is: 0.8054706260569754
The training time is: 491.5763416290283
The inference time is: 0.000266149640083313
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: shoes
tgt: watches
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 114
tp 82
fp 35
recall 0.7192982393044014
precision 0.7008546948644898
f1 0.7099562038945819
tgt_res:
===== RES ====
p 461
tp 320
fp 139
recall 0.694143165522466
precision 0.6971677544724014
f1 0.6956516724034785
save pretrained model to: checkpoint/shoes/bert/3000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/shoes/bert/3000/source-classifier.ptwatchesbest
===== RES ====
p 115
tp 88
fp 36
recall 0.7652173846502837
precision 0.7096774136316337
f1 0.73640116818717
tgt_res:
===== RES ====
p 460
tp 335
fp 165
recall 0.7282608679820416
precision 0.66999999866
f1 0.6979161660810861
save pretrained model to: checkpoint/shoes/bert/3000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/shoes/bert/3000/source-classifier.ptwatchesbest
===== RES ====
p 115
tp 71
fp 22
recall 0.6173912989792061
precision 0.7634408520060124
f1 0.6826918067218818
===== RES ====
p 115
tp 83
fp 32
recall 0.7217391241587903
precision 0.7217391241587903
f1 0.7217386241591367
===== RES ====
p 115
tp 65
fp 16
recall 0.565217386389414
precision 0.802469125895443
f1 0.6632648144006043
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/3000/source-encoder.ptwatchesbest
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/3000/source-classifier.ptwatchesbest
Pretraining time:  70.98871159553528
=== Training F' and A ===
===== RES ====
p 115
tp 87
fp 44
recall 0.756521732551985
precision 0.6641221323349455
f1 0.70731656953568
======== tgt result =======
===== RES ====
p 463
tp 328
fp 166
recall 0.7084233246038373
precision 0.6639676099919684
f1 0.685474943188597
===== RES ====
p 114
tp 85
fp 38
recall 0.7456140285472453
precision 0.6910569049507569
f1 0.7172990727272966
======== tgt result =======
===== RES ====
p 460
tp 333
fp 161
recall 0.7239130419045369
precision 0.6740890674613582
f1 0.6981127067190613
===== RES ====
p 115
tp 86
fp 39
recall 0.7478260804536863
precision 0.687999994496
f1 0.7166661615628477
===== RES ====
p 114
tp 85
fp 43
recall 0.7456140285472453
precision 0.6640624948120117
f1 0.7024788347110974
===== RES ====
p 116
tp 82
fp 24
recall 0.7068965456302022
precision 0.7735848983624066
f1 0.7387382330982999
======== tgt result =======
===== RES ====
p 462
tp 307
fp 98
recall 0.6645021630638481
precision 0.7580246894863588
f1 0.7081886585439765
===== RES ====
p 115
tp 84
fp 30
recall 0.730434776257089
precision 0.7368420987996307
f1 0.7336239477511466
===== RES ====
p 116
tp 87
fp 39
recall 0.7499999935344829
precision 0.6904761849962208
f1 0.7190077593747063
===== RES ====
p 116
tp 83
fp 21
recall 0.7155172352110584
precision 0.7980769154031067
f1 0.7545449491738833
======== tgt result =======
===== RES ====
p 461
tp 314
fp 118
recall 0.6811279811689198
precision 0.7268518501693244
f1 0.7032469793557745
===== RES ====
p 115
tp 86
fp 39
recall 0.7478260804536863
precision 0.687999994496
f1 0.7166661615628477
===== RES ====
p 116
tp 87
fp 37
recall 0.7499999935344829
precision 0.7016128975676379
f1 0.724999494514233
===== RES ====
p 115
tp 86
fp 35
recall 0.7478260804536863
precision 0.710743795778977
f1 0.7288130534691736
===== RES ====
p 115
tp 84
fp 35
recall 0.730434776257089
precision 0.705882347009392
f1 0.7179482119588549
===== RES ====
p 116
tp 84
fp 34
recall 0.7241379247919144
precision 0.7118644007469118
f1 0.7179482118492779
===== RES ====
p 116
tp 87
fp 43
recall 0.7499999935344829
precision 0.6692307640828403
f1 0.7073165690399441
===== RES ====
p 115
tp 84
fp 31
recall 0.730434776257089
precision 0.730434776257089
f1 0.7304342762574313
===== RES ====
p 116
tp 86
fp 39
recall 0.7413793039536267
precision 0.687999994496
f1 0.7136924408329797
===== RES ====
p 116
tp 85
fp 34
recall 0.7327586143727706
precision 0.7142857082833134
f1 0.7234037492443474
===== RES ====
p 114
tp 83
fp 38
recall 0.728070169052016
precision 0.6859504075541288
f1 0.7063824731556272
===== RES ====
p 115
tp 84
fp 35
recall 0.730434776257089
precision 0.705882347009392
f1 0.7179482119588549
===== RES ====
p 114
tp 81
fp 28
recall 0.7105263095567868
precision 0.743119259237438
f1 0.7264568928395322
===== RES ====
p 116
tp 83
fp 37
recall 0.7155172352110584
precision 0.6916666609027778
f1 0.703389324691536
===== RES ====
p 115
tp 73
fp 26
recall 0.6347826031758035
precision 0.7373737299255179
f1 0.6822424870734692
===== RES ====
p 116
tp 85
fp 33
recall 0.7327586143727706
precision 0.7203389769462799
f1 0.7264952203232311
===== RES ====
p 114
tp 83
fp 38
recall 0.728070169052016
precision 0.6859504075541288
f1 0.7063824731556272
===== RES ====
p 115
tp 85
fp 35
recall 0.7391304283553876
precision 0.7083333274305557
f1 0.7234037493892093
===== RES ====
p 115
tp 84
fp 38
recall 0.730434776257089
precision 0.6885245845202903
f1 0.7088602539482595
===== RES ====
p 116
tp 85
fp 57
recall 0.7327586143727706
precision 0.5985915450803413
f1 0.658914228652496
===== RES ====
p 115
tp 84
fp 36
recall 0.730434776257089
precision 0.6999999941666667
f1 0.7148931111637719
===== RES ====
p 116
tp 85
fp 33
recall 0.7327586143727706
precision 0.7203389769462799
f1 0.7264952203232311
===== RES ====
p 116
tp 84
fp 35
recall 0.7241379247919144
precision 0.705882347009392
f1 0.7148931110189102
===== RES ====
p 116
tp 83
fp 28
recall 0.7155172352110584
precision 0.7477477410112816
f1 0.7312770268395973
===== RES ====
p 116
tp 82
fp 28
recall 0.7068965456302022
precision 0.7454545386776861
f1 0.7256632107451165
===== RES ====
p 116
tp 86
fp 31
recall 0.7413793039536267
precision 0.7350427287603186
f1 0.7381969185657942
===== RES ====
p 114
tp 83
fp 37
recall 0.728070169052016
precision 0.6916666609027778
f1 0.7094012036675301
===== RES ====
p 114
tp 84
fp 32
recall 0.7368420987996307
precision 0.7241379247919144
f1 0.7304342762952383
===== RES ====
p 116
tp 86
fp 50
recall 0.7413793039536267
precision 0.6323529365268167
f1 0.6825391802724705
===== RES ====
p 115
tp 80
fp 25
recall 0.6956521678638942
precision 0.7619047546485261
f1 0.7272722216945572
===== RES ====
p 116
tp 86
fp 42
recall 0.7413793039536267
precision 0.6718749947509766
f1 0.7049175282185739
===== RES ====
p 116
tp 82
fp 30
recall 0.7068965456302022
precision 0.7321428506058674
f1 0.7192977394586424
===== RES ====
p 114
tp 84
fp 34
recall 0.7368420987996307
precision 0.7118644007469118
f1 0.7241374249408921
===== RES ====
p 300
tp 258
fp 151
recall 0.8599999971333333
precision 0.6308068444234551
f1 0.7277851233051669
=== Result of InvGAN+KD: ===
0.7277851233051669
The source-target datasets are: shoes_watches with seed 3000
The F1 score is: 0.7277851233051669
The training time is: 588.0097916126251
The inference time is: 0.0002696886658668518
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: watches
tgt: shoes
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 101
tp 79
fp 44
recall 0.7821782100774435
precision 0.6422764175424681
f1 0.7053566413826818
tgt_res:
===== RES ====
p 417
tp 324
fp 184
recall 0.7769784154029294
precision 0.6377952743350487
f1 0.700540043865361
save pretrained model to: checkpoint/watches/bert/3000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/watches/bert/3000/source-classifier.ptshoesbest
===== RES ====
p 97
tp 84
fp 73
recall 0.8659793725156766
precision 0.5350318437259118
f1 0.6614168455270277
===== RES ====
p 100
tp 93
fp 109
recall 0.9299999907000002
precision 0.4603960373247721
f1 0.6158935926936209
===== RES ====
p 101
tp 92
fp 94
recall 0.9108910800901874
precision 0.4946236532547115
f1 0.6411145219685401
===== RES ====
p 97
tp 84
fp 77
recall 0.8659793725156766
precision 0.5217391271941669
f1 0.6511623164176203
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/3000/source-encoder.ptshoesbest
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/3000/source-classifier.ptshoesbest
Pretraining time:  71.2013590335846
=== Training F' and A ===
===== RES ====
p 99
tp 54
fp 10
recall 0.5454545399449037
precision 0.8437499868164064
f1 0.6625762020403144
======== tgt result =======
===== RES ====
p 417
tp 188
fp 47
recall 0.4508393274560208
precision 0.7999999965957447
f1 0.5766866537556847
===== RES ====
p 100
tp 49
fp 14
recall 0.4899999951000001
precision 0.777777765432099
f1 0.6012265122514934
===== RES ====
p 99
tp 64
fp 24
recall 0.6464646399347006
precision 0.7272727190082646
f1 0.6844914730193224
======== tgt result =======
===== RES ====
p 420
tp 261
fp 102
recall 0.6214285699489795
precision 0.719008262482071
f1 0.6666661676138823
===== RES ====
p 99
tp 67
fp 24
recall 0.6767676699316397
precision 0.7362637281729261
f1 0.7052626513576941
======== tgt result =======
===== RES ====
p 417
tp 280
fp 114
recall 0.6714628281259885
precision 0.7106598966734521
f1 0.6905050474049601
===== RES ====
p 98
tp 66
fp 28
recall 0.6734693808829655
precision 0.702127652105025
f1 0.6874994930559191
===== RES ====
p 97
tp 66
fp 31
recall 0.6804123641194602
precision 0.6804123641194602
f1 0.6804118641198276
===== RES ====
p 99
tp 64
fp 23
recall 0.6464646399347006
precision 0.7356321754525037
f1 0.6881715376925791
===== RES ====
p 95
tp 64
fp 28
recall 0.6736842034349031
precision 0.6956521663516069
f1 0.6844914714179063
===== RES ====
p 102
tp 71
fp 30
recall 0.6960784245482508
precision 0.7029702900696011
f1 0.6995068822833539
===== RES ====
p 99
tp 56
fp 17
recall 0.565656559942863
precision 0.7671232771626949
f1 0.6511622945514751
===== RES ====
p 98
tp 71
fp 34
recall 0.7244897885256144
precision 0.676190469750567
f1 0.6995068828657502
===== RES ====
p 98
tp 64
fp 26
recall 0.653061217825906
precision 0.7111111032098767
f1 0.6808505574924438
===== RES ====
p 102
tp 65
fp 27
recall 0.6372548957131873
precision 0.7065217314508507
f1 0.6701025872041122
===== RES ====
p 101
tp 66
fp 22
recall 0.6534653400646996
precision 0.7499999914772728
f1 0.6984121933879978
===== RES ====
p 98
tp 69
fp 21
recall 0.7040816254685549
precision 0.7666666581481483
f1 0.7340420462882525
======== tgt result =======
===== RES ====
p 420
tp 274
fp 100
recall 0.6523809508276645
precision 0.7326203188967371
f1 0.6901758223582202
===== RES ====
p 95
tp 67
fp 27
recall 0.7052631504709143
precision 0.7127659498641921
f1 0.7089942015064697
===== RES ====
p 99
tp 63
fp 23
recall 0.6363636299357209
precision 0.7325581310167659
f1 0.6810805761873614
===== RES ====
p 100
tp 66
fp 27
recall 0.6599999934
precision 0.7096774117238989
f1 0.6839373174048587
===== RES ====
p 99
tp 70
fp 28
recall 0.7070706999285788
precision 0.7142857069970846
f1 0.7106593912755717
===== RES ====
p 97
tp 60
fp 20
recall 0.6185566946540547
precision 0.7499999906250001
f1 0.6779655986469834
===== RES ====
p 97
tp 66
fp 23
recall 0.6804123641194602
precision 0.741573025375584
f1 0.7096769126492123
===== RES ====
p 98
tp 69
fp 27
recall 0.7040816254685549
precision 0.718749992513021
f1 0.711339698905655
===== RES ====
p 96
tp 65
fp 23
recall 0.677083326280382
precision 0.7386363552427687
f1 0.7065212323963829
===== RES ====
p 96
tp 62
fp 19
recall 0.6458333266059029
precision 0.7654320893156532
f1 0.700564467426698
===== RES ====
p 101
tp 67
fp 19
recall 0.6633663300656799
precision 0.7790697583829098
f1 0.7165770356605007
===== RES ====
p 100
tp 72
fp 19
recall 0.7199999928
precision 0.7912087825141892
f1 0.7539261947866575
======== tgt result =======
===== RES ====
p 419
tp 271
fp 101
recall 0.6467780414158042
precision 0.7284946216975952
f1 0.6852080967461481
===== RES ====
p 98
tp 58
fp 12
recall 0.5918367286547273
precision 0.828571416734694
f1 0.6904756961454667
===== RES ====
p 98
tp 68
fp 21
recall 0.6938775439400251
precision 0.7640449352354501
f1 0.7272722206529203
===== RES ====
p 96
tp 67
fp 23
recall 0.6979166593967014
precision 0.7444444361728396
f1 0.7204296003009591
===== RES ====
p 96
tp 60
fp 17
recall 0.6249999934895835
precision 0.779220769101029
f1 0.6936411165094233
===== RES ====
p 100
tp 70
fp 24
recall 0.699999993
precision 0.7446808431416931
f1 0.7216489775750085
===== RES ====
p 95
tp 66
fp 26
recall 0.6947368347922439
precision 0.7173912965500946
f1 0.7058818455206721
===== RES ====
p 94
tp 53
fp 13
recall 0.5638297812358534
precision 0.8030302908631775
f1 0.6624995070316044
===== RES ====
p 96
tp 64
fp 22
recall 0.6666666597222223
precision 0.744186037858302
f1 0.7032961970780011
===== RES ====
p 96
tp 63
fp 19
recall 0.6562499931640626
precision 0.7682926735574065
f1 0.7078646636791773
===== RES ====
p 100
tp 69
fp 22
recall 0.6899999931
precision 0.7582417499094314
f1 0.722512582550165
===== RES ====
p 97
tp 57
fp 12
recall 0.5876288599213519
precision 0.8260869445494646
f1 0.6867464939036676
===== RES ====
p 100
tp 67
fp 17
recall 0.6699999933
precision 0.7976190381235829
f1 0.7282603654303949
===== RES ====
p 97
tp 64
fp 19
recall 0.6597938076309917
precision 0.771084328059225
f1 0.7111106062349153
===== RES ====
p 102
tp 66
fp 17
recall 0.6470588171856979
precision 0.7951807133110758
f1 0.7135130110741196
===== RES ====
p 300
tp 201
fp 101
recall 0.6699999977666666
precision 0.6655629117034341
f1 0.6677735841661121
=== Result of InvGAN+KD: ===
0.6677735841661121
The source-target datasets are: watches_shoes with seed 3000
The F1 score is: 0.6677735841661121
The training time is: 536.0788502693176
The inference time is: 0.0002679675817489624
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: computers
tgt: shoes
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 95
tp 93
fp 186
recall 0.9789473581163436
precision 0.33333333213859023
f1 0.4973258215708215
tgt_res:
===== RES ====
p 422
tp 415
fp 773
recall 0.9834123199445206
precision 0.3493265990325533
f1 0.5155275628520318
save pretrained model to: checkpoint/computers/bert/3000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/computers/bert/3000/source-classifier.ptshoesbest
===== RES ====
p 99
tp 92
fp 89
recall 0.9292929199061322
precision 0.5082872900094625
f1 0.6571423953319501
tgt_res:
===== RES ====
p 415
tp 387
fp 360
recall 0.9325301182348672
precision 0.5180722884630893
f1 0.6660924828715604
save pretrained model to: checkpoint/computers/bert/3000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/computers/bert/3000/source-classifier.ptshoesbest
===== RES ====
p 96
tp 87
fp 97
recall 0.906249990559896
precision 0.4728260843868148
f1 0.6214281163778771
===== RES ====
p 96
tp 91
fp 128
recall 0.9479166567925349
precision 0.4155251122578762
f1 0.577777350345485
===== RES ====
p 97
tp 86
fp 128
recall 0.8865979290041451
precision 0.40186915700061143
f1 0.5530542295885293
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/3000/source-encoder.ptshoesbest
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/3000/source-classifier.ptshoesbest
Pretraining time:  83.27864003181458
=== Training F' and A ===
===== RES ====
p 101
tp 82
fp 32
recall 0.8118811800803843
precision 0.7192982393044014
f1 0.7627901924070317
======== tgt result =======
===== RES ====
p 419
tp 337
fp 154
recall 0.804295940801203
precision 0.6863543774208668
f1 0.7406588421619079
===== RES ====
p 101
tp 78
fp 29
recall 0.7722772200764632
precision 0.7289719558040004
f1 0.7499994932048447
===== RES ====
p 97
tp 73
fp 25
recall 0.7525773118290999
precision 0.744897951582674
f1 0.7487174410522737
===== RES ====
p 103
tp 78
fp 21
recall 0.7572815460458102
precision 0.7878787799204164
f1 0.772276720272846
======== tgt result =======
===== RES ====
p 422
tp 330
fp 100
recall 0.7819905194739561
precision 0.7674418586803677
f1 0.7746473855499272
===== RES ====
p 100
tp 75
fp 21
recall 0.7499999925
precision 0.7812499918619793
f1 0.7653056148483065
===== RES ====
p 100
tp 77
fp 25
recall 0.7699999923
precision 0.7549019533833142
f1 0.7623757301248256
===== RES ====
p 101
tp 73
fp 20
recall 0.7227722700715618
precision 0.7849462281188578
f1 0.7525768126796807
===== RES ====
p 94
tp 68
fp 20
recall 0.7234042476233591
precision 0.7727272639462811
f1 0.7472522395849253
===== RES ====
p 101
tp 76
fp 20
recall 0.7524752400745026
precision 0.791666658420139
f1 0.7715730965500931
===== RES ====
p 93
tp 68
fp 24
recall 0.7311827878367443
precision 0.7391304267485823
f1 0.7351346272026777
===== RES ====
p 103
tp 77
fp 21
recall 0.7475728082759922
precision 0.7857142776967931
f1 0.7661686469150064
===== RES ====
p 96
tp 77
fp 27
recall 0.8020833249782987
precision 0.7403846082655327
f1 0.7699994931003238
===== RES ====
p 94
tp 72
fp 22
recall 0.7659574386600273
precision 0.7659574386600273
f1 0.7659569386603537
===== RES ====
p 99
tp 72
fp 17
recall 0.7272727199265382
precision 0.8089887549551825
f1 0.7659569400750191
===== RES ====
p 99
tp 74
fp 20
recall 0.7474747399244976
precision 0.7872340341783614
f1 0.7668388706277252
===== RES ====
p 96
tp 78
fp 24
recall 0.8124999915364585
precision 0.7647058748558248
f1 0.7878782803798701
======== tgt result =======
===== RES ====
p 420
tp 329
fp 112
recall 0.783333331468254
precision 0.746031744340064
f1 0.7642271407989821
===== RES ====
p 98
tp 67
fp 15
recall 0.6836734624114953
precision 0.8170731607674006
f1 0.7444439401237873
===== RES ====
p 94
tp 75
fp 24
recall 0.7978723319375284
precision 0.7575757499234773
f1 0.7772015648208536
===== RES ====
p 98
tp 76
fp 21
recall 0.7755101961682633
precision 0.7835051465618026
f1 0.7794866715059092
===== RES ====
p 99
tp 75
fp 21
recall 0.7575757499234773
precision 0.7812499918619793
f1 0.7692302614598909
===== RES ====
p 98
tp 78
fp 25
recall 0.7959183592253228
precision 0.7572815460458102
f1 0.7761188955722136
===== RES ====
p 101
tp 75
fp 22
recall 0.7425742500735223
precision 0.7731958683175684
f1 0.757575250127868
===== RES ====
p 98
tp 75
fp 24
recall 0.7653061146397335
precision 0.7575757499234773
f1 0.7614208120800007
===== RES ====
p 99
tp 77
fp 24
recall 0.7777777699214367
precision 0.7623762300754829
f1 0.7699994923503248
===== RES ====
p 100
tp 78
fp 22
recall 0.7799999922
precision 0.7799999922
f1 0.7799994922003206
===== RES ====
p 93
tp 64
fp 16
recall 0.6881720356110534
precision 0.7999999900000001
f1 0.7398838873336898
===== RES ====
p 98
tp 73
fp 21
recall 0.744897951582674
precision 0.7765957364191943
f1 0.7604161589630022
===== RES ====
p 100
tp 81
fp 31
recall 0.8099999919
precision 0.7232142792570154
f1 0.764150437789574
===== RES ====
p 99
tp 76
fp 25
recall 0.767676759922457
precision 0.7524752400745026
f1 0.759999492450329
===== RES ====
p 98
tp 70
fp 20
recall 0.7142857069970846
precision 0.7777777691358025
f1 0.7446803440474147
===== RES ====
p 94
tp 74
fp 25
recall 0.7872340341783614
precision 0.7474747399244976
f1 0.7668388706277252
===== RES ====
p 100
tp 82
fp 29
recall 0.8199999918
precision 0.7387387320834349
f1 0.7772506788260427
===== RES ====
p 99
tp 77
fp 31
recall 0.7777777699214367
precision 0.7129629563614541
f1 0.7439608464144868
===== RES ====
p 97
tp 74
fp 23
recall 0.7628865900733341
precision 0.7628865900733341
f1 0.7628860900736618
===== RES ====
p 96
tp 72
fp 22
recall 0.7499999921875001
precision 0.7659574386600273
f1 0.7578942289199976
===== RES ====
p 96
tp 67
fp 20
recall 0.6979166593967014
precision 0.7701149336768399
f1 0.7322399303655345
===== RES ====
p 101
tp 70
fp 19
recall 0.6930693000686208
precision 0.7865168450953164
f1 0.7368415995017217
===== RES ====
p 97
tp 76
fp 26
recall 0.7835051465618026
precision 0.7450980319108036
f1 0.7638185881167887
===== RES ====
p 100
tp 78
fp 21
recall 0.7799999922
precision 0.7878787799204164
f1 0.7839190901243059
===== RES ====
p 97
tp 82
fp 32
recall 0.8453608160272081
precision 0.7192982393044014
f1 0.7772506807127902
===== RES ====
p 300
tp 265
fp 157
recall 0.883333330388889
precision 0.6279620838199951
f1 0.7340715344038327
=== Result of InvGAN+KD: ===
0.7340715344038327
The source-target datasets are: computers_shoes with seed 3000
The F1 score is: 0.7340715344038327
The training time is: 533.3660652637482
The inference time is: 0.00026755034923553467
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: shoes
tgt: computers
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 137
tp 102
fp 36
recall 0.7445255420107625
precision 0.7391304294265911
f1 0.7418176764300891
tgt_res:
===== RES ====
p 570
tp 408
fp 126
recall 0.7157894724284395
precision 0.764044942389429
f1 0.7391299339756054
save pretrained model to: checkpoint/shoes/bert/3000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/shoes/bert/3000/source-classifier.ptcomputersbest
===== RES ====
p 139
tp 107
fp 44
recall 0.7697841671238549
precision 0.7086092668304023
f1 0.7379305302500405
===== RES ====
p 140
tp 91
fp 14
recall 0.6499999953571429
precision 0.8666666584126985
f1 0.7428566469974074
tgt_res:
===== RES ====
p 569
tp 374
fp 47
recall 0.657293496208623
precision 0.8883610430205201
f1 0.755555065203867
save pretrained model to: checkpoint/shoes/bert/3000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/shoes/bert/3000/source-classifier.ptcomputersbest
===== RES ====
p 140
tp 109
fp 40
recall 0.7785714230102041
precision 0.7315436192513851
f1 0.7543247547805658
tgt_res:
===== RES ====
p 575
tp 451
fp 143
recall 0.7843478247228733
precision 0.759259257981045
f1 0.7715991566395078
save pretrained model to: checkpoint/shoes/bert/3000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/shoes/bert/3000/source-classifier.ptcomputersbest
===== RES ====
p 137
tp 99
fp 22
recall 0.7226277319516224
precision 0.818181811419985
f1 0.7674413564392392
tgt_res:
===== RES ====
p 574
tp 410
fp 87
recall 0.714285713041314
precision 0.8249496965292763
f1 0.7656390903240343
save pretrained model to: checkpoint/shoes/bert/3000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/shoes/bert/3000/source-classifier.ptcomputersbest
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/3000/source-encoder.ptcomputersbest
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/3000/source-classifier.ptcomputersbest
Pretraining time:  77.35550427436829
=== Training F' and A ===
===== RES ====
p 136
tp 103
fp 46
recall 0.757352935607699
precision 0.6912751631458043
f1 0.7228065135121943
======== tgt result =======
===== RES ====
p 573
tp 453
fp 157
recall 0.7905759148506528
precision 0.7426229496022575
f1 0.765849034274979
===== RES ====
p 133
tp 98
fp 22
recall 0.7368420997229918
precision 0.8166666598611112
f1 0.7747030525085621
======== tgt result =======
===== RES ====
p 570
tp 446
fp 95
recall 0.7824561389781471
precision 0.8243992591046224
f1 0.802879786924458
===== RES ====
p 142
tp 104
fp 22
recall 0.7323943610394763
precision 0.825396818846057
f1 0.7761188989755927
======== tgt result =======
===== RES ====
p 574
tp 452
fp 114
recall 0.787456444621156
precision 0.7985865710272322
f1 0.7929819547740917
===== RES ====
p 138
tp 104
fp 26
recall 0.7536231829447596
precision 0.799999993846154
f1 0.776118897638994
===== RES ====
p 139
tp 101
fp 26
recall 0.7266186998084986
precision 0.7952755842891687
f1 0.7593979915487479
===== RES ====
p 136
tp 95
fp 20
recall 0.6985294066284603
precision 0.8260869493383743
f1 0.7569716090224048
===== RES ====
p 141
tp 108
fp 46
recall 0.7659574413761884
precision 0.7012986967448137
f1 0.7322028858377432
===== RES ====
p 139
tp 109
fp 33
recall 0.7841726562289737
precision 0.7676056283971435
f1 0.7758002062793714
===== RES ====
p 142
tp 107
fp 25
recall 0.7535211214540766
precision 0.8106060544651057
f1 0.7810213927756402
======== tgt result =======
===== RES ====
p 570
tp 453
fp 125
recall 0.794736840710988
precision 0.7837370228655068
f1 0.7891981049214642
===== RES ====
p 136
tp 104
fp 32
recall 0.7647058767301038
precision 0.7647058767301038
f1 0.7647053767304307
===== RES ====
p 136
tp 94
fp 23
recall 0.6911764655060554
precision 0.8034187965519761
f1 0.7430825008986438
===== RES ====
p 137
tp 109
fp 39
recall 0.795620432148756
precision 0.7364864815102264
f1 0.7649117760791193
===== RES ====
p 143
tp 62
fp 1
recall 0.4335664305345005
precision 0.9841269685059212
f1 0.6019413171366917
===== RES ====
p 138
tp 107
fp 33
recall 0.7753623132220122
precision 0.7642857088265307
f1 0.7697836671500583
===== RES ====
p 141
tp 99
fp 19
recall 0.7021276545948394
precision 0.8389830437374318
f1 0.7644782625187697
===== RES ====
p 139
tp 111
fp 42
recall 0.7985611453340925
precision 0.7254901913366655
f1 0.7602734685450825
===== RES ====
p 141
tp 94
fp 19
recall 0.6666666619385343
precision 0.8318583997180673
f1 0.7401569805632908
===== RES ====
p 139
tp 102
fp 32
recall 0.7338129443610579
precision 0.7611940241701939
f1 0.7472522419464226
===== RES ====
p 139
tp 110
fp 32
recall 0.7913669007815332
precision 0.774647881868677
f1 0.7829176439511305
======== tgt result =======
===== RES ====
p 572
tp 452
fp 162
recall 0.7902097888283046
precision 0.7361563505925792
f1 0.7622254689878688
===== RES ====
p 138
tp 95
fp 22
recall 0.6884057921130016
precision 0.8119658050259333
f1 0.7450975367631146
===== RES ====
p 138
tp 103
fp 29
recall 0.7463768061856754
precision 0.7803030243916438
f1 0.7629624575586267
===== RES ====
p 141
tp 100
fp 20
recall 0.7092198531261004
precision 0.833333326388889
f1 0.7662830222695196
===== RES ====
p 139
tp 109
fp 32
recall 0.7841726562289737
precision 0.7730496399074493
f1 0.7785709230360355
===== RES ====
p 136
tp 89
fp 17
recall 0.6544117598940312
precision 0.8396226335884658
f1 0.7355366916880559
===== RES ====
p 141
tp 108
fp 38
recall 0.7659574413761884
precision 0.7397260223306437
f1 0.7526127353255151
===== RES ====
p 137
tp 85
fp 16
recall 0.6204379516756354
precision 0.8415841500833253
f1 0.7142852197235176
===== RES ====
p 140
tp 110
fp 41
recall 0.7857142801020409
precision 0.7284768163677032
f1 0.7560132412232724
===== RES ====
p 137
tp 103
fp 33
recall 0.7518248120304758
precision 0.757352935607699
f1 0.7545782490577454
===== RES ====
p 141
tp 99
fp 30
recall 0.7021276545948394
precision 0.7674418545159546
f1 0.7333328288892286
===== RES ====
p 139
tp 102
fp 42
recall 0.7338129443610579
precision 0.7083333284143519
f1 0.7208475515991929
===== RES ====
p 138
tp 99
fp 32
recall 0.7173912991493384
precision 0.7557251850708001
f1 0.7360589744202617
===== RES ====
p 137
tp 98
fp 35
recall 0.715328461931909
precision 0.7368420997229918
f1 0.7259254206587805
===== RES ====
p 139
tp 93
fp 21
recall 0.6690647433880235
precision 0.8157894665281626
f1 0.7351773646834249
===== RES ====
p 138
tp 106
fp 40
recall 0.768115936462928
precision 0.7260273922874836
f1 0.7464783683796242
===== RES ====
p 139
tp 99
fp 30
recall 0.7122302107033798
precision 0.7674418545159546
f1 0.7388054653322601
===== RES ====
p 137
tp 102
fp 36
recall 0.7445255420107625
precision 0.7391304294265911
f1 0.7418176764300891
===== RES ====
p 136
tp 94
fp 19
recall 0.6911764655060554
precision 0.8318583997180673
f1 0.7550195785232526
===== RES ====
p 142
tp 107
fp 43
recall 0.7535211214540766
precision 0.7133333285777779
f1 0.7328762076847092
===== RES ====
p 137
tp 103
fp 29
recall 0.7518248120304758
precision 0.7803030243916438
f1 0.7657987509849727
===== RES ====
p 143
tp 105
fp 33
recall 0.7342657291310088
precision 0.7608695597038437
f1 0.7473304556936513
===== RES ====
p 300
tp 256
fp 138
recall 0.8533333304888889
precision 0.649746191244299
f1 0.7377516684304274
=== Result of InvGAN+KD: ===
0.7377516684304274
The source-target datasets are: shoes_computers with seed 3000
The F1 score is: 0.7377516684304274
The training time is: 738.871835231781
The inference time is: 0.0002656504511833191
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: cameras
tgt: shoes
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 96
tp 85
fp 66
recall 0.8854166574435766
precision 0.5629139035568616
f1 0.6882586285305446
tgt_res:
===== RES ====
p 419
tp 375
fp 310
recall 0.894988064689766
precision 0.5474452546752624
f1 0.679347353883111
save pretrained model to: checkpoint/cameras/bert/3000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/cameras/bert/3000/source-classifier.ptshoesbest
===== RES ====
p 104
tp 102
fp 154
recall 0.9807692213387575
precision 0.39843749844360354
f1 0.5666662526546179
===== RES ====
p 99
tp 96
fp 123
recall 0.969696959902051
precision 0.4383561623819353
f1 0.6037731523083611
===== RES ====
p 98
tp 82
fp 47
recall 0.836734685339442
precision 0.6356589098010937
f1 0.7224664633122546
tgt_res:
===== RES ====
p 419
tp 375
fp 230
recall 0.894988064689766
precision 0.6198347097192815
f1 0.732421390066466
save pretrained model to: checkpoint/cameras/bert/3000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/cameras/bert/3000/source-classifier.ptshoesbest
===== RES ====
p 102
tp 96
fp 115
recall 0.9411764613610151
precision 0.45497630116124976
f1 0.6134180870686724
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/3000/source-encoder.ptshoesbest
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/3000/source-classifier.ptshoesbest
Pretraining time:  64.12446761131287
=== Training F' and A ===
===== RES ====
p 96
tp 51
fp 15
recall 0.531249994466146
precision 0.7727272610192839
f1 0.6296291390035708
======== tgt result =======
===== RES ====
p 422
tp 242
fp 76
recall 0.5734597142809011
precision 0.7610062869150745
f1 0.6540535621625294
===== RES ====
p 98
tp 80
fp 35
recall 0.8163265222823824
precision 0.6956521678638942
f1 0.7511732050522364
======== tgt result =======
===== RES ====
p 417
tp 347
fp 170
recall 0.8321342905704213
precision 0.6711798826476211
f1 0.7430401893656796
===== RES ====
p 97
tp 59
fp 18
recall 0.6082474164098205
precision 0.7662337562826785
f1 0.6781604183515283
===== RES ====
p 92
tp 63
fp 17
recall 0.684782601252363
precision 0.7874999901562502
f1 0.732557633450852
===== RES ====
p 100
tp 58
fp 16
recall 0.5799999942
precision 0.7837837731921112
f1 0.6666661701681479
===== RES ====
p 101
tp 71
fp 20
recall 0.7029702900696011
precision 0.7802197716459366
f1 0.7395828269860135
===== RES ====
p 100
tp 74
fp 18
recall 0.7399999926
precision 0.8043478173440455
f1 0.7708328261721984
======== tgt result =======
===== RES ====
p 420
tp 299
fp 125
recall 0.7119047602097506
precision 0.7051886775821021
f1 0.7085303040198044
===== RES ====
p 99
tp 71
fp 19
recall 0.7171717099275585
precision 0.7888888801234569
f1 0.7513222445063643
===== RES ====
p 95
tp 69
fp 20
recall 0.726315781828255
precision 0.7752808901653833
f1 0.7499994923798224
===== RES ====
p 97
tp 66
fp 18
recall 0.6804123641194602
precision 0.7857142763605444
f1 0.7292812624770646
===== RES ====
p 95
tp 73
fp 21
recall 0.7684210445429364
precision 0.7765957364191943
f1 0.7724862643266304
======== tgt result =======
===== RES ====
p 421
tp 306
fp 124
recall 0.7268408533804255
precision 0.7116279053217955
f1 0.7191534349113736
===== RES ====
p 100
tp 71
fp 16
recall 0.7099999929
precision 0.8160919446426214
f1 0.7593577830653265
===== RES ====
p 93
tp 65
fp 17
recall 0.6989247236674762
precision 0.7926829171624035
f1 0.7428566363431912
===== RES ====
p 96
tp 66
fp 17
recall 0.6874999928385418
precision 0.7951807133110758
f1 0.7374296619959037
===== RES ====
p 98
tp 64
fp 18
recall 0.653061217825906
precision 0.780487795359905
f1 0.71111060716084
===== RES ====
p 101
tp 72
fp 21
recall 0.7128712800705814
precision 0.7741935400624351
f1 0.7422675344354511
===== RES ====
p 97
tp 64
fp 15
recall 0.6597938076309917
precision 0.8101265720237143
f1 0.7272722242384565
===== RES ====
p 96
tp 70
fp 19
recall 0.7291666590711806
precision 0.7865168450953164
f1 0.7567562492917832
===== RES ====
p 97
tp 51
fp 16
recall 0.5257731904559465
precision 0.7611940184896415
f1 0.6219507286589118
===== RES ====
p 103
tp 84
fp 22
recall 0.8155339726647187
precision 0.7924528227127092
f1 0.8038272436073713
======== tgt result =======
===== RES ====
p 417
tp 304
fp 132
recall 0.7290167848225018
precision 0.6972477048228264
f1 0.7127779276510531
===== RES ====
p 97
tp 66
fp 18
recall 0.6804123641194602
precision 0.7857142763605444
f1 0.7292812624770646
===== RES ====
p 96
tp 53
fp 16
recall 0.5520833275824654
precision 0.7681159308968706
f1 0.6424237480260802
===== RES ====
p 96
tp 75
fp 23
recall 0.7812499918619793
precision 0.7653061146397335
f1 0.7731953683710323
===== RES ====
p 100
tp 69
fp 16
recall 0.6899999931
precision 0.8117646963321801
f1 0.7459454411690671
===== RES ====
p 99
tp 65
fp 15
recall 0.6565656499336803
precision 0.8124999898437502
f1 0.726256480759364
===== RES ====
p 98
tp 77
fp 21
recall 0.7857142776967931
precision 0.7857142776967931
f1 0.7857137776971114
===== RES ====
p 98
tp 57
fp 16
recall 0.5816326471261974
precision 0.7808219071120287
f1 0.6666661695568038
===== RES ====
p 100
tp 74
fp 18
recall 0.7399999926
precision 0.8043478173440455
f1 0.7708328261721984
===== RES ====
p 102
tp 78
fp 19
recall 0.7647058748558248
precision 0.8041237030502711
f1 0.783919090427328
===== RES ====
p 97
tp 72
fp 22
recall 0.7422680335848657
precision 0.7659574386600273
f1 0.7539261937998437
===== RES ====
p 97
tp 68
fp 18
recall 0.7010309206079287
precision 0.7906976652244457
f1 0.7431688925919313
===== RES ====
p 102
tp 62
fp 21
recall 0.6078431312956556
precision 0.7469879428073742
f1 0.6702697682983929
===== RES ====
p 97
tp 77
fp 23
recall 0.7938144248060369
precision 0.7699999923
f1 0.781725380504842
===== RES ====
p 98
tp 50
fp 17
recall 0.510204076426489
precision 0.7462686455780799
f1 0.6060601163640199
===== RES ====
p 100
tp 81
fp 23
recall 0.8099999919
precision 0.7788461463572486
f1 0.7941171394659049
===== RES ====
p 94
tp 64
fp 20
recall 0.6808510565866909
precision 0.7619047528344672
f1 0.7191006170941469
===== RES ====
p 95
tp 63
fp 19
recall 0.6631578877562327
precision 0.7682926735574065
f1 0.7118639014335245
===== RES ====
p 99
tp 56
fp 16
recall 0.565656559942863
precision 0.7777777669753089
f1 0.6549702650391781
===== RES ====
p 96
tp 74
fp 21
recall 0.7708333253038195
precision 0.7789473602216067
f1 0.7748686018478598
===== RES ====
p 99
tp 80
fp 24
recall 0.8080807999183758
precision 0.7692307618343196
f1 0.7881768324398326
===== RES ====
p 300
tp 249
fp 173
recall 0.8299999972333334
precision 0.5900473919667123
f1 0.6897502048867379
=== Result of InvGAN+KD: ===
0.6897502048867379
The source-target datasets are: cameras_shoes with seed 3000
The F1 score is: 0.6897502048867379
The training time is: 537.3521237373352
The inference time is: 0.0002695024013519287
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: shoes
tgt: cameras
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 91
tp 62
fp 6
recall 0.6813186738316629
precision 0.9117646924740487
f1 0.7798737144894492
tgt_res:
===== RES ====
p 389
tp 251
fp 58
recall 0.6452442142795779
precision 0.8122977319990364
f1 0.7191972122440803
save pretrained model to: checkpoint/shoes/bert/3000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/shoes/bert/3000/source-classifier.ptcamerasbest
===== RES ====
p 94
tp 77
fp 36
recall 0.8191489274558625
precision 0.6814159231733105
f1 0.74396084968177
===== RES ====
p 90
tp 73
fp 45
recall 0.8111111020987656
precision 0.6186440625538638
f1 0.7019225792348107
===== RES ====
p 89
tp 64
fp 14
recall 0.7191011155157179
precision 0.8205128099934256
f1 0.7664665588586548
===== RES ====
p 90
tp 69
fp 14
recall 0.7666666581481483
precision 0.8313252911888519
f1 0.7976873528687679
tgt_res:
===== RES ====
p 387
tp 275
fp 100
recall 0.710594313409317
precision 0.7333333313777778
f1 0.7217842751327854
save pretrained model to: checkpoint/shoes/bert/3000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/shoes/bert/3000/source-classifier.ptcamerasbest
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/3000/source-encoder.ptcamerasbest
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/3000/source-classifier.ptcamerasbest
Pretraining time:  65.52469062805176
=== Training F' and A ===
===== RES ====
p 92
tp 67
fp 10
recall 0.7282608616493385
precision 0.8701298588294823
f1 0.7928989028398468
======== tgt result =======
===== RES ====
p 389
tp 264
fp 68
recall 0.6786632373299145
precision 0.7951807204964436
f1 0.7323157285557993
===== RES ====
p 90
tp 67
fp 13
recall 0.7444444361728396
precision 0.8374999895312502
f1 0.7882347865747096
===== RES ====
p 88
tp 63
fp 12
recall 0.7159090827737604
precision 0.8399999888000002
f1 0.773005628665305
===== RES ====
p 91
tp 66
fp 11
recall 0.7252747173046734
precision 0.8571428460111319
f1 0.7857137798330803
===== RES ====
p 92
tp 69
fp 17
recall 0.7499999918478262
precision 0.8023255720659818
f1 0.7752803907338152
===== RES ====
p 88
tp 61
fp 9
recall 0.6931818103047521
precision 0.871428558979592
f1 0.7721513954497626
===== RES ====
p 91
tp 69
fp 17
recall 0.7582417499094314
precision 0.8023255720659818
f1 0.7796605085387351
===== RES ====
p 92
tp 63
fp 6
recall 0.684782601252363
precision 0.9130434650283556
f1 0.7826081961347149
===== RES ====
p 88
tp 65
fp 14
recall 0.7386363552427687
precision 0.8227847997115848
f1 0.778442605902288
===== RES ====
p 89
tp 65
fp 10
recall 0.7303370704456509
precision 0.8666666551111113
f1 0.7926824208063787
===== RES ====
p 91
tp 62
fp 8
recall 0.6813186738316629
precision 0.8857142730612247
f1 0.7701858343431245
===== RES ====
p 94
tp 73
fp 12
recall 0.7765957364191943
precision 0.8588235193079586
f1 0.8156419502515457
======== tgt result =======
===== RES ====
p 388
tp 288
fp 115
recall 0.7422680393240515
precision 0.7146401967378655
f1 0.7281916601594338
===== RES ====
p 91
tp 74
fp 23
recall 0.8131868042506945
precision 0.7628865900733341
f1 0.7872335346879585
===== RES ====
p 90
tp 66
fp 8
recall 0.7333333251851853
precision 0.891891879839299
f1 0.8048775437242787
===== RES ====
p 94
tp 67
fp 9
recall 0.7127659498641921
precision 0.8815789357686983
f1 0.7882347904501371
===== RES ====
p 91
tp 71
fp 22
recall 0.7802197716459366
precision 0.7634408520060124
f1 0.7717386221057115
===== RES ====
p 92
tp 67
fp 9
recall 0.7282608616493385
precision 0.8815789357686983
f1 0.797618542659038
===== RES ====
p 92
tp 74
fp 21
recall 0.8043478173440455
precision 0.7789473602216067
f1 0.7914433419317408
===== RES ====
p 93
tp 61
fp 10
recall 0.6559139714417853
precision 0.8591549174766914
f1 0.7439019389503538
===== RES ====
p 92
tp 67
fp 9
recall 0.7282608616493385
precision 0.8815789357686983
f1 0.797618542659038
===== RES ====
p 92
tp 63
fp 7
recall 0.684782601252363
precision 0.8999999871428573
f1 0.777777277397048
===== RES ====
p 93
tp 66
fp 9
recall 0.7096774117238989
precision 0.8799999882666668
f1 0.7857137821006512
===== RES ====
p 89
tp 64
fp 14
recall 0.7191011155157179
precision 0.8205128099934256
f1 0.7664665588586548
===== RES ====
p 88
tp 59
fp 7
recall 0.6704545378357439
precision 0.8939393803948579
f1 0.7662332664870731
===== RES ====
p 89
tp 70
fp 23
recall 0.7865168450953164
precision 0.7526881639495897
f1 0.7692302610195255
===== RES ====
p 90
tp 71
fp 25
recall 0.7888888801234569
precision 0.7395833256293404
f1 0.7634403525266306
===== RES ====
p 90
tp 60
fp 9
recall 0.6666666592592594
precision 0.8695652047889101
f1 0.754716480361065
===== RES ====
p 90
tp 75
fp 24
recall 0.8333333240740742
precision 0.7575757499234773
f1 0.7936502863864731
===== RES ====
p 91
tp 69
fp 21
recall 0.7582417499094314
precision 0.7666666581481483
f1 0.7624304308174581
===== RES ====
p 90
tp 61
fp 8
recall 0.6777777702469137
precision 0.8840579582020586
f1 0.7672950965550395
===== RES ====
p 93
tp 65
fp 9
recall 0.6989247236674762
precision 0.8783783665084005
f1 0.7784426109221818
===== RES ====
p 92
tp 71
fp 25
recall 0.7717391220463139
precision 0.7395833256293404
f1 0.7553186411275378
===== RES ====
p 91
tp 63
fp 11
recall 0.6923076846999155
precision 0.8513513398466035
f1 0.7636358596881074
===== RES ====
p 93
tp 68
fp 18
recall 0.7311827878367443
precision 0.7906976652244457
f1 0.7597760285886994
===== RES ====
p 90
tp 58
fp 8
recall 0.6444444372839507
precision 0.8787878654729112
f1 0.7435892458911816
===== RES ====
p 91
tp 63
fp 15
recall 0.6923076846999155
precision 0.8076922973372783
f1 0.7455616243132057
===== RES ====
p 87
tp 66
fp 16
recall 0.7586206809353945
precision 0.804878038964902
f1 0.7810645799520021
===== RES ====
p 89
tp 66
fp 11
recall 0.741573025375584
precision 0.8571428460111319
f1 0.7951802159242479
===== RES ====
p 92
tp 68
fp 17
recall 0.7391304267485823
precision 0.7999999905882355
f1 0.7683610740211998
===== RES ====
p 90
tp 69
fp 24
recall 0.7666666581481483
precision 0.741935475893167
f1 0.7540978525489296
===== RES ====
p 300
tp 230
fp 118
recall 0.7666666641111112
precision 0.660919538330691
f1 0.7098760437627321
=== Result of InvGAN+KD: ===
0.7098760437627321
The source-target datasets are: shoes_cameras with seed 3000
The F1 score is: 0.7098760437627321
The training time is: 491.27715134620667
The inference time is: 0.0002726837992668152
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: computers
tgt: cameras
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 89
tp 84
fp 98
recall 0.9438202141143797
precision 0.46153845900253593
f1 0.6199257535712067
tgt_res:
===== RES ====
p 388
tp 345
fp 382
recall 0.88917525544027
precision 0.4745529567062545
f1 0.6188336258267116
save pretrained model to: checkpoint/computers/bert/3000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/computers/bert/3000/source-classifier.ptcamerasbest
===== RES ====
p 90
tp 83
fp 43
recall 0.9222222119753087
precision 0.6587301535021417
f1 0.7685180252918026
tgt_res:
===== RES ====
p 389
tp 335
fp 174
recall 0.8611825170663688
precision 0.6581532403572627
f1 0.7461019571557935
save pretrained model to: checkpoint/computers/bert/3000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/computers/bert/3000/source-classifier.ptcamerasbest
===== RES ====
p 91
tp 76
fp 35
recall 0.8351648259871998
precision 0.6846846785163543
f1 0.7524747449763085
===== RES ====
p 94
tp 85
fp 31
recall 0.9042553095291989
precision 0.7327586143727706
f1 0.8095233073018894
tgt_res:
===== RES ====
p 388
tp 307
fp 99
recall 0.791237111362791
precision 0.7561576336055231
f1 0.773299246420261
save pretrained model to: checkpoint/computers/bert/3000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/computers/bert/3000/source-classifier.ptcamerasbest
===== RES ====
p 91
tp 81
fp 38
recall 0.8901098803284628
precision 0.6806722631876281
f1 0.771428072970834
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/3000/source-encoder.ptcamerasbest
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/3000/source-classifier.ptcamerasbest
Pretraining time:  85.2933304309845
=== Training F' and A ===
===== RES ====
p 89
tp 66
fp 9
recall 0.741573025375584
precision 0.8799999882666668
f1 0.8048775426088726
======== tgt result =======
===== RES ====
p 388
tp 264
fp 44
recall 0.6804123693803805
precision 0.8571428543599258
f1 0.7586201940814412
===== RES ====
p 88
tp 70
fp 10
recall 0.7954545364152894
precision 0.8749999890625002
f1 0.8333328245467841
======== tgt result =======
===== RES ====
p 389
tp 286
fp 68
recall 0.7352185071074074
precision 0.8079096022375435
f1 0.7698514505853258
===== RES ====
p 88
tp 70
fp 11
recall 0.7954545364152894
precision 0.8641975201950923
f1 0.8284018579184411
===== RES ====
p 90
tp 71
fp 11
recall 0.7888888801234569
precision 0.8658536479773946
f1 0.8255808868310209
===== RES ====
p 94
tp 77
fp 10
recall 0.8191489274558625
precision 0.8850574610912936
f1 0.8508282206284791
======== tgt result =======
===== RES ====
p 388
tp 298
fp 76
recall 0.7680412351339143
precision 0.796791441719809
f1 0.7821517290873332
===== RES ====
p 93
tp 71
fp 10
recall 0.7634408520060124
precision 0.8765431990550223
f1 0.8160914470210462
===== RES ====
p 89
tp 75
fp 12
recall 0.8426966197449818
precision 0.8620689556084028
f1 0.8522722176526695
======== tgt result =======
===== RES ====
p 388
tp 303
fp 79
recall 0.7809278330388458
precision 0.7931937152010636
f1 0.7870124849994745
===== RES ====
p 92
tp 75
fp 10
recall 0.8152173824432893
precision 0.8823529307958479
f1 0.8474571183251689
===== RES ====
p 94
tp 81
fp 14
recall 0.8617021184925306
precision 0.8526315699722993
f1 0.8571423480868515
======== tgt result =======
===== RES ====
p 387
tp 303
fp 80
recall 0.7829457344109929
precision 0.7911227133391052
f1 0.7870124849826081
===== RES ====
p 87
tp 72
fp 12
recall 0.8275861973840667
precision 0.8571428469387756
f1 0.8421047534629008
===== RES ====
p 91
tp 75
fp 12
recall 0.8241758151189471
precision 0.8620689556084028
f1 0.8426961199977716
===== RES ====
p 88
tp 77
fp 12
recall 0.8749999900568183
precision 0.865168529604848
f1 0.8700559873602416
======== tgt result =======
===== RES ====
p 387
tp 304
fp 79
recall 0.7855297137324814
precision 0.7937336793897293
f1 0.7896098875732631
===== RES ====
p 91
tp 75
fp 12
recall 0.8241758151189471
precision 0.8620689556084028
f1 0.8426961199977716
===== RES ====
p 89
tp 71
fp 12
recall 0.7977528000252495
precision 0.8554216764407028
f1 0.8255808863577927
===== RES ====
p 89
tp 76
fp 15
recall 0.8539325746749149
precision 0.8351648259871998
f1 0.844443935123753
===== RES ====
p 92
tp 71
fp 12
recall 0.7717391220463139
precision 0.8554216764407028
f1 0.8114280634778576
===== RES ====
p 89
tp 69
fp 10
recall 0.7752808901653833
precision 0.8734177104630669
f1 0.8214280634215042
===== RES ====
p 90
tp 73
fp 11
recall 0.8111111020987656
precision 0.8690476087018142
f1 0.8390799507203396
===== RES ====
p 90
tp 76
fp 11
recall 0.8444444350617285
precision 0.8735632083498482
f1 0.858756552587351
===== RES ====
p 93
tp 75
fp 12
recall 0.8064516042317033
precision 0.8620689556084028
f1 0.8333328246299291
===== RES ====
p 89
tp 75
fp 11
recall 0.8426966197449818
precision 0.8720930131151976
f1 0.8571423474941692
===== RES ====
p 94
tp 78
fp 10
recall 0.8297872252150296
precision 0.8863636262913225
f1 0.8571423482674088
===== RES ====
p 89
tp 72
fp 11
recall 0.8089887549551825
precision 0.8674698690666282
f1 0.8372087931993245
===== RES ====
p 92
tp 80
fp 13
recall 0.8695652079395086
precision 0.8602150445138167
f1 0.8648643555298728
===== RES ====
p 94
tp 79
fp 14
recall 0.8404255229741966
precision 0.849462356457394
f1 0.8449192770742757
===== RES ====
p 93
tp 80
fp 15
recall 0.8602150445138167
precision 0.8421052542936289
f1 0.851063320790244
===== RES ====
p 88
tp 80
fp 19
recall 0.9090908987603307
precision 0.8080807999183758
f1 0.855614465841464
===== RES ====
p 88
tp 72
fp 14
recall 0.8181818088842977
precision 0.8372092925905896
f1 0.8275856974504276
===== RES ====
p 91
tp 78
fp 23
recall 0.857142847723705
precision 0.7722772200764632
f1 0.8124994928931012
===== RES ====
p 89
tp 73
fp 14
recall 0.8202247098851156
precision 0.8390804501255121
f1 0.8295449451836693
===== RES ====
p 88
tp 72
fp 14
recall 0.8181818088842977
precision 0.8372092925905896
f1 0.8275856974504276
===== RES ====
p 90
tp 75
fp 14
recall 0.8333333240740742
precision 0.8426966197449818
f1 0.8379883174685423
===== RES ====
p 93
tp 76
fp 14
recall 0.817204292288126
precision 0.8444444350617285
f1 0.8306005839532408
===== RES ====
p 92
tp 79
fp 14
recall 0.8586956428402648
precision 0.849462356457394
f1 0.8540535448359392
===== RES ====
p 88
tp 73
fp 14
recall 0.8295454451188018
precision 0.8390804501255121
f1 0.8342852047676469
===== RES ====
p 90
tp 73
fp 13
recall 0.8111111020987656
precision 0.8488371994321257
f1 0.8295449453773672
===== RES ====
p 90
tp 77
fp 15
recall 0.8555555460493829
precision 0.8369565126417771
f1 0.846153336916127
===== RES ====
p 92
tp 72
fp 13
recall 0.7826086871455578
precision 0.847058813564014
f1 0.8135588136234672
===== RES ====
p 91
tp 73
fp 16
recall 0.8021977933824419
precision 0.8202247098851156
f1 0.8111106021608021
===== RES ====
p 92
tp 79
fp 18
recall 0.8586956428402648
precision 0.8144329812945054
f1 0.835978327482732
===== RES ====
p 300
tp 273
fp 81
recall 0.9099999969666667
precision 0.7711864384994733
f1 0.8348618861771044
=== Result of InvGAN+KD: ===
0.8348618861771044
The source-target datasets are: computers_cameras with seed 3000
The F1 score is: 0.8348618861771044
The training time is: 497.7954626083374
The inference time is: 0.00027048587799072266
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: cameras
tgt: computers
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 134
tp 116
fp 75
recall 0.8656716353308087
precision 0.6073298397521998
f1 0.713845664833465
tgt_res:
===== RES ====
p 568
tp 503
fp 273
recall 0.8855633787225997
precision 0.6481958754533559
f1 0.7485114156239895
save pretrained model to: checkpoint/cameras/bert/3000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/cameras/bert/3000/source-classifier.ptcomputersbest
===== RES ====
p 139
tp 122
fp 52
recall 0.8776978354122458
precision 0.7011494212577619
f1 0.7795522169260749
tgt_res:
===== RES ====
p 574
tp 501
fp 214
recall 0.8728222981309716
precision 0.7006992997193017
f1 0.7773462852269226
save pretrained model to: checkpoint/cameras/bert/3000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/cameras/bert/3000/source-classifier.ptcomputersbest
===== RES ====
p 137
tp 109
fp 25
recall 0.795620432148756
precision 0.8134328297505012
f1 0.8044275384052889
tgt_res:
===== RES ====
p 573
tp 464
fp 94
recall 0.809773122496033
precision 0.8315412171477756
f1 0.8205123191501225
save pretrained model to: checkpoint/cameras/bert/3000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/cameras/bert/3000/source-classifier.ptcomputersbest
===== RES ====
p 140
tp 126
fp 71
recall 0.8999999935714287
precision 0.639593905382772
f1 0.7477739905787302
===== RES ====
p 139
tp 120
fp 42
recall 0.863309346307127
precision 0.7407407361682671
f1 0.7973416903127789
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/3000/source-encoder.ptcomputersbest
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/3000/source-classifier.ptcomputersbest
Pretraining time:  74.94854855537415
=== Training F' and A ===
===== RES ====
p 138
tp 108
fp 25
recall 0.7826086899810965
precision 0.8120300690824807
f1 0.7970474647679502
======== tgt result =======
===== RES ====
p 571
tp 455
fp 99
recall 0.796847634331265
precision 0.8212996375066793
f1 0.808888387565346
===== RES ====
p 137
tp 107
fp 15
recall 0.7810218921093293
precision 0.8770491731389413
f1 0.8262543215518577
======== tgt result =======
===== RES ====
p 574
tp 440
fp 75
recall 0.7665505213126298
precision 0.8543689303798662
f1 0.8080803080646727
===== RES ====
p 134
tp 104
fp 21
recall 0.7761193971931388
precision 0.8319999933440001
f1 0.8030882974914035
===== RES ====
p 136
tp 105
fp 20
recall 0.7720588178525087
precision 0.8399999932800001
f1 0.8045971958723608
===== RES ====
p 141
tp 120
fp 34
recall 0.8510638237513204
precision 0.7792207741609041
f1 0.8135588174895333
===== RES ====
p 141
tp 112
fp 27
recall 0.7943262355012324
precision 0.8057553898866519
f1 0.799999494311537
===== RES ====
p 137
tp 110
fp 15
recall 0.8029197021684693
precision 0.8799999929600001
f1 0.8396941511278532
======== tgt result =======
===== RES ====
p 569
tp 446
fp 90
recall 0.7838312815749889
precision 0.8320895506864001
f1 0.8072393179897033
===== RES ====
p 140
tp 116
fp 28
recall 0.8285714226530613
precision 0.8055555499614198
f1 0.8169009027973699
===== RES ====
p 140
tp 115
fp 28
recall 0.8214285655612246
precision 0.8041957985720574
f1 0.8127203423694219
===== RES ====
p 137
tp 113
fp 31
recall 0.8248175122276095
precision 0.7847222167727624
f1 0.8042699572196974
===== RES ====
p 138
tp 108
fp 24
recall 0.7826086899810965
precision 0.8181818119834712
f1 0.7999994943212999
===== RES ====
p 138
tp 111
fp 27
recall 0.8043478202583492
precision 0.8043478202583492
f1 0.8043473202586601
===== RES ====
p 138
tp 114
fp 25
recall 0.8260869505356018
precision 0.8201438789917707
f1 0.8231041872046203
===== RES ====
p 141
tp 109
fp 21
recall 0.7730496399074493
precision 0.8384615320118344
f1 0.8044275391678049
===== RES ====
p 137
tp 116
fp 27
recall 0.8467153222867495
precision 0.8111888055161622
f1 0.8285709228829546
===== RES ====
p 137
tp 109
fp 22
recall 0.795620432148756
precision 0.8320610623506789
f1 0.813432330001421
===== RES ====
p 139
tp 116
fp 30
recall 0.8345323680968895
precision 0.794520542503284
f1 0.8140345823087095
===== RES ====
p 142
tp 117
fp 26
recall 0.8239436561694109
precision 0.8181818124602671
f1 0.8210521258236348
===== RES ====
p 137
tp 114
fp 26
recall 0.8321167822473228
precision 0.8142857084693879
f1 0.8231041872567519
===== RES ====
p 141
tp 115
fp 23
recall 0.8156028310950154
precision 0.833333327294686
f1 0.8243722540052622
===== RES ====
p 137
tp 110
fp 25
recall 0.8029197021684693
precision 0.8148148087791496
f1 0.8088230234918748
===== RES ====
p 137
tp 116
fp 25
recall 0.8467153222867495
precision 0.8226950296262764
f1 0.8345318682007032
===== RES ====
p 139
tp 114
fp 25
recall 0.8201438789917707
precision 0.8201438789917707
f1 0.8201433789920756
===== RES ====
p 140
tp 120
fp 32
recall 0.8571428510204082
precision 0.7894736790166206
f1 0.8219173034343434
===== RES ====
p 139
tp 113
fp 23
recall 0.8129496344392113
precision 0.8308823468317474
f1 0.8218176759011308
===== RES ====
p 136
tp 113
fp 24
recall 0.8308823468317474
precision 0.8248175122276095
f1 0.8278383217810855
===== RES ====
p 135
tp 109
fp 24
recall 0.8074074014266118
precision 0.8195488660184296
f1 0.8134323297786543
===== RES ====
p 136
tp 109
fp 24
recall 0.8014705823421281
precision 0.8195488660184296
f1 0.810408415970237
===== RES ====
p 140
tp 117
fp 27
recall 0.835714279744898
precision 0.812499994357639
f1 0.8239431562689009
===== RES ====
p 139
tp 113
fp 21
recall 0.8129496344392113
precision 0.8432835757963912
f1 0.8278383219420963
===== RES ====
p 138
tp 116
fp 28
recall 0.8405797040537702
precision 0.8055555499614198
f1 0.8226945298529268
===== RES ====
p 138
tp 112
fp 19
recall 0.8115941970174334
precision 0.8549618255346426
f1 0.8327132487945399
===== RES ====
p 138
tp 117
fp 27
recall 0.8478260808128545
precision 0.812499994357639
f1 0.8297867283841852
===== RES ====
p 137
tp 115
fp 30
recall 0.8394160522670361
precision 0.7931034428061832
f1 0.8156023314977157
===== RES ====
p 140
tp 118
fp 25
recall 0.8428571368367348
precision 0.8251748194043719
f1 0.8339217556471425
===== RES ====
p 141
tp 115
fp 20
recall 0.8156028310950154
precision 0.8518518455418382
f1 0.8333328275312807
===== RES ====
p 143
tp 119
fp 31
recall 0.8321678263484767
precision 0.7933333280444445
f1 0.8122861841608684
===== RES ====
p 136
tp 109
fp 18
recall 0.8014705823421281
precision 0.8582677097774196
f1 0.8288968326854634
===== RES ====
p 140
tp 111
fp 21
recall 0.7928571371938776
precision 0.8409090845385676
f1 0.8161759650197696
===== RES ====
p 134
tp 112
fp 35
recall 0.8358208892849187
precision 0.7619047567217364
f1 0.7971525203078059
===== RES ====
p 300
tp 275
fp 153
recall 0.9166666636111112
precision 0.6425233629847585
f1 0.7554940188763524
=== Result of InvGAN+KD: ===
0.7554940188763524
The source-target datasets are: cameras_computers with seed 3000
The F1 score is: 0.7554940188763524
The training time is: 731.50164103508
The inference time is: 0.0002695322036743164
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
usage: main_invgan_kd.py [-h] [--src SRC] [--tgt TGT] [--srcfix SRCFIX]
                         [--tgtfix TGTFIX] [--pretrain] [--adapt]
                         [--seed SEED] [--train_seed TRAIN_SEED] [--load]
                         [--model {bert}] [--max_seq_length MAX_SEQ_LENGTH]
                         [--alpha ALPHA] [--beta BETA]
                         [--temperature TEMPERATURE]
                         [--max_grad_norm MAX_GRAD_NORM]
                         [--clip_value CLIP_VALUE] [--batch_size BATCH_SIZE]
                         [--pre_epochs PRE_EPOCHS] [--epoch EPOCH]
                         [--pre_log_step PRE_LOG_STEP]
                         [--num_epochs NUM_EPOCHS] [--log_step LOG_STEP]
                         [--model_index MODEL_INDEX] [--out_file OUT_FILE]
                         [--d_learning_rate D_LEARNING_RATE]
                         [--rec_epoch REC_EPOCH] [--rec_lr REC_LR]
                         [--epoch_path EPOCH_PATH] [--adda ADDA]
                         [--seed_list SEED_LIST]
                         [--need_kd_model NEED_KD_MODEL]
                         [--need_pred_res NEED_PRED_RES]
main_invgan_kd.py: error: argument --src: expected one argument
usage: main_invgan_kd.py [-h] [--src SRC] [--tgt TGT] [--srcfix SRCFIX]
                         [--tgtfix TGTFIX] [--pretrain] [--adapt]
                         [--seed SEED] [--train_seed TRAIN_SEED] [--load]
                         [--model {bert}] [--max_seq_length MAX_SEQ_LENGTH]
                         [--alpha ALPHA] [--beta BETA]
                         [--temperature TEMPERATURE]
                         [--max_grad_norm MAX_GRAD_NORM]
                         [--clip_value CLIP_VALUE] [--batch_size BATCH_SIZE]
                         [--pre_epochs PRE_EPOCHS] [--epoch EPOCH]
                         [--pre_log_step PRE_LOG_STEP]
                         [--num_epochs NUM_EPOCHS] [--log_step LOG_STEP]
                         [--model_index MODEL_INDEX] [--out_file OUT_FILE]
                         [--d_learning_rate D_LEARNING_RATE]
                         [--rec_epoch REC_EPOCH] [--rec_lr REC_LR]
                         [--epoch_path EPOCH_PATH] [--adda ADDA]
                         [--seed_list SEED_LIST]
                         [--need_kd_model NEED_KD_MODEL]
                         [--need_pred_res NEED_PRED_RES]
main_invgan_kd.py: error: argument --src: expected one argument
=== Argument Setting ===
src: computers
tgt: watches
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 115
tp 100
fp 65
recall 0.8695652098298677
precision 0.6060606023875115
f1 0.7142852251278788
tgt_res:
===== RES ====
p 459
tp 385
fp 307
recall 0.838779954599608
precision 0.5563583806989041
f1 0.6689830119425146
save pretrained model to: checkpoint/computers/bert/1000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/computers/bert/1000/source-classifier.ptwatchesbest
===== RES ====
p 115
tp 91
fp 33
recall 0.7913043409451797
precision 0.7338709618236213
f1 0.7615057704875389
tgt_res:
===== RES ====
p 460
tp 353
fp 159
recall 0.7673913026795841
precision 0.6894531236534118
f1 0.7263369484965065
save pretrained model to: checkpoint/computers/bert/1000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/computers/bert/1000/source-classifier.ptwatchesbest
===== RES ====
p 115
tp 101
fp 41
recall 0.8782608619281664
precision 0.711267600624876
f1 0.7859917173011028
tgt_res:
===== RES ====
p 461
tp 380
fp 202
recall 0.8242950090579284
precision 0.6529209610774553
f1 0.7286668111809298
save pretrained model to: checkpoint/computers/bert/1000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/computers/bert/1000/source-classifier.ptwatchesbest
===== RES ====
p 116
tp 107
fp 94
recall 0.9224137851516053
precision 0.5323383058092621
f1 0.6750783960436667
===== RES ====
p 115
tp 106
fp 80
recall 0.9217391224196598
precision 0.5698924700543415
f1 0.7043184600173139
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/1000/source-encoder.ptwatchesbest
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/1000/source-classifier.ptwatchesbest
Pretraining time:  87.87744855880737
=== Training F' and A ===
===== RES ====
p 114
tp 72
fp 19
recall 0.6315789418282549
precision 0.7912087825141892
f1 0.7024385238313999
======== tgt result =======
===== RES ====
p 462
tp 293
fp 78
recall 0.6341991328264088
precision 0.7897574102701956
f1 0.7034808968354316
===== RES ====
p 116
tp 73
fp 21
recall 0.6293103394024971
precision 0.7765957364191943
f1 0.6952375941046602
===== RES ====
p 116
tp 84
fp 21
recall 0.7241379247919144
precision 0.7999999923809524
f1 0.7601804898346878
======== tgt result =======
===== RES ====
p 462
tp 326
fp 96
recall 0.7056277041003729
precision 0.7725118465106354
f1 0.7375560604413605
===== RES ====
p 115
tp 83
fp 24
recall 0.7217391241587903
precision 0.7757009273298979
f1 0.7477472416609131
===== RES ====
p 116
tp 81
fp 21
recall 0.698275856049346
precision 0.7941176392733565
f1 0.7431187612998875
===== RES ====
p 115
tp 84
fp 21
recall 0.730434776257089
precision 0.7999999923809524
f1 0.7636358577275989
======== tgt result =======
===== RES ====
p 461
tp 330
fp 96
recall 0.7158351394450431
precision 0.7746478855055214
f1 0.7440806715926321
===== RES ====
p 115
tp 80
fp 18
recall 0.6956521678638942
precision 0.8163265222823824
f1 0.7511732050522364
===== RES ====
p 116
tp 81
fp 17
recall 0.698275856049346
precision 0.8265306038109123
f1 0.7570088422572914
===== RES ====
p 115
tp 85
fp 22
recall 0.7391304283553876
precision 0.7943925159402568
f1 0.7657652595165987
======== tgt result =======
===== RES ====
p 460
tp 339
fp 103
recall 0.736956520137051
precision 0.7669683240566327
f1 0.7516624697079543
===== RES ====
p 114
tp 78
fp 20
recall 0.6842105203139428
precision 0.7959183592253228
f1 0.735848552510126
===== RES ====
p 116
tp 84
fp 21
recall 0.7241379247919144
precision 0.7999999923809524
f1 0.7601804898346878
===== RES ====
p 116
tp 79
fp 18
recall 0.6810344768876339
precision 0.8144329812945054
f1 0.7417835345723963
===== RES ====
p 114
tp 83
fp 21
recall 0.728070169052016
precision 0.7980769154031067
f1 0.7614673839747399
===== RES ====
p 116
tp 81
fp 20
recall 0.698275856049346
precision 0.8019801900794041
f1 0.7465432743106802
===== RES ====
p 114
tp 79
fp 19
recall 0.6929824500615575
precision 0.8061224407538526
f1 0.745282514685273
===== RES ====
p 115
tp 82
fp 18
recall 0.7130434720604916
precision 0.8199999918
f1 0.7627901930127637
===== RES ====
p 114
tp 84
fp 23
recall 0.7368420987996307
precision 0.7850467216350774
f1 0.7601804890976032
===== RES ====
p 115
tp 79
fp 18
recall 0.6869565157655956
precision 0.8144329812945054
f1 0.745282515441769
===== RES ====
p 116
tp 85
fp 20
recall 0.7327586143727706
precision 0.809523801814059
f1 0.7692302635084417
======== tgt result =======
===== RES ====
p 461
tp 339
fp 104
recall 0.7353579159753625
precision 0.7652370185886298
f1 0.7499994985392751
===== RES ====
p 115
tp 84
fp 20
recall 0.730434776257089
precision 0.8076922999260356
f1 0.7671227819273068
===== RES ====
p 116
tp 85
fp 22
recall 0.7327586143727706
precision 0.7943925159402568
f1 0.7623313325427066
===== RES ====
p 115
tp 80
fp 17
recall 0.6956521678638942
precision 0.8247422595387396
f1 0.754716477616916
===== RES ====
p 116
tp 87
fp 25
recall 0.7499999935344829
precision 0.7767857073501276
f1 0.7631573881966955
===== RES ====
p 114
tp 83
fp 20
recall 0.728070169052016
precision 0.8058252348949007
f1 0.7649764527599932
===== RES ====
p 115
tp 84
fp 19
recall 0.730434776257089
precision 0.8155339726647187
f1 0.7706416962800969
======== tgt result =======
===== RES ====
p 464
tp 340
fp 108
recall 0.732758619110434
precision 0.7589285697345345
f1 0.7456135336068295
===== RES ====
p 114
tp 80
fp 19
recall 0.7017543798091721
precision 0.8080807999183758
f1 0.7511732043469099
===== RES ====
p 116
tp 86
fp 26
recall 0.7413793039536267
precision 0.7678571360012756
f1 0.7543854584490848
===== RES ====
p 114
tp 88
fp 27
recall 0.7719298177900893
precision 0.7652173846502837
f1 0.7685584452626202
===== RES ====
p 115
tp 80
fp 19
recall 0.6956521678638942
precision 0.8080807999183758
f1 0.747663047209694
===== RES ====
p 115
tp 86
fp 21
recall 0.7478260804536863
precision 0.8037383102454364
f1 0.7747742684444417
======== tgt result =======
===== RES ====
p 460
tp 340
fp 104
recall 0.7391304331758034
precision 0.7657657640410681
f1 0.7522118878733058
===== RES ====
p 115
tp 84
fp 22
recall 0.730434776257089
precision 0.7924528227127092
f1 0.7601804894251963
===== RES ====
p 115
tp 88
fp 24
recall 0.7652173846502837
precision 0.7857142786989797
f1 0.7753298897323181
======== tgt result =======
===== RES ====
p 463
tp 352
fp 121
recall 0.7602591776236303
precision 0.7441860449382959
f1 0.752136250587026
===== RES ====
p 115
tp 86
fp 21
recall 0.7478260804536863
precision 0.8037383102454364
f1 0.7747742684444417
===== RES ====
p 116
tp 85
fp 22
recall 0.7327586143727706
precision 0.7943925159402568
f1 0.7623313325427066
===== RES ====
p 115
tp 86
fp 21
recall 0.7478260804536863
precision 0.8037383102454364
f1 0.7747742684444417
===== RES ====
p 114
tp 85
fp 22
recall 0.7456140285472453
precision 0.7943925159402568
f1 0.769230262771357
===== RES ====
p 116
tp 86
fp 22
recall 0.7413793039536267
precision 0.7962962889231825
f1 0.7678566366393555
===== RES ====
p 116
tp 87
fp 22
recall 0.7499999935344829
precision 0.798165130292063
f1 0.7733328269435327
===== RES ====
p 115
tp 87
fp 23
recall 0.756521732551985
precision 0.7909090837190084
f1 0.773332826706496
===== RES ====
p 115
tp 85
fp 20
recall 0.7391304283553876
precision 0.809523801814059
f1 0.7727267667358594
===== RES ====
p 300
tp 261
fp 84
recall 0.8699999971
precision 0.7565217369376181
f1 0.8093018255059846
=== Result of InvGAN+KD: ===
0.8093018255059846
The source-target datasets are: computers_watches with seed 1000
The F1 score is: 0.8093018255059846
The training time is: 598.1462225914001
The inference time is: 0.00026967376470565796
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: watches
tgt: computers
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 136
tp 115
fp 97
recall 0.8455882290765572
precision 0.5424528276299395
f1 0.6609190602791115
tgt_res:
===== RES ====
p 573
tp 510
fp 383
recall 0.8900523544676224
precision 0.5711086219808414
f1 0.695770327785745
save pretrained model to: checkpoint/watches/bert/1000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/watches/bert/1000/source-classifier.ptcomputersbest
===== RES ====
p 133
tp 108
fp 40
recall 0.8120300690824807
precision 0.7297297247991235
f1 0.7686827699753743
tgt_res:
===== RES ====
p 569
tp 465
fp 168
recall 0.81722319715778
precision 0.7345971552376033
f1 0.7737099826595609
save pretrained model to: checkpoint/watches/bert/1000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/watches/bert/1000/source-classifier.ptcomputersbest
===== RES ====
p 139
tp 108
fp 27
recall 0.7769784116764143
precision 0.7999999940740742
f1 0.7883206622359185
tgt_res:
===== RES ====
p 570
tp 450
fp 97
recall 0.7894736828254848
precision 0.8226691027007877
f1 0.8057291317150244
save pretrained model to: checkpoint/watches/bert/1000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/watches/bert/1000/source-classifier.ptcomputersbest
===== RES ====
p 136
tp 106
fp 23
recall 0.7794117589749135
precision 0.8217054199867797
f1 0.7999994943114549
tgt_res:
===== RES ====
p 572
tp 461
fp 109
recall 0.8059440545350628
precision 0.8087719284056633
f1 0.8073550152253888
save pretrained model to: checkpoint/watches/bert/1000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/watches/bert/1000/source-classifier.ptcomputersbest
===== RES ====
p 141
tp 108
fp 28
recall 0.7659574413761884
precision 0.7941176412197233
f1 0.7797828880348315
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/1000/source-encoder.ptcomputersbest
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/1000/source-classifier.ptcomputersbest
Pretraining time:  82.83860778808594
=== Training F' and A ===
===== RES ====
p 141
tp 109
fp 19
recall 0.7730496399074493
precision 0.851562493347168
f1 0.8104084170758035
======== tgt result =======
===== RES ====
p 572
tp 454
fp 85
recall 0.7937062923186953
precision 0.8423005550235612
f1 0.8172812271430017
===== RES ====
p 139
tp 111
fp 29
recall 0.7985611453340925
precision 0.7928571371938776
f1 0.7956984190339855
===== RES ====
p 134
tp 109
fp 38
recall 0.8134328297505012
precision 0.7414965935952613
f1 0.7758002072925285
===== RES ====
p 140
tp 110
fp 23
recall 0.7857142801020409
precision 0.8270676629543785
f1 0.8058603002861048
===== RES ====
p 136
tp 105
fp 16
recall 0.7720588178525087
precision 0.8677685878696811
f1 0.817120117912763
======== tgt result =======
===== RES ====
p 571
tp 465
fp 101
recall 0.8143607691517325
precision 0.821554768866511
f1 0.817941451077799
===== RES ====
p 139
tp 110
fp 19
recall 0.7913669007815332
precision 0.8527131716843941
f1 0.8208950169584237
======== tgt result =======
===== RES ====
p 573
tp 471
fp 115
recall 0.8219895273612748
precision 0.8037542648400098
f1 0.8127691276511863
===== RES ====
p 138
tp 104
fp 14
recall 0.7536231829447596
precision 0.8813559247342718
f1 0.8124994967044056
===== RES ====
p 141
tp 113
fp 41
recall 0.8014184340324934
precision 0.733766229001518
f1 0.7661011906926549
===== RES ====
p 139
tp 109
fp 20
recall 0.7841726562289737
precision 0.8449612337599904
f1 0.8134323304469538
===== RES ====
p 139
tp 109
fp 29
recall 0.7841726562289737
precision 0.7898550667401807
f1 0.787003104432801
===== RES ====
p 141
tp 114
fp 33
recall 0.8085106325637544
precision 0.7755101988060531
f1 0.7916661613863111
===== RES ====
p 139
tp 115
fp 51
recall 0.82733812354433
precision 0.6927710801640297
f1 0.7540978596294583
===== RES ====
p 137
tp 109
fp 41
recall 0.795620432148756
precision 0.7266666618222223
f1 0.7595813772660468
===== RES ====
p 137
tp 107
fp 22
recall 0.7810218921093293
precision 0.8294573579111832
f1 0.8045107725991005
===== RES ====
p 137
tp 113
fp 25
recall 0.8248175122276095
precision 0.8188405737765176
f1 0.8218176758482382
======== tgt result =======
===== RES ====
p 573
tp 471
fp 120
recall 0.8219895273612748
precision 0.7969543133723277
f1 0.8092778492448265
===== RES ====
p 141
tp 107
fp 19
recall 0.7588652428449274
precision 0.8492063424666163
f1 0.8014976229154863
===== RES ====
p 137
tp 107
fp 29
recall 0.7810218921093293
precision 0.7867647000973184
f1 0.7838822781470807
===== RES ====
p 137
tp 104
fp 19
recall 0.7591240820501892
precision 0.8455284484103378
f1 0.7999994952961688
===== RES ====
p 141
tp 106
fp 20
recall 0.7517730443136663
precision 0.8412698345930966
f1 0.7940069862674789
===== RES ====
p 140
tp 112
fp 17
recall 0.7999999942857143
precision 0.8682170475332012
f1 0.8327132492920448
======== tgt result =======
===== RES ====
p 570
tp 461
fp 117
recall 0.8087719284056633
precision 0.7975778532913878
f1 0.8031353871271429
===== RES ====
p 138
tp 110
fp 22
recall 0.7971014434992649
precision 0.8333333270202021
f1 0.8148143090263698
===== RES ====
p 136
tp 111
fp 31
recall 0.8161764645869378
precision 0.7816901353402104
f1 0.7985606455673124
===== RES ====
p 141
tp 105
fp 18
recall 0.7446808457824053
precision 0.8536585296450526
f1 0.7954540417530662
===== RES ====
p 138
tp 113
fp 33
recall 0.8188405737765176
precision 0.7739725974385439
f1 0.7957741426803375
===== RES ====
p 138
tp 110
fp 33
recall 0.7971014434992649
precision 0.7692307638515331
f1 0.7829176440524462
===== RES ====
p 140
tp 108
fp 17
recall 0.7714285659183674
precision 0.863999993088
f1 0.8150938350732845
===== RES ====
p 140
tp 109
fp 23
recall 0.7785714230102041
precision 0.8257575695018367
f1 0.8014700827749655
===== RES ====
p 137
tp 107
fp 27
recall 0.7810218921093293
precision 0.7985074567275563
f1 0.7896673909127497
===== RES ====
p 140
tp 109
fp 18
recall 0.7785714230102041
precision 0.8582677097774196
f1 0.816478895818734
===== RES ====
p 136
tp 108
fp 26
recall 0.7941176412197233
precision 0.8059701432390288
f1 0.7999994941018216
===== RES ====
p 140
tp 106
fp 16
recall 0.757142851734694
precision 0.8688524518946521
f1 0.8091598015270237
===== RES ====
p 136
tp 108
fp 26
recall 0.7941176412197233
precision 0.8059701432390288
f1 0.7999994941018216
===== RES ====
p 140
tp 107
fp 19
recall 0.7642857088265307
precision 0.8492063424666163
f1 0.8045107735318824
===== RES ====
p 137
tp 106
fp 19
recall 0.773722622089616
precision 0.8479999932160001
f1 0.809159800215913
===== RES ====
p 138
tp 111
fp 30
recall 0.8043478202583492
precision 0.7872340369699714
f1 0.7956984190853723
===== RES ====
p 137
tp 97
fp 13
recall 0.7080291919121957
precision 0.881818173801653
f1 0.7854246008296965
===== RES ====
p 136
tp 106
fp 23
recall 0.7794117589749135
precision 0.8217054199867797
f1 0.7999994943114549
===== RES ====
p 140
tp 111
fp 20
recall 0.7928571371938776
precision 0.8473282378066547
f1 0.8191876863880172
===== RES ====
p 139
tp 108
fp 18
recall 0.7769784116764143
precision 0.8571428503401362
f1 0.8150938346745666
===== RES ====
p 137
tp 98
fp 8
recall 0.715328461931909
precision 0.9245282931648274
f1 0.8065838636389728
===== RES ====
p 300
tp 237
fp 40
recall 0.7899999973666667
precision 0.855595664781243
f1 0.8214899658849214
=== Result of InvGAN+KD: ===
0.8214899658849214
The source-target datasets are: watches_computers with seed 1000
The F1 score is: 0.8214899658849214
The training time is: 738.1021180152893
The inference time is: 0.0002680942416191101
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: cameras
tgt: watches
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 115
tp 97
fp 81
recall 0.8434782535349717
precision 0.544943817163237
f1 0.6621155595525803
tgt_res:
===== RES ====
p 463
tp 360
fp 342
recall 0.7775377952968946
precision 0.5128205120899992
f1 0.6180252710556493
save pretrained model to: checkpoint/cameras/bert/1000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/cameras/bert/1000/source-classifier.ptwatchesbest
===== RES ====
p 116
tp 102
fp 63
recall 0.8793103372473247
precision 0.6181818144352618
f1 0.7259781577237567
tgt_res:
===== RES ====
p 462
tp 386
fp 296
recall 0.8354978336893987
precision 0.5659824038621959
f1 0.6748246921368782
save pretrained model to: checkpoint/cameras/bert/1000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/cameras/bert/1000/source-classifier.ptwatchesbest
===== RES ====
p 116
tp 95
fp 44
recall 0.8189655101813318
precision 0.6834532324931423
f1 0.7450975374397765
tgt_res:
===== RES ====
p 462
tp 358
fp 202
recall 0.77489177321452
precision 0.6392857131441326
f1 0.7005865873755519
save pretrained model to: checkpoint/cameras/bert/1000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/cameras/bert/1000/source-classifier.ptwatchesbest
===== RES ====
p 115
tp 87
fp 37
recall 0.756521732551985
precision 0.7016128975676379
f1 0.7280329674203806
===== RES ====
p 116
tp 97
fp 50
recall 0.8362068893430441
precision 0.659863941089361
f1 0.7376420868889646
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/1000/source-encoder.ptwatchesbest
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/1000/source-classifier.ptwatchesbest
Pretraining time:  71.33766603469849
=== Training F' and A ===
===== RES ====
p 116
tp 84
fp 29
recall 0.7241379247919144
precision 0.743362825279975
f1 0.7336239478274226
======== tgt result =======
===== RES ====
p 462
tp 322
fp 116
recall 0.6969696954611045
precision 0.7351598156731511
f1 0.7155550543213366
===== RES ====
p 114
tp 76
fp 15
recall 0.6666666608187135
precision 0.8351648259871998
f1 0.7414629136945584
======== tgt result =======
===== RES ====
p 461
tp 296
fp 83
recall 0.6420824281082811
precision 0.7810026364617345
f1 0.7047614078489876
===== RES ====
p 116
tp 88
fp 31
recall 0.7586206831153389
precision 0.7394957921050774
f1 0.7489356639206598
======== tgt result =======
===== RES ====
p 463
tp 335
fp 127
recall 0.7235421150679436
precision 0.7251082235387268
f1 0.7243238227591473
===== RES ====
p 115
tp 84
fp 25
recall 0.730434776257089
precision 0.7706421947647505
f1 0.7499994936626416
======== tgt result =======
===== RES ====
p 462
tp 331
fp 119
recall 0.7164502148994584
precision 0.7355555539209877
f1 0.7258766914775295
===== RES ====
p 116
tp 84
fp 20
recall 0.7241379247919144
precision 0.8076922999260356
f1 0.7636358581821436
======== tgt result =======
===== RES ====
p 463
tp 327
fp 113
recall 0.7062634973946793
precision 0.7431818164927686
f1 0.7242519904149711
===== RES ====
p 115
tp 88
fp 26
recall 0.7652173846502837
precision 0.7719298177900893
f1 0.7685584452626202
======== tgt result =======
===== RES ====
p 460
tp 335
fp 129
recall 0.7282608679820416
precision 0.7219827570646924
f1 0.7251077235484418
===== RES ====
p 115
tp 87
fp 31
recall 0.756521732551985
precision 0.7372881293450159
f1 0.7467806095529145
===== RES ====
p 116
tp 86
fp 27
recall 0.7413793039536267
precision 0.7610619401675934
f1 0.7510911965831593
===== RES ====
p 116
tp 89
fp 34
recall 0.7672413726961951
precision 0.723577229889616
f1 0.7447693686738529
===== RES ====
p 116
tp 87
fp 29
recall 0.7499999935344829
precision 0.7499999935344829
f1 0.7499994935348163
===== RES ====
p 116
tp 90
fp 28
recall 0.7758620622770512
precision 0.7627118579431199
f1 0.769230262692998
======== tgt result =======
===== RES ====
p 460
tp 342
fp 140
recall 0.7434782592533081
precision 0.7095435669926482
f1 0.7261141484129457
===== RES ====
p 115
tp 85
fp 27
recall 0.7391304283553876
precision 0.7589285646524235
f1 0.7488981719035339
===== RES ====
p 115
tp 87
fp 25
recall 0.756521732551985
precision 0.7767857073501276
f1 0.7665193171227233
===== RES ====
p 116
tp 88
fp 27
recall 0.7586206831153389
precision 0.7652173846502837
f1 0.7619042553178823
===== RES ====
p 115
tp 87
fp 26
recall 0.756521732551985
precision 0.7699114976114027
f1 0.7631573880812756
===== RES ====
p 115
tp 93
fp 28
recall 0.808695645141777
precision 0.768595034970289
f1 0.7881350868647237
======== tgt result =======
===== RES ====
p 462
tp 344
fp 136
recall 0.7445887429770807
precision 0.7166666651736111
f1 0.7303604328148353
===== RES ====
p 115
tp 85
fp 25
recall 0.7391304283553876
precision 0.7727272657024794
f1 0.7555550490867503
===== RES ====
p 116
tp 89
fp 30
recall 0.7672413726961951
precision 0.7478991533789987
f1 0.7574463021460973
===== RES ====
p 114
tp 89
fp 28
recall 0.7807017475377039
precision 0.7606837541821901
f1 0.770562263975887
===== RES ====
p 115
tp 89
fp 29
recall 0.7739130367485824
precision 0.7542372817437518
f1 0.7639479913797963
===== RES ====
p 115
tp 88
fp 27
recall 0.7652173846502837
precision 0.7652173846502837
f1 0.7652168846506104
===== RES ====
p 115
tp 91
fp 31
recall 0.7913043409451797
precision 0.7459016332303144
f1 0.7679319834075424
===== RES ====
p 116
tp 86
fp 26
recall 0.7413793039536267
precision 0.7678571360012756
f1 0.7543854584490848
===== RES ====
p 116
tp 90
fp 28
recall 0.7758620622770512
precision 0.7627118579431199
f1 0.769230262692998
===== RES ====
p 116
tp 87
fp 27
recall 0.7499999935344829
precision 0.7631578880424746
f1 0.7565212325901226
===== RES ====
p 115
tp 88
fp 26
recall 0.7652173846502837
precision 0.7719298177900893
f1 0.7685584452626202
===== RES ====
p 115
tp 87
fp 25
recall 0.756521732551985
precision 0.7767857073501276
f1 0.7665193171227233
===== RES ====
p 116
tp 89
fp 31
recall 0.7672413726961951
precision 0.7416666604861112
f1 0.75423678188772
===== RES ====
p 116
tp 86
fp 28
recall 0.7413793039536267
precision 0.75438595829486
f1 0.7478255804918277
===== RES ====
p 116
tp 85
fp 30
recall 0.7327586143727706
precision 0.7391304283553876
f1 0.7359302295687512
===== RES ====
p 115
tp 87
fp 29
recall 0.756521732551985
precision 0.7499999935344829
f1 0.7532462467348385
===== RES ====
p 116
tp 89
fp 31
recall 0.7672413726961951
precision 0.7416666604861112
f1 0.75423678188772
===== RES ====
p 116
tp 86
fp 30
recall 0.7413793039536267
precision 0.7413793039536267
f1 0.7413788039539639
===== RES ====
p 114
tp 85
fp 28
recall 0.7456140285472453
precision 0.7522123827237842
f1 0.7488981718259078
===== RES ====
p 116
tp 88
fp 29
recall 0.7586206831153389
precision 0.752136745708233
f1 0.7553643003926757
===== RES ====
p 115
tp 88
fp 30
recall 0.7652173846502837
precision 0.7457627055443838
f1 0.7553643004663554
===== RES ====
p 115
tp 86
fp 25
recall 0.7478260804536863
precision 0.7747747677948219
f1 0.7610614403245511
===== RES ====
p 114
tp 79
fp 23
recall 0.6929824500615575
precision 0.7745097963283354
f1 0.7314809762520544
===== RES ====
p 115
tp 88
fp 29
recall 0.7652173846502837
precision 0.752136745708233
f1 0.7586201831528268
===== RES ====
p 115
tp 86
fp 30
recall 0.7478260804536863
precision 0.7413793039536267
f1 0.7445882381517949
===== RES ====
p 300
tp 273
fp 148
recall 0.9099999969666667
precision 0.6484560554668503
f1 0.7572810653799182
=== Result of InvGAN+KD: ===
0.7572810653799182
The source-target datasets are: cameras_watches with seed 1000
The F1 score is: 0.7572810653799182
The training time is: 596.9316082000732
The inference time is: 0.0002735257148742676
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: watches
tgt: cameras
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 91
tp 74
fp 50
recall 0.8131868042506945
precision 0.596774188735692
f1 0.6883715983994808
tgt_res:
===== RES ====
p 389
tp 318
fp 226
recall 0.8174807176928516
precision 0.5845588224548551
f1 0.6816715380622866
save pretrained model to: checkpoint/watches/bert/1000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/watches/bert/1000/source-classifier.ptcamerasbest
===== RES ====
p 89
tp 70
fp 30
recall 0.7865168450953164
precision 0.699999993
f1 0.7407402345962313
tgt_res:
===== RES ====
p 389
tp 296
fp 121
recall 0.7609254479153588
precision 0.7098321325903306
f1 0.7344908139176718
save pretrained model to: checkpoint/watches/bert/1000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/watches/bert/1000/source-classifier.ptcamerasbest
===== RES ====
p 91
tp 69
fp 29
recall 0.7582417499094314
precision 0.7040816254685549
f1 0.7301582231183954
===== RES ====
p 88
tp 66
fp 20
recall 0.7499999914772728
precision 0.7674418515413738
f1 0.7586201810017829
tgt_res:
===== RES ====
p 388
tp 270
fp 95
recall 0.6958762868662982
precision 0.7397260253706136
f1 0.717130972665685
save pretrained model to: checkpoint/watches/bert/1000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/watches/bert/1000/source-classifier.ptcamerasbest
===== RES ====
p 91
tp 59
fp 16
recall 0.648351641226905
precision 0.786666656177778
f1 0.7108428695750295
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/1000/source-encoder.ptcamerasbest
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/1000/source-classifier.ptcamerasbest
Pretraining time:  76.07456231117249
=== Training F' and A ===
===== RES ====
p 86
tp 64
fp 24
recall 0.744186037858302
precision 0.7272727190082646
f1 0.7356316755189026
======== tgt result =======
===== RES ====
p 388
tp 280
fp 91
recall 0.7216494826761611
precision 0.7547169790977979
f1 0.7378124100329554
===== RES ====
p 91
tp 70
fp 18
recall 0.7692307607776839
precision 0.7954545364152894
f1 0.7821223964298942
======== tgt result =======
===== RES ====
p 387
tp 281
fp 95
recall 0.7260981893382475
precision 0.7473404235443074
f1 0.7365656842810208
===== RES ====
p 91
tp 67
fp 19
recall 0.7362637281729261
precision 0.7790697583829098
f1 0.7570616387376021
===== RES ====
p 91
tp 68
fp 22
recall 0.7472527390411787
precision 0.7555555471604939
f1 0.7513807071826534
===== RES ====
p 93
tp 69
fp 18
recall 0.741935475893167
precision 0.7931034391597306
f1 0.7666661587040292
===== RES ====
p 91
tp 66
fp 19
recall 0.7252747173046734
precision 0.7764705791003461
f1 0.7499994920587005
===== RES ====
p 89
tp 67
fp 19
recall 0.7528089803055171
precision 0.7790697583829098
f1 0.7657137771105306
===== RES ====
p 90
tp 72
fp 27
recall 0.7999999911111112
precision 0.7272727199265382
f1 0.7619042549763914
===== RES ====
p 91
tp 69
fp 21
recall 0.7582417499094314
precision 0.7666666581481483
f1 0.7624304308174581
===== RES ====
p 92
tp 64
fp 16
recall 0.6956521663516069
precision 0.7999999900000001
f1 0.7441855402923825
===== RES ====
p 88
tp 71
fp 27
recall 0.8068181726497935
precision 0.7244897885256144
f1 0.7634403534515918
===== RES ====
p 90
tp 64
fp 19
recall 0.7111111032098767
precision 0.771084328059225
f1 0.7398838853289481
===== RES ====
p 91
tp 65
fp 17
recall 0.7142857064364209
precision 0.7926829171624035
f1 0.7514445793715095
===== RES ====
p 92
tp 68
fp 23
recall 0.7391304267485823
precision 0.7472527390411787
f1 0.7431688908003007
===== RES ====
p 87
tp 70
fp 23
recall 0.804597691901176
precision 0.7526881639495897
f1 0.777777269691679
===== RES ====
p 90
tp 65
fp 18
recall 0.722222214197531
precision 0.7831325206851504
f1 0.7514445788369116
===== RES ====
p 89
tp 70
fp 22
recall 0.7865168450953164
precision 0.76086955694707
f1 0.7734801545743595
===== RES ====
p 93
tp 68
fp 17
recall 0.7311827878367443
precision 0.7999999905882355
f1 0.7640444362457496
===== RES ====
p 91
tp 70
fp 14
recall 0.7692307607776839
precision 0.8333333234126985
f1 0.7999994916574545
======== tgt result =======
===== RES ====
p 388
tp 290
fp 109
recall 0.7474226784860241
precision 0.7268170407849197
f1 0.736975355912566
===== RES ====
p 92
tp 71
fp 11
recall 0.7717391220463139
precision 0.8658536479773946
f1 0.8160914462943988
======== tgt result =======
===== RES ====
p 388
tp 286
fp 93
recall 0.7371134001620788
precision 0.7546174122569461
f1 0.7457622099889631
===== RES ====
p 88
tp 69
fp 16
recall 0.7840909001807852
precision 0.8117646963321801
f1 0.7976873522005204
===== RES ====
p 92
tp 73
fp 13
recall 0.7934782522448016
precision 0.8488371994321257
f1 0.8202242104535299
======== tgt result =======
===== RES ====
p 388
tp 291
fp 99
recall 0.7499999980670103
precision 0.7461538442406311
f1 0.7480714775150216
===== RES ====
p 86
tp 65
fp 13
recall 0.7558139446998379
precision 0.8333333226495728
f1 0.7926824183524853
===== RES ====
p 89
tp 67
fp 16
recall 0.7528089803055171
precision 0.8072289059370011
f1 0.7790692589916669
===== RES ====
p 88
tp 66
fp 12
recall 0.7499999914772728
precision 0.8461538353057201
f1 0.7951802151258747
===== RES ====
p 94
tp 74
fp 17
recall 0.7872340341783614
precision 0.8131868042506945
f1 0.7999994914831468
===== RES ====
p 93
tp 69
fp 16
recall 0.741935475893167
precision 0.8117646963321801
f1 0.7752803911756779
===== RES ====
p 91
tp 69
fp 15
recall 0.7582417499094314
precision 0.82142856164966
f1 0.7885709203594998
===== RES ====
p 89
tp 67
fp 15
recall 0.7528089803055171
precision 0.8170731607674006
f1 0.7836252226671216
===== RES ====
p 90
tp 73
fp 23
recall 0.8111111020987656
precision 0.7604166587456598
f1 0.784945728639467
===== RES ====
p 94
tp 72
fp 19
recall 0.7659574386600273
precision 0.7912087825141892
f1 0.778377870095281
===== RES ====
p 91
tp 73
fp 23
recall 0.8021977933824419
precision 0.7604166587456598
f1 0.7807481551091306
===== RES ====
p 93
tp 72
fp 22
recall 0.7741935400624351
precision 0.7659574386600273
f1 0.7700529677145861
===== RES ====
p 88
tp 66
fp 19
recall 0.7499999914772728
precision 0.7764705791003461
f1 0.7630052716766282
===== RES ====
p 91
tp 70
fp 20
recall 0.7692307607776839
precision 0.7777777691358025
f1 0.7734801544522633
===== RES ====
p 88
tp 67
fp 19
recall 0.761363627711777
precision 0.7790697583829098
f1 0.7701144337432233
===== RES ====
p 91
tp 71
fp 17
recall 0.7802197716459366
precision 0.8068181726497935
f1 0.7932955806625915
===== RES ====
p 87
tp 67
fp 20
recall 0.7701149336768399
precision 0.7701149336768399
f1 0.7701144336771646
===== RES ====
p 92
tp 74
fp 28
recall 0.8043478173440455
precision 0.7254901889657824
f1 0.7628860914021752
===== RES ====
p 90
tp 74
fp 37
recall 0.8222222130864199
precision 0.6666666606606607
f1 0.7363179060917655
===== RES ====
p 300
tp 273
fp 124
recall 0.9099999969666667
precision 0.687657428998344
f1 0.7833567527735161
=== Result of InvGAN+KD: ===
0.7833567527735161
The source-target datasets are: watches_cameras with seed 1000
The F1 score is: 0.7833567527735161
The training time is: 495.19587683677673
The inference time is: 0.0002689063549041748
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: shoes
tgt: watches
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 115
tp 86
fp 41
recall 0.7478260804536863
precision 0.6771653489986981
f1 0.7107432970087512
tgt_res:
===== RES ====
p 458
tp 339
fp 195
recall 0.740174670872981
precision 0.6348314594853344
f1 0.6834672434926548
save pretrained model to: checkpoint/shoes/bert/1000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/shoes/bert/1000/source-classifier.ptwatchesbest
===== RES ====
p 115
tp 90
fp 28
recall 0.782608688846881
precision 0.7627118579431199
f1 0.7725316822932375
tgt_res:
===== RES ====
p 458
tp 348
fp 136
recall 0.7598253258519098
precision 0.7190082629772556
f1 0.7388530019972637
save pretrained model to: checkpoint/shoes/bert/1000/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/shoes/bert/1000/source-classifier.ptwatchesbest
===== RES ====
p 116
tp 107
fp 114
recall 0.9224137851516053
precision 0.4841628937368195
f1 0.6350143815657123
===== RES ====
p 115
tp 95
fp 50
recall 0.8260869493383743
precision 0.655172409274673
f1 0.7307687318050667
===== RES ====
p 116
tp 92
fp 44
recall 0.7931034414387634
precision 0.6764705832612458
f1 0.7301582275135656
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/1000/source-encoder.ptwatchesbest
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/1000/source-classifier.ptwatchesbest
Pretraining time:  71.50567722320557
=== Training F' and A ===
===== RES ====
p 115
tp 93
fp 37
recall 0.808695645141777
precision 0.7153846098816569
f1 0.759183169146516
======== tgt result =======
===== RES ====
p 460
tp 351
fp 157
recall 0.7630434766020794
precision 0.6909448805296361
f1 0.7252061113016545
===== RES ====
p 116
tp 87
fp 27
recall 0.7499999935344829
precision 0.7631578880424746
f1 0.7565212325901226
===== RES ====
p 115
tp 86
fp 31
recall 0.7478260804536863
precision 0.7350427287603186
f1 0.7413788039911221
===== RES ====
p 115
tp 87
fp 30
recall 0.756521732551985
precision 0.7435897372342758
f1 0.7499994935719744
===== RES ====
p 115
tp 84
fp 30
recall 0.730434776257089
precision 0.7368420987996307
f1 0.7336239477511466
===== RES ====
p 116
tp 86
fp 27
recall 0.7413793039536267
precision 0.7610619401675934
f1 0.7510911965831593
===== RES ====
p 116
tp 88
fp 37
recall 0.7586206831153389
precision 0.7039999943680001
f1 0.7302899510686771
===== RES ====
p 116
tp 86
fp 28
recall 0.7413793039536267
precision 0.75438595829486
f1 0.7478255804918277
===== RES ====
p 115
tp 87
fp 36
recall 0.756521732551985
precision 0.7073170674201864
f1 0.731091931396429
===== RES ====
p 115
tp 84
fp 25
recall 0.730434776257089
precision 0.7706421947647505
f1 0.7499994936626416
===== RES ====
p 115
tp 84
fp 36
recall 0.730434776257089
precision 0.6999999941666667
f1 0.7148931111637719
===== RES ====
p 115
tp 83
fp 23
recall 0.7217391241587903
precision 0.783018860537558
f1 0.7511307157514427
===== RES ====
p 116
tp 86
fp 31
recall 0.7413793039536267
precision 0.7350427287603186
f1 0.7381969185657942
===== RES ====
p 115
tp 85
fp 30
recall 0.7391304283553876
precision 0.7391304283553876
f1 0.7391299283557259
===== RES ====
p 116
tp 86
fp 30
recall 0.7413793039536267
precision 0.7413793039536267
f1 0.7413788039539639
===== RES ====
p 115
tp 86
fp 35
recall 0.7478260804536863
precision 0.710743795778977
f1 0.7288130534691736
===== RES ====
p 116
tp 82
fp 19
recall 0.7068965456302022
precision 0.8118811800803843
f1 0.7557598640874819
===== RES ====
p 116
tp 90
fp 41
recall 0.7758620622770512
precision 0.6870228955189092
f1 0.7287444352148172
===== RES ====
p 116
tp 87
fp 29
recall 0.7499999935344829
precision 0.7499999935344829
f1 0.7499994935348163
===== RES ====
p 115
tp 89
fp 35
recall 0.7739130367485824
precision 0.7177419296956297
f1 0.7447693689539596
===== RES ====
p 116
tp 86
fp 26
recall 0.7413793039536267
precision 0.7678571360012756
f1 0.7543854584490848
===== RES ====
p 115
tp 88
fp 43
recall 0.7652173846502837
precision 0.6717557200629335
f1 0.7154466507703908
===== RES ====
p 116
tp 81
fp 22
recall 0.698275856049346
precision 0.7864077593552645
f1 0.7397255224039554
===== RES ====
p 116
tp 89
fp 35
recall 0.7672413726961951
precision 0.7177419296956297
f1 0.7416661610420032
===== RES ====
p 116
tp 87
fp 32
recall 0.7499999935344829
precision 0.731092430831156
f1 0.7404250256952222
===== RES ====
p 116
tp 86
fp 32
recall 0.7413793039536267
precision 0.7288135531456479
f1 0.7350422287971844
===== RES ====
p 116
tp 87
fp 34
recall 0.7499999935344829
precision 0.7190082585205929
f1 0.7341767092171682
===== RES ====
p 114
tp 83
fp 25
recall 0.728070169052016
precision 0.7685185114026064
f1 0.7477472413768456
===== RES ====
p 114
tp 88
fp 36
recall 0.7719298177900893
precision 0.7096774136316337
f1 0.7394952929881203
===== RES ====
p 115
tp 83
fp 29
recall 0.7217391241587903
precision 0.7410714219547194
f1 0.731277026684345
===== RES ====
p 115
tp 88
fp 34
recall 0.7652173846502837
precision 0.721311469497447
f1 0.7426155279249921
===== RES ====
p 114
tp 84
fp 33
recall 0.7368420987996307
precision 0.7179487118124042
f1 0.7272722210606687
===== RES ====
p 116
tp 87
fp 33
recall 0.7499999935344829
precision 0.7249999939583334
f1 0.7372876294889917
===== RES ====
p 115
tp 86
fp 33
recall 0.7478260804536863
precision 0.7226890695572348
f1 0.7350422289067613
===== RES ====
p 115
tp 85
fp 32
recall 0.7391304283553876
precision 0.7264957202863613
f1 0.7327581144102698
===== RES ====
p 115
tp 89
fp 33
recall 0.7739130367485824
precision 0.7295081907417361
f1 0.7510543464191755
===== RES ====
p 116
tp 88
fp 30
recall 0.7586206831153389
precision 0.7457627055443838
f1 0.7521362457450909
===== RES ====
p 116
tp 89
fp 35
recall 0.7672413726961951
precision 0.7177419296956297
f1 0.7416661610420032
===== RES ====
p 116
tp 89
fp 34
recall 0.7672413726961951
precision 0.723577229889616
f1 0.7447693686738529
===== RES ====
p 114
tp 85
fp 29
recall 0.7456140285472453
precision 0.7456140285472453
f1 0.7456135285475807
===== RES ====
p 300
tp 259
fp 126
recall 0.8633333304555556
precision 0.6727272709799291
f1 0.7562038850533335
=== Result of InvGAN+KD: ===
0.7562038850533335
The source-target datasets are: shoes_watches with seed 1000
The F1 score is: 0.7562038850533335
The training time is: 581.8478939533234
The inference time is: 0.00027064234018325806
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: watches
tgt: shoes
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 96
tp 89
fp 116
recall 0.9270833236762154
precision 0.43414633934562763
f1 0.5913616878845581
tgt_res:
===== RES ====
p 417
tp 393
fp 477
recall 0.9424460409054052
precision 0.45172413741181133
f1 0.610722171719297
save pretrained model to: checkpoint/watches/bert/1000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/watches/bert/1000/source-classifier.ptshoesbest
===== RES ====
p 95
tp 85
fp 71
recall 0.8947368326869808
precision 0.544871791379027
f1 0.6772903607882504
tgt_res:
===== RES ====
p 417
tp 376
fp 271
recall 0.9016786549120416
precision 0.5811437394418181
f1 0.7067664393287884
save pretrained model to: checkpoint/watches/bert/1000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/watches/bert/1000/source-classifier.ptshoesbest
===== RES ====
p 97
tp 93
fp 119
recall 0.9587628767137848
precision 0.43867924321377716
f1 0.601941312931676
===== RES ====
p 97
tp 87
fp 110
recall 0.8969072072483794
precision 0.4416243632404855
f1 0.5918362885143748
===== RES ====
p 99
tp 85
fp 76
recall 0.8585858499132742
precision 0.5279503072798117
f1 0.6538456772488604
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/1000/source-encoder.ptshoesbest
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/1000/source-classifier.ptshoesbest
Pretraining time:  72.11047577857971
=== Training F' and A ===
===== RES ====
p 95
tp 39
fp 9
recall 0.4105263114681441
precision 0.812499983072917
f1 0.545454091838595
======== tgt result =======
===== RES ====
p 420
tp 168
fp 41
recall 0.39999999904761907
precision 0.8038277473501065
f1 0.534180794629843
===== RES ====
p 98
tp 63
fp 16
recall 0.6428571362973762
precision 0.7974683443358438
f1 0.7118639044977737
======== tgt result =======
===== RES ====
p 420
tp 279
fp 82
recall 0.6642857127040817
precision 0.7728531834547003
f1 0.7144681309857697
===== RES ====
p 99
tp 76
fp 23
recall 0.767676759922457
precision 0.767676759922457
f1 0.7676762599227827
======== tgt result =======
===== RES ====
p 421
tp 319
fp 115
recall 0.7577197131645612
precision 0.7350230397810529
f1 0.7461983287797883
===== RES ====
p 100
tp 70
fp 17
recall 0.699999993
precision 0.804597691901176
f1 0.7486625960139428
===== RES ====
p 99
tp 73
fp 23
recall 0.7373737299255179
precision 0.7604166587456598
f1 0.7487174411574673
===== RES ====
p 99
tp 76
fp 26
recall 0.767676759922457
precision 0.7450980319108036
f1 0.7562183980597845
===== RES ====
p 98
tp 72
fp 20
recall 0.7346938700541442
precision 0.7826086871455578
f1 0.7578942293632103
===== RES ====
p 97
tp 78
fp 29
recall 0.8041237030502711
precision 0.7289719558040004
f1 0.7647053760576112
===== RES ====
p 97
tp 68
fp 16
recall 0.7010309206079287
precision 0.8095237998866215
f1 0.7513807097466741
===== RES ====
p 95
tp 74
fp 29
recall 0.7789473602216067
precision 0.718446594966538
f1 0.7474742407410742
===== RES ====
p 100
tp 73
fp 25
recall 0.7299999927
precision 0.744897951582674
f1 0.737373229976872
===== RES ====
p 99
tp 68
fp 17
recall 0.6868686799306194
precision 0.7999999905882355
f1 0.7391299296435291
===== RES ====
p 97
tp 77
fp 26
recall 0.7938144248060369
precision 0.7475728082759922
f1 0.7699994927503242
======== tgt result =======
===== RES ====
p 417
tp 316
fp 120
recall 0.7577937631707584
precision 0.7247706405395169
f1 0.7409139182064066
===== RES ====
p 100
tp 68
fp 17
recall 0.6799999932
precision 0.7999999905882355
f1 0.7351346304751347
===== RES ====
p 98
tp 73
fp 24
recall 0.744897951582674
precision 0.7525773118290999
f1 0.7487174410522737
===== RES ====
p 98
tp 72
fp 19
recall 0.7346938700541442
precision 0.7912087825141892
f1 0.7619042545284761
===== RES ====
p 97
tp 70
fp 24
recall 0.7216494770963972
precision 0.7446808431416931
f1 0.7329837856421819
===== RES ====
p 99
tp 71
fp 20
recall 0.7171717099275585
precision 0.7802197716459366
f1 0.7473679140723555
===== RES ====
p 99
tp 72
fp 18
recall 0.7272727199265382
precision 0.7999999911111112
f1 0.7619042549763914
===== RES ====
p 98
tp 73
fp 19
recall 0.744897951582674
precision 0.7934782522448016
f1 0.768420545041876
===== RES ====
p 98
tp 75
fp 25
recall 0.7653061146397335
precision 0.7499999925
f1 0.7575752499748224
===== RES ====
p 96
tp 67
fp 16
recall 0.6979166593967014
precision 0.8072289059370011
f1 0.7486028462286005
===== RES ====
p 101
tp 74
fp 23
recall 0.732673260072542
precision 0.7628865900733341
f1 0.7474742401288926
===== RES ====
p 98
tp 75
fp 25
recall 0.7653061146397335
precision 0.7499999925
f1 0.7575752499748224
===== RES ====
p 98
tp 67
fp 14
recall 0.6836734624114953
precision 0.8271604836153027
f1 0.7486028481011987
===== RES ====
p 97
tp 75
fp 22
recall 0.7731958683175684
precision 0.7731958683175684
f1 0.7731953683178918
======== tgt result =======
===== RES ====
p 416
tp 309
fp 109
recall 0.7427884597529124
precision 0.7392344479922621
f1 0.7410066924708215
===== RES ====
p 101
tp 71
fp 15
recall 0.7029702900696011
precision 0.8255813857490537
f1 0.7593577838660347
===== RES ====
p 100
tp 69
fp 15
recall 0.6899999931
precision 0.82142856164966
f1 0.7499994956288729
===== RES ====
p 96
tp 73
fp 21
recall 0.7604166587456598
precision 0.7765957364191943
f1 0.7684205445986634
===== RES ====
p 98
tp 69
fp 17
recall 0.7040816254685549
precision 0.8023255720659818
f1 0.7499994939748109
===== RES ====
p 100
tp 77
fp 20
recall 0.7699999923
precision 0.7938144248060369
f1 0.781725380504842
======== tgt result =======
===== RES ====
p 417
tp 300
fp 96
recall 0.7194244587064161
precision 0.7575757556626875
f1 0.7380068785922238
===== RES ====
p 100
tp 56
fp 12
recall 0.5599999944
precision 0.8235293996539794
f1 0.6666661768710963
===== RES ====
p 95
tp 78
fp 33
recall 0.8210526229362882
precision 0.7027026963720477
f1 0.7572810490624433
===== RES ====
p 101
tp 72
fp 20
recall 0.7128712800705814
precision 0.7826086871455578
f1 0.7461134829931658
===== RES ====
p 98
tp 64
fp 14
recall 0.653061217825906
precision 0.8205128099934256
f1 0.7272722254652111
===== RES ====
p 101
tp 75
fp 28
recall 0.7425742500735223
precision 0.728155332736356
f1 0.7352936104866915
===== RES ====
p 100
tp 68
fp 16
recall 0.6799999932
precision 0.8095237998866215
f1 0.7391299305296338
===== RES ====
p 99
tp 71
fp 19
recall 0.7171717099275585
precision 0.7888888801234569
f1 0.7513222445063643
===== RES ====
p 99
tp 70
fp 18
recall 0.7070706999285788
precision 0.7954545364152894
f1 0.7486625953276217
===== RES ====
p 99
tp 74
fp 26
recall 0.7474747399244976
precision 0.7399999926
f1 0.7437180855032276
===== RES ====
p 300
tp 253
fp 122
recall 0.8433333305222223
precision 0.6746666648675556
f1 0.7496291335816697
=== Result of InvGAN+KD: ===
0.7496291335816697
The source-target datasets are: watches_shoes with seed 1000
The F1 score is: 0.7496291335816697
The training time is: 539.2044188976288
The inference time is: 0.0002674311399459839
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: computers
tgt: shoes
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 92
tp 82
fp 109
recall 0.8913043381379964
precision 0.4293193694800033
f1 0.5795048574464228
tgt_res:
===== RES ====
p 423
tp 390
fp 428
recall 0.9219858134232014
precision 0.4767726155540677
f1 0.6285249323980326
save pretrained model to: checkpoint/computers/bert/1000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/computers/bert/1000/source-classifier.ptshoesbest
===== RES ====
p 93
tp 82
fp 100
recall 0.8817204206266621
precision 0.4505494480739041
f1 0.5963631843970295
tgt_res:
===== RES ====
p 421
tp 389
fp 389
recall 0.9239904966175998
precision 0.4999999993573265
f1 0.648873604963032
save pretrained model to: checkpoint/computers/bert/1000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/computers/bert/1000/source-classifier.ptshoesbest
===== RES ====
p 100
tp 92
fp 108
recall 0.9199999908000002
precision 0.4599999977
f1 0.6133328848003213
tgt_res:
===== RES ====
p 422
tp 398
fp 448
recall 0.9431279598504076
precision 0.47044917202074565
f1 0.6277598072827032
save pretrained model to: checkpoint/computers/bert/1000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/computers/bert/1000/source-classifier.ptshoesbest
===== RES ====
p 99
tp 92
fp 109
recall 0.9292929199061322
precision 0.45771144050889834
f1 0.6133328870447626
tgt_res:
===== RES ====
p 421
tp 399
fp 431
recall 0.9477434656823196
precision 0.4807228909870809
f1 0.6378892406742801
save pretrained model to: checkpoint/computers/bert/1000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/computers/bert/1000/source-classifier.ptshoesbest
===== RES ====
p 101
tp 88
fp 75
recall 0.8712871200862662
precision 0.5398772973013662
f1 0.6666661891934015
tgt_res:
===== RES ====
p 420
tp 393
fp 334
recall 0.9357142834863945
precision 0.5405777159001682
f1 0.6852654456973263
save pretrained model to: checkpoint/computers/bert/1000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/computers/bert/1000/source-classifier.ptshoesbest
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/1000/source-encoder.ptshoesbest
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/1000/source-classifier.ptshoesbest
Pretraining time:  88.78610420227051
=== Training F' and A ===
===== RES ====
p 99
tp 68
fp 13
recall 0.6868686799306194
precision 0.8395061624752326
f1 0.7555550521608183
======== tgt result =======
===== RES ====
p 423
tp 271
fp 83
recall 0.6406619370197117
precision 0.7655367210013087
f1 0.6975541997025371
===== RES ====
p 97
tp 77
fp 23
recall 0.7938144248060369
precision 0.7699999923
f1 0.781725380504842
======== tgt result =======
===== RES ====
p 416
tp 330
fp 134
recall 0.7932692288623336
precision 0.7112068950189506
f1 0.7499994997833892
===== RES ====
p 99
tp 78
fp 20
recall 0.7878787799204164
precision 0.7959183592253228
f1 0.7918776645626596
======== tgt result =======
===== RES ====
p 420
tp 326
fp 114
recall 0.7761904743424036
precision 0.7409090892252066
f1 0.7581390333913515
===== RES ====
p 97
tp 76
fp 15
recall 0.7835051465618026
precision 0.8351648259871998
f1 0.8085101302062843
======== tgt result =======
===== RES ====
p 418
tp 311
fp 103
recall 0.7440191369760307
precision 0.7512077276540876
f1 0.7475956520609391
===== RES ====
p 98
tp 79
fp 21
recall 0.8061224407538526
precision 0.7899999921
f1 0.7979792899707245
===== RES ====
p 98
tp 81
fp 25
recall 0.8265306038109123
precision 0.7641509361872554
f1 0.7941171400426055
===== RES ====
p 99
tp 81
fp 26
recall 0.8181818099173555
precision 0.7570093387195389
f1 0.7864072601096581
===== RES ====
p 94
tp 76
fp 19
recall 0.8085106296966954
precision 0.7999999915789475
f1 0.8042322957367126
===== RES ====
p 101
tp 82
fp 20
recall 0.8118811800803843
precision 0.8039215607458671
f1 0.8078812654520313
===== RES ====
p 94
tp 72
fp 20
recall 0.7659574386600273
precision 0.7826086871455578
f1 0.7741930401205681
===== RES ====
p 100
tp 78
fp 21
recall 0.7799999922
precision 0.7878787799204164
f1 0.7839190901243059
===== RES ====
p 99
tp 73
fp 16
recall 0.7373737299255179
precision 0.8202247098851156
f1 0.7765952378341816
===== RES ====
p 97
tp 76
fp 17
recall 0.7835051465618026
precision 0.817204292288126
f1 0.7999994918008664
===== RES ====
p 94
tp 71
fp 19
recall 0.7553191409008603
precision 0.7888888801234569
f1 0.7717386222829326
===== RES ====
p 100
tp 78
fp 17
recall 0.7799999922
precision 0.8210526229362882
f1 0.7999994921239151
===== RES ====
p 98
tp 73
fp 18
recall 0.744897951582674
precision 0.8021977933824419
f1 0.7724862649985033
===== RES ====
p 101
tp 78
fp 19
recall 0.7722772200764632
precision 0.8041237030502711
f1 0.7878782801247943
===== RES ====
p 98
tp 79
fp 21
recall 0.8061224407538526
precision 0.7899999921
f1 0.7979792899707245
===== RES ====
p 99
tp 77
fp 21
recall 0.7777777699214367
precision 0.7857142776967931
f1 0.7817253804017732
===== RES ====
p 98
tp 74
fp 19
recall 0.7551020331112037
precision 0.7956989161752805
f1 0.7748686021767978
===== RES ====
p 93
tp 73
fp 20
recall 0.7849462281188578
precision 0.7849462281188578
f1 0.7849457281191764
===== RES ====
p 96
tp 78
fp 22
recall 0.8124999915364585
precision 0.7799999922
f1 0.7959178594338832
===== RES ====
p 98
tp 67
fp 16
recall 0.6836734624114953
precision 0.8072289059370011
f1 0.740330986966543
===== RES ====
p 99
tp 83
fp 22
recall 0.8383838299153149
precision 0.7904761829478459
f1 0.8137249826512104
======== tgt result =======
===== RES ====
p 416
tp 315
fp 114
recall 0.7572115366413185
precision 0.7342657325541592
f1 0.7455616285315492
===== RES ====
p 104
tp 75
fp 16
recall 0.7211538392196747
precision 0.8241758151189471
f1 0.7692302635637673
===== RES ====
p 99
tp 80
fp 22
recall 0.8080807999183758
precision 0.7843137178008459
f1 0.7960193926886139
===== RES ====
p 98
tp 73
fp 19
recall 0.744897951582674
precision 0.7934782522448016
f1 0.768420545041876
===== RES ====
p 96
tp 74
fp 23
recall 0.7708333253038195
precision 0.7628865900733341
f1 0.7668388703055693
===== RES ====
p 96
tp 78
fp 20
recall 0.8124999915364585
precision 0.7959183592253228
f1 0.8041232031037225
===== RES ====
p 102
tp 72
fp 13
recall 0.7058823460207614
precision 0.847058813564014
f1 0.7700529718325136
===== RES ====
p 100
tp 82
fp 21
recall 0.8199999918
precision 0.7961164971250826
f1 0.8078812655490973
===== RES ====
p 100
tp 74
fp 19
recall 0.7399999926
precision 0.7956989161752805
f1 0.766838870949881
===== RES ====
p 95
tp 76
fp 21
recall 0.7999999915789475
precision 0.7835051465618026
f1 0.7916661584747082
===== RES ====
p 100
tp 80
fp 24
recall 0.799999992
precision 0.7692307618343196
f1 0.7843132179933983
===== RES ====
p 101
tp 72
fp 13
recall 0.7128712800705814
precision 0.847058813564014
f1 0.774193043762603
===== RES ====
p 97
tp 79
fp 24
recall 0.8144329812945054
precision 0.7669902838156283
f1 0.789999492550316
===== RES ====
p 100
tp 82
fp 22
recall 0.8199999918
precision 0.7884615308801776
f1 0.8039210609384116
===== RES ====
p 99
tp 78
fp 21
recall 0.7878787799204164
precision 0.7878787799204164
f1 0.7878782799207338
===== RES ====
p 99
tp 84
fp 27
recall 0.8484848399142946
precision 0.7567567499391284
f1 0.799999494013916
===== RES ====
p 96
tp 78
fp 19
recall 0.8124999915364585
precision 0.8041237030502711
f1 0.8082896470780833
===== RES ====
p 300
tp 256
fp 126
recall 0.8533333304888889
precision 0.6701570663084894
f1 0.750732642856852
=== Result of InvGAN+KD: ===
0.750732642856852
The source-target datasets are: computers_shoes with seed 1000
The F1 score is: 0.750732642856852
The training time is: 537.7300610542297
The inference time is: 0.00026632100343704224
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: shoes
tgt: computers
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 137
tp 111
fp 58
recall 0.8102189721881827
precision 0.6568047298413922
f1 0.7254896968049849
tgt_res:
===== RES ====
p 569
tp 481
fp 210
recall 0.8453427050169724
precision 0.6960926183848153
f1 0.7634915669680714
save pretrained model to: checkpoint/shoes/bert/1000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/shoes/bert/1000/source-classifier.ptcomputersbest
===== RES ====
p 135
tp 105
fp 37
recall 0.7777777720164609
precision 0.7394366145110097
f1 0.7581222385281351
tgt_res:
===== RES ====
p 571
tp 437
fp 121
recall 0.7653239916544239
precision 0.7831541204602973
f1 0.7741359025925038
save pretrained model to: checkpoint/shoes/bert/1000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/shoes/bert/1000/source-classifier.ptcomputersbest
===== RES ====
p 138
tp 112
fp 45
recall 0.8115941970174334
precision 0.7133757916345491
f1 0.7593215308248022
tgt_res:
===== RES ====
p 572
tp 469
fp 184
recall 0.8199300684966258
precision 0.7182235823610665
f1 0.7657137866505528
save pretrained model to: checkpoint/shoes/bert/1000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/shoes/bert/1000/source-classifier.ptcomputersbest
===== RES ====
p 140
tp 101
fp 28
recall 0.7214285662755102
precision 0.7829457303647618
f1 0.7509288632830394
===== RES ====
p 137
tp 114
fp 50
recall 0.8321167822473228
precision 0.6951219469809637
f1 0.7574745820468806
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/1000/source-encoder.ptcomputersbest
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/1000/source-classifier.ptcomputersbest
Pretraining time:  74.64956569671631
=== Training F' and A ===
===== RES ====
p 139
tp 113
fp 59
recall 0.8129496344392113
precision 0.6569767403664143
f1 0.726687603850586
======== tgt result =======
===== RES ====
p 569
tp 457
fp 196
recall 0.8031634432281838
precision 0.6998468595714443
f1 0.74795367462485
===== RES ====
p 138
tp 113
fp 64
recall 0.8188405737765176
precision 0.6384180754891634
f1 0.7174598205697508
===== RES ====
p 140
tp 113
fp 34
recall 0.807142851377551
precision 0.7687074777638947
f1 0.7874559408032892
======== tgt result =======
===== RES ====
p 571
tp 456
fp 126
recall 0.7985989478133119
precision 0.7835051532929465
f1 0.7909795507119481
===== RES ====
p 138
tp 94
fp 19
recall 0.6811594153539173
precision 0.8318583997180673
f1 0.7490034830561517
===== RES ====
p 134
tp 107
fp 38
recall 0.7985074567275563
precision 0.7379310293935791
f1 0.7670245848848976
===== RES ====
p 136
tp 111
fp 68
recall 0.8161764645869378
precision 0.6201117283792641
f1 0.704761409604776
===== RES ====
p 138
tp 111
fp 49
recall 0.8043478202583492
precision 0.6937499956640626
f1 0.7449659406786797
===== RES ====
p 139
tp 113
fp 58
recall 0.8129496344392113
precision 0.6608187095858555
f1 0.7290317586892014
===== RES ====
p 138
tp 110
fp 36
recall 0.7971014434992649
precision 0.7534246523738037
f1 0.7746473822657458
===== RES ====
p 138
tp 96
fp 24
recall 0.6956521688720857
precision 0.7999999933333334
f1 0.7441855431768244
===== RES ====
p 136
tp 113
fp 60
recall 0.8308823468317474
precision 0.6531791869758429
f1 0.7313910881958894
===== RES ====
p 138
tp 111
fp 46
recall 0.8043478202583492
precision 0.7070063649235263
f1 0.7525418698538199
===== RES ====
p 135
tp 98
fp 32
recall 0.7259259205486969
precision 0.7538461480473373
f1 0.7396221361057134
===== RES ====
p 138
tp 112
fp 44
recall 0.8115941970174334
precision 0.7179487133464826
f1 0.7619042585962811
===== RES ====
p 139
tp 113
fp 52
recall 0.8129496344392113
precision 0.684848480697888
f1 0.7434205513983577
===== RES ====
p 132
tp 98
fp 26
recall 0.7424242367998164
precision 0.7903225742715921
f1 0.7656244945071619
===== RES ====
p 138
tp 110
fp 49
recall 0.7971014434992649
precision 0.6918238950199755
f1 0.7407402382526667
===== RES ====
p 142
tp 115
fp 45
recall 0.809859149226344
precision 0.7187499955078125
f1 0.7615889007064355
===== RES ====
p 138
tp 108
fp 43
recall 0.7826086899810965
precision 0.7152317833428359
f1 0.7474043401303605
===== RES ====
p 138
tp 105
fp 32
recall 0.7608695597038437
precision 0.7664233520699025
f1 0.7636358580895836
===== RES ====
p 138
tp 108
fp 42
recall 0.7826086899810965
precision 0.7199999952
f1 0.7499994956600545
===== RES ====
p 138
tp 111
fp 47
recall 0.8043478202583492
precision 0.7025316411232174
f1 0.7499994972154509
===== RES ====
p 139
tp 97
fp 26
recall 0.697841721598261
precision 0.7886178797673343
f1 0.7404575114798527
===== RES ====
p 135
tp 108
fp 49
recall 0.7999999940740742
precision 0.6878980847904581
f1 0.7397255251692215
===== RES ====
p 141
tp 111
fp 33
recall 0.7872340369699714
precision 0.7708333279803241
f1 0.7789468630104779
===== RES ====
p 137
tp 109
fp 41
recall 0.795620432148756
precision 0.7266666618222223
f1 0.7595813772660468
===== RES ====
p 139
tp 112
fp 41
recall 0.8057553898866519
precision 0.7320261390063652
f1 0.7671227835666596
===== RES ====
p 137
tp 99
fp 29
recall 0.7226277319516224
precision 0.7734374939575196
f1 0.747169306258789
===== RES ====
p 138
tp 108
fp 45
recall 0.7826086899810965
precision 0.7058823483275664
f1 0.7422675374644652
===== RES ====
p 139
tp 113
fp 55
recall 0.8129496344392113
precision 0.6726190436153628
f1 0.7361558514576436
===== RES ====
p 138
tp 110
fp 33
recall 0.7971014434992649
precision 0.7692307638515331
f1 0.7829176440524462
===== RES ====
p 137
tp 107
fp 42
recall 0.7810218921093293
precision 0.7181208005495249
f1 0.7482512438997901
===== RES ====
p 139
tp 111
fp 54
recall 0.7985611453340925
precision 0.6727272686501378
f1 0.7302626567480867
===== RES ====
p 141
tp 99
fp 26
recall 0.7021276545948394
precision 0.7919999936640001
f1 0.744360398468308
===== RES ====
p 139
tp 113
fp 55
recall 0.8129496344392113
precision 0.6726190436153628
f1 0.7361558514576436
===== RES ====
p 138
tp 99
fp 26
recall 0.7173912991493384
precision 0.7919999936640001
f1 0.7528512065234839
===== RES ====
p 143
tp 118
fp 57
recall 0.8251748194043719
precision 0.6742857104326531
f1 0.7421378651757583
===== RES ====
p 138
tp 99
fp 30
recall 0.7173912991493384
precision 0.7674418545159546
f1 0.7415725287214574
===== RES ====
p 139
tp 115
fp 52
recall 0.82733812354433
precision 0.6886227503675284
f1 0.7516334862022152
===== RES ====
p 138
tp 108
fp 39
recall 0.7826086899810965
precision 0.7346938725531029
f1 0.75789423202249
===== RES ====
p 300
tp 255
fp 162
recall 0.8499999971666666
precision 0.6115107899004537
f1 0.7112965824597903
=== Result of InvGAN+KD: ===
0.7112965824597903
The source-target datasets are: shoes_computers with seed 1000
The F1 score is: 0.7112965824597903
The training time is: 729.8288366794586
The inference time is: 0.000270254909992218
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: cameras
tgt: shoes
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 99
tp 91
fp 96
recall 0.9191919099071525
precision 0.4866310134404759
f1 0.6363631792511406
tgt_res:
===== RES ====
p 421
tp 383
fp 376
recall 0.9097387151787679
precision 0.5046113300334502
f1 0.6491520822970782
save pretrained model to: checkpoint/cameras/bert/1000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/cameras/bert/1000/source-classifier.ptshoesbest
===== RES ====
p 99
tp 89
fp 65
recall 0.898989889909193
precision 0.5779220741693372
f1 0.7035568303210584
tgt_res:
===== RES ====
p 418
tp 384
fp 299
recall 0.9186602848835879
precision 0.5622254750187036
f1 0.697547211622813
save pretrained model to: checkpoint/cameras/bert/1000/source-encoder.ptshoesbest
save pretrained model to: checkpoint/cameras/bert/1000/source-classifier.ptshoesbest
===== RES ====
p 95
tp 92
fp 107
recall 0.9684210424376732
precision 0.4623115554657711
f1 0.6258498984453973
===== RES ====
p 104
tp 93
fp 68
recall 0.8942307606323966
precision 0.5776397479649705
f1 0.7018863102886825
===== RES ====
p 99
tp 90
fp 82
recall 0.9090908999081727
precision 0.5232558109113035
f1 0.6642061734456741
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/1000/source-encoder.ptshoesbest
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/1000/source-classifier.ptshoesbest
Pretraining time:  65.80131506919861
=== Training F' and A ===
===== RES ====
p 104
tp 43
fp 12
recall 0.4134615344859468
precision 0.781818167603306
f1 0.5408800438277578
======== tgt result =======
===== RES ====
p 415
tp 169
fp 61
recall 0.4072289146813761
precision 0.7347826055009452
f1 0.5240305472607826
===== RES ====
p 101
tp 75
fp 31
recall 0.7425742500735223
precision 0.7075471631363476
f1 0.7246371744501569
======== tgt result =======
===== RES ====
p 420
tp 330
fp 150
recall 0.7857142838435375
precision 0.6874999985677084
f1 0.7333328339262639
===== RES ====
p 99
tp 80
fp 35
recall 0.8080807999183758
precision 0.6956521678638942
f1 0.747663047209694
======== tgt result =======
===== RES ====
p 419
tp 340
fp 171
recall 0.8114558453187212
precision 0.6653620339229706
f1 0.7311822990198751
===== RES ====
p 99
tp 76
fp 30
recall 0.767676759922457
precision 0.7169811253114988
f1 0.7414629079836796
===== RES ====
p 96
tp 46
fp 18
recall 0.47916666167534727
precision 0.7187499887695314
f1 0.5749995128129004
===== RES ====
p 100
tp 80
fp 32
recall 0.799999992
precision 0.7142857079081634
f1 0.7547164756144268
======== tgt result =======
===== RES ====
p 420
tp 330
fp 149
recall 0.7857142838435375
precision 0.6889352803988825
f1 0.7341485550256347
===== RES ====
p 95
tp 73
fp 27
recall 0.7684210445429364
precision 0.7299999927
f1 0.7487174413678549
===== RES ====
p 98
tp 71
fp 26
recall 0.7244897885256144
precision 0.7319587553406314
f1 0.7282046207498504
===== RES ====
p 102
tp 73
fp 27
recall 0.7156862674932719
precision 0.7299999927
f1 0.7227717701209222
===== RES ====
p 99
tp 71
fp 25
recall 0.7171717099275585
precision 0.7395833256293404
f1 0.728204620855044
===== RES ====
p 99
tp 77
fp 32
recall 0.7777777699214367
precision 0.7064220118676879
f1 0.740384109421564
===== RES ====
p 98
tp 74
fp 27
recall 0.7551020331112037
precision 0.732673260072542
f1 0.7437180856042348
===== RES ====
p 97
tp 73
fp 30
recall 0.7525773118290999
precision 0.7087378571967199
f1 0.729999493150342
===== RES ====
p 97
tp 80
fp 34
recall 0.8247422595387396
precision 0.7017543798091721
f1 0.7582933349209247
======== tgt result =======
===== RES ====
p 420
tp 334
fp 158
recall 0.7952380933446712
precision 0.6788617872380858
f1 0.7324556418612943
===== RES ====
p 98
tp 73
fp 30
recall 0.744897951582674
precision 0.7087378571967199
f1 0.7263676522861784
===== RES ====
p 95
tp 59
fp 22
recall 0.6210526250415513
precision 0.7283950527358636
f1 0.6704540409998517
===== RES ====
p 99
tp 81
fp 34
recall 0.8181818099173555
precision 0.7043478199621929
f1 0.7570088415148694
===== RES ====
p 100
tp 72
fp 26
recall 0.7199999928
precision 0.7346938700541442
f1 0.727272219977897
===== RES ====
p 96
tp 67
fp 25
recall 0.6979166593967014
precision 0.7282608616493385
f1 0.7127654500908892
===== RES ====
p 97
tp 70
fp 29
recall 0.7216494770963972
precision 0.7070706999285788
f1 0.7142852070494963
===== RES ====
p 98
tp 74
fp 30
recall 0.7551020331112037
precision 0.7115384546967456
f1 0.7326727605140159
===== RES ====
p 100
tp 75
fp 30
recall 0.7499999925
precision 0.7142857074829932
f1 0.7317068102323462
===== RES ====
p 99
tp 64
fp 24
recall 0.6464646399347006
precision 0.7272727190082646
f1 0.6844914730193224
===== RES ====
p 100
tp 73
fp 30
recall 0.7299999927
precision 0.7087378571967199
f1 0.7192113156838148
===== RES ====
p 96
tp 70
fp 27
recall 0.7291666590711806
precision 0.7216494770963972
f1 0.7253880935330571
===== RES ====
p 99
tp 76
fp 29
recall 0.767676759922457
precision 0.7238095169160998
f1 0.7450975323436646
===== RES ====
p 100
tp 73
fp 25
recall 0.7299999927
precision 0.744897951582674
f1 0.737373229976872
===== RES ====
p 98
tp 79
fp 32
recall 0.8061224407538526
precision 0.7117117052998946
f1 0.7559803559445604
===== RES ====
p 98
tp 69
fp 26
recall 0.7040816254685549
precision 0.726315781828255
f1 0.7150253994473148
===== RES ====
p 97
tp 77
fp 28
recall 0.7938144248060369
precision 0.7333333263492064
f1 0.7623757308600467
======== tgt result =======
===== RES ====
p 420
tp 315
fp 141
recall 0.7499999982142858
precision 0.6907894721693213
f1 0.7191775813946043
===== RES ====
p 95
tp 64
fp 27
recall 0.6736842034349031
precision 0.7032966955681682
f1 0.688171535842657
===== RES ====
p 99
tp 71
fp 26
recall 0.7171717099275585
precision 0.7319587553406314
f1 0.7244892885780211
===== RES ====
p 99
tp 79
fp 33
recall 0.7979797899193961
precision 0.7053571365593113
f1 0.7488146606773153
===== RES ====
p 96
tp 79
fp 34
recall 0.8229166580946181
precision 0.6991150380609289
f1 0.7559803573181533
===== RES ====
p 100
tp 58
fp 17
recall 0.5799999942
precision 0.7733333230222224
f1 0.662856645486076
===== RES ====
p 98
tp 80
fp 37
recall 0.8163265222823824
precision 0.6837606779165755
f1 0.7441855434941111
===== RES ====
p 98
tp 69
fp 24
recall 0.7040816254685549
precision 0.741935475893167
f1 0.7225125817826432
===== RES ====
p 98
tp 70
fp 29
recall 0.7142857069970846
precision 0.7070706999285788
f1 0.7106593912755717
===== RES ====
p 97
tp 66
fp 24
recall 0.6804123641194602
precision 0.7333333251851853
f1 0.7058818460926065
===== RES ====
p 101
tp 83
fp 32
recall 0.8217821700813647
precision 0.7217391241587903
f1 0.768518013503409
======== tgt result =======
===== RES ====
p 420
tp 338
fp 164
recall 0.804761902845805
precision 0.6733067715671179
f1 0.7331882225383437
===== RES ====
p 300
tp 269
fp 197
recall 0.8966666636777778
precision 0.5772532176453794
f1 0.7023493910998604
=== Result of InvGAN+KD: ===
0.7023493910998604
The source-target datasets are: cameras_shoes with seed 1000
The F1 score is: 0.7023493910998604
The training time is: 539.9780058860779
The inference time is: 0.00026635080575942993
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: shoes
tgt: cameras
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 93
tp 70
fp 28
recall 0.7526881639495897
precision 0.7142857069970846
f1 0.7329837858614741
tgt_res:
===== RES ====
p 387
tp 288
fp 128
recall 0.7441860445886666
precision 0.6923076906434912
f1 0.7173095860390041
save pretrained model to: checkpoint/shoes/bert/1000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/shoes/bert/1000/source-classifier.ptcamerasbest
===== RES ====
p 92
tp 59
fp 6
recall 0.6413043408553876
precision 0.9076922937278109
f1 0.7515918619014043
tgt_res:
===== RES ====
p 388
tp 254
fp 38
recall 0.6546391735705176
precision 0.8698630107196472
f1 0.7470583312978992
save pretrained model to: checkpoint/shoes/bert/1000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/shoes/bert/1000/source-classifier.ptcamerasbest
===== RES ====
p 89
tp 54
fp 7
recall 0.6067415662163869
precision 0.8852458871271166
f1 0.7199995078225453
===== RES ====
p 91
tp 51
fp 3
recall 0.560439554280884
precision 0.9444444269547329
f1 0.7034477987161244
===== RES ====
p 89
tp 62
fp 12
recall 0.6966292056558516
precision 0.837837826515705
f1 0.760735691219413
tgt_res:
===== RES ====
p 386
tp 257
fp 56
recall 0.6658031070834116
precision 0.8210862593575519
f1 0.7353356979133536
save pretrained model to: checkpoint/shoes/bert/1000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/shoes/bert/1000/source-classifier.ptcamerasbest
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/1000/source-encoder.ptcamerasbest
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/1000/source-classifier.ptcamerasbest
Pretraining time:  69.27478909492493
=== Training F' and A ===
===== RES ====
p 93
tp 62
fp 7
recall 0.666666659498208
precision 0.8985507116152072
f1 0.7654316002899024
======== tgt result =======
===== RES ====
p 388
tp 258
fp 51
recall 0.6649484518944627
precision 0.8349514536085714
f1 0.740315142749846
===== RES ====
p 93
tp 66
fp 17
recall 0.7096774117238989
precision 0.7951807133110758
f1 0.749999493091757
===== RES ====
p 91
tp 62
fp 9
recall 0.6813186738316629
precision 0.8732394243205716
f1 0.7654315969367593
===== RES ====
p 87
tp 60
fp 13
recall 0.6896551644867223
precision 0.8219177969600302
f1 0.7499994944534533
===== RES ====
p 92
tp 65
fp 17
recall 0.7065217314508507
precision 0.7926829171624035
f1 0.7471259298457545
===== RES ====
p 87
tp 63
fp 19
recall 0.7241379227110584
precision 0.7682926735574065
f1 0.7455616217922889
===== RES ====
p 89
tp 62
fp 17
recall 0.6966292056558516
precision 0.7848101166479732
f1 0.7380947310802682
===== RES ====
p 92
tp 65
fp 17
recall 0.7065217314508507
precision 0.7926829171624035
f1 0.7471259298457545
===== RES ====
p 87
tp 63
fp 20
recall 0.7241379227110584
precision 0.7590361354332996
f1 0.7411759621456658
===== RES ====
p 84
tp 57
fp 19
recall 0.6785714204931974
precision 0.7499999901315791
f1 0.7124994923440992
===== RES ====
p 90
tp 63
fp 17
recall 0.6999999922222223
precision 0.7874999901562502
f1 0.741175963598951
===== RES ====
p 88
tp 61
fp 16
recall 0.6931818103047521
precision 0.7922077819193795
f1 0.739393432654146
===== RES ====
p 89
tp 62
fp 11
recall 0.6966292056558516
precision 0.8493150568586979
f1 0.7654315941932787
===== RES ====
p 92
tp 63
fp 17
recall 0.684782601252363
precision 0.7874999901562502
f1 0.732557633450852
===== RES ====
p 92
tp 63
fp 14
recall 0.684782601252363
precision 0.8181818075560805
f1 0.7455616252935621
===== RES ====
p 91
tp 65
fp 14
recall 0.7142857064364209
precision 0.8227847997115848
f1 0.7647053758480746
===== RES ====
p 92
tp 67
fp 22
recall 0.7282608616493385
precision 0.7528089803055171
f1 0.7403309836699449
===== RES ====
p 93
tp 60
fp 8
recall 0.6451612833853626
precision 0.8823529282006922
f1 0.7453411177041116
===== RES ====
p 90
tp 61
fp 13
recall 0.6777777702469137
precision 0.8243243131848066
f1 0.743901934711811
===== RES ====
p 93
tp 70
fp 23
recall 0.7526881639495897
precision 0.7526881639495897
f1 0.7526876639499218
===== RES ====
p 93
tp 64
fp 12
recall 0.6881720356110534
precision 0.8421052520775625
f1 0.7573959458005406
===== RES ====
p 93
tp 61
fp 12
recall 0.6559139714417853
precision 0.835616426909364
f1 0.7349392574397265
===== RES ====
p 89
tp 65
fp 21
recall 0.7303370704456509
precision 0.7558139446998379
f1 0.7428566345146221
===== RES ====
p 90
tp 51
fp 6
recall 0.5666666603703705
precision 0.8947368264081259
f1 0.6938770667780558
===== RES ====
p 93
tp 65
fp 18
recall 0.6989247236674762
precision 0.7831325206851504
f1 0.738635856857258
===== RES ====
p 92
tp 67
fp 18
recall 0.7282608616493385
precision 0.7882352848442908
f1 0.7570616391206333
===== RES ====
p 90
tp 62
fp 11
recall 0.688888881234568
precision 0.8493150568586979
f1 0.7607356924238228
===== RES ====
p 92
tp 67
fp 20
recall 0.7282608616493385
precision 0.7701149336768399
f1 0.7486028439814827
===== RES ====
p 90
tp 61
fp 15
recall 0.6777777702469137
precision 0.8026315683864268
f1 0.7349392537381784
===== RES ====
p 91
tp 65
fp 11
recall 0.7142857064364209
precision 0.8552631466412745
f1 0.7784426084839476
======== tgt result =======
===== RES ====
p 389
tp 282
fp 108
recall 0.7249357307842269
precision 0.7230769212228797
f1 0.7240046329305527
===== RES ====
p 91
tp 63
fp 18
recall 0.6923076846999155
precision 0.7777777681755831
f1 0.7325576327072078
===== RES ====
p 90
tp 61
fp 9
recall 0.6777777702469137
precision 0.871428558979592
f1 0.7624994982815677
===== RES ====
p 89
tp 59
fp 11
recall 0.6629213408660524
precision 0.8428571308163267
f1 0.7421378625848769
===== RES ====
p 90
tp 64
fp 15
recall 0.7111111032098767
precision 0.8101265720237143
f1 0.757395942859471
===== RES ====
p 94
tp 64
fp 10
recall 0.6808510565866909
precision 0.8648648531775021
f1 0.7619042599209539
===== RES ====
p 88
tp 59
fp 17
recall 0.6704545378357439
precision 0.776315779259003
f1 0.7195116890247342
===== RES ====
p 87
tp 64
fp 26
recall 0.7356321754525037
precision 0.7111111032098767
f1 0.7231633337805493
===== RES ====
p 87
tp 62
fp 14
recall 0.712643669969613
precision 0.8157894629501387
f1 0.7607356892622475
===== RES ====
p 92
tp 63
fp 8
recall 0.684782601252363
precision 0.8873239311644517
f1 0.7730056337840456
===== RES ====
p 93
tp 67
fp 24
recall 0.7204300997803216
precision 0.7362637281729261
f1 0.7282603617087554
===== RES ====
p 300
tp 253
fp 117
recall 0.8433333305222223
precision 0.6837837819357195
f1 0.7552233838007248
=== Result of InvGAN+KD: ===
0.7552233838007248
The source-target datasets are: shoes_cameras with seed 1000
The F1 score is: 0.7552233838007248
The training time is: 490.906840801239
The inference time is: 0.00026986002922058105
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: computers
tgt: cameras
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 91
tp 81
fp 37
recall 0.8901098803284628
precision 0.6864406721488079
f1 0.7751191181523688
tgt_res:
===== RES ====
p 388
tp 321
fp 158
recall 0.827319585496599
precision 0.6701461363880039
f1 0.7404839328661832
save pretrained model to: checkpoint/computers/bert/1000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/computers/bert/1000/source-classifier.ptcamerasbest
===== RES ====
p 93
tp 82
fp 22
recall 0.8817204206266621
precision 0.7884615308801776
f1 0.8324868027522375
tgt_res:
===== RES ====
p 388
tp 308
fp 100
recall 0.7938144309437772
precision 0.7549019589340639
f1 0.7738688451052443
save pretrained model to: checkpoint/computers/bert/1000/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/computers/bert/1000/source-classifier.ptcamerasbest
===== RES ====
p 90
tp 83
fp 31
recall 0.9222222119753087
precision 0.728070169052016
f1 0.8137249891390915
===== RES ====
p 90
tp 78
fp 31
recall 0.8666666570370372
precision 0.7155963237101255
f1 0.7839190946696397
===== RES ====
p 88
tp 75
fp 21
recall 0.8522727175878101
precision 0.7812499918619793
f1 0.8152168833887746
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/1000/source-encoder.ptcamerasbest
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/1000/source-classifier.ptcamerasbest
Pretraining time:  82.94969010353088
=== Training F' and A ===
===== RES ====
p 91
tp 74
fp 16
recall 0.8131868042506945
precision 0.8222222130864199
f1 0.8176790489914844
======== tgt result =======
===== RES ====
p 388
tp 300
fp 81
recall 0.7731958742958869
precision 0.7874015727364788
f1 0.7802335682335992
===== RES ====
p 93
tp 76
fp 18
recall 0.817204292288126
precision 0.8085106296966954
f1 0.8128337159201224
===== RES ====
p 91
tp 70
fp 15
recall 0.7692307607776839
precision 0.8235294020761247
f1 0.795454036996698
===== RES ====
p 88
tp 70
fp 19
recall 0.7954545364152894
precision 0.7865168450953164
f1 0.7909599430562708
===== RES ====
p 90
tp 75
fp 15
recall 0.8333333240740742
precision 0.8333333240740742
f1 0.8333328240743743
======== tgt result =======
===== RES ====
p 388
tp 308
fp 78
recall 0.7938144309437772
precision 0.7979274590727268
f1 0.7958651310220878
===== RES ====
p 89
tp 69
fp 12
recall 0.7752808901653833
precision 0.8518518413351625
f1 0.8117641974397531
===== RES ====
p 92
tp 72
fp 14
recall 0.7826086871455578
precision 0.8372092925905896
f1 0.8089882555236011
===== RES ====
p 91
tp 77
fp 16
recall 0.8461538368554523
precision 0.8279569803445487
f1 0.8369560127011495
======== tgt result =======
===== RES ====
p 388
tp 309
fp 79
recall 0.7963917505247635
precision 0.7963917505247635
f1 0.7963912505250774
===== RES ====
p 92
tp 77
fp 15
recall 0.8369565126417771
precision 0.8369565126417771
f1 0.8369560126420758
===== RES ====
p 91
tp 71
fp 12
recall 0.7802197716459366
precision 0.8554216764407028
f1 0.8160914456998691
===== RES ====
p 93
tp 69
fp 12
recall 0.741935475893167
precision 0.8518518413351625
f1 0.7931029415381641
===== RES ====
p 91
tp 67
fp 10
recall 0.7362637281729261
precision 0.8701298588294823
f1 0.7976185415961141
===== RES ====
p 88
tp 69
fp 13
recall 0.7840909001807852
precision 0.8414634043723975
f1 0.8117641969553246
===== RES ====
p 88
tp 73
fp 16
recall 0.8295454451188018
precision 0.8202247098851156
f1 0.8248582477579719
===== RES ====
p 90
tp 76
fp 15
recall 0.8444444350617285
precision 0.8351648259871998
f1 0.8397784962610957
======== tgt result =======
===== RES ====
p 387
tp 312
fp 78
recall 0.8062015483043887
precision 0.799999997948718
f1 0.8030883010294154
===== RES ====
p 90
tp 71
fp 13
recall 0.7888888801234569
precision 0.8452380851757371
f1 0.8160914452374572
===== RES ====
p 92
tp 73
fp 15
recall 0.7934782522448016
precision 0.8295454451188018
f1 0.8111106023459871
===== RES ====
p 91
tp 76
fp 18
recall 0.8351648259871998
precision 0.8085106296966954
f1 0.8216211128710129
===== RES ====
p 91
tp 76
fp 12
recall 0.8351648259871998
precision 0.8636363538223142
f1 0.8491615018260802
======== tgt result =======
===== RES ====
p 389
tp 313
fp 81
recall 0.804627247288876
precision 0.7944162416385374
f1 0.7994886422953139
===== RES ====
p 89
tp 74
fp 19
recall 0.8314606648150488
precision 0.7956989161752805
f1 0.8131863044925184
===== RES ====
p 92
tp 82
fp 18
recall 0.8913043381379964
precision 0.8199999918
f1 0.8541661586374446
======== tgt result =======
===== RES ====
p 388
tp 315
fp 91
recall 0.8118556680106813
precision 0.775862067054527
f1 0.7934503798707537
===== RES ====
p 92
tp 74
fp 14
recall 0.8043478173440455
precision 0.8409090813533059
f1 0.8222217133336371
===== RES ====
p 87
tp 68
fp 12
recall 0.7816091864182853
precision 0.8499999893750002
f1 0.8143707486108694
===== RES ====
p 91
tp 74
fp 11
recall 0.8131868042506945
precision 0.8705882250519033
f1 0.8409085819346975
===== RES ====
p 91
tp 76
fp 15
recall 0.8351648259871998
precision 0.8351648259871998
f1 0.8351643259874991
===== RES ====
p 90
tp 75
fp 14
recall 0.8333333240740742
precision 0.8426966197449818
f1 0.8379883174685423
===== RES ====
p 88
tp 68
fp 13
recall 0.7727272639462811
precision 0.8395061624752326
f1 0.8047332191452977
===== RES ====
p 87
tp 70
fp 14
recall 0.804597691901176
precision 0.8333333234126985
f1 0.8187129408710007
===== RES ====
p 90
tp 80
fp 16
recall 0.8888888790123458
precision 0.8333333246527779
f1 0.8602145450343981
======== tgt result =======
===== RES ====
p 388
tp 313
fp 81
recall 0.8067010288487086
precision 0.7944162416385374
f1 0.8005110069338096
===== RES ====
p 94
tp 79
fp 16
recall 0.8404255229741966
precision 0.8315789386149586
f1 0.8359783271467958
===== RES ====
p 94
tp 74
fp 12
recall 0.7872340341783614
precision 0.8604651062736616
f1 0.822221714074377
===== RES ====
p 87
tp 70
fp 16
recall 0.804597691901176
precision 0.8139534789075177
f1 0.8092480455748355
===== RES ====
p 92
tp 79
fp 16
recall 0.8586956428402648
precision 0.8315789386149586
f1 0.8449192771886627
===== RES ====
p 90
tp 72
fp 12
recall 0.7999999911111112
precision 0.8571428469387756
f1 0.8275856979788985
===== RES ====
p 95
tp 78
fp 16
recall 0.8210526229362882
precision 0.8297872252150296
f1 0.8253963166767679
===== RES ====
p 93
tp 74
fp 14
recall 0.7956989161752805
precision 0.8409090813533059
f1 0.8176790493577731
===== RES ====
p 91
tp 72
fp 13
recall 0.7912087825141892
precision 0.847058813564014
f1 0.8181813094656976
===== RES ====
p 89
tp 73
fp 15
recall 0.8202247098851156
precision 0.8295454451188018
f1 0.8248582477579719
===== RES ====
p 94
tp 74
fp 13
recall 0.7872340341783614
precision 0.8505747028669575
f1 0.8176790497240619
===== RES ====
p 88
tp 73
fp 19
recall 0.8295454451188018
precision 0.7934782522448016
f1 0.8111106023459871
===== RES ====
p 300
tp 275
fp 69
recall 0.9166666636111112
precision 0.7994186023272715
f1 0.8540367667627607
=== Result of InvGAN+KD: ===
0.8540367667627607
The source-target datasets are: computers_cameras with seed 1000
The F1 score is: 0.8540367667627607
The training time is: 498.59491300582886
The inference time is: 0.0002673342823982239
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: cameras
tgt: computers
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 138
tp 124
fp 113
recall 0.898550718126444
precision 0.523206748847229
f1 0.6613328646545491
tgt_res:
===== RES ====
p 573
tp 518
fp 441
recall 0.9040139600278988
precision 0.5401459848382211
f1 0.6762397397362621
save pretrained model to: checkpoint/cameras/bert/1000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/cameras/bert/1000/source-classifier.ptcomputersbest
===== RES ====
p 139
tp 128
fp 104
recall 0.9208633027276022
precision 0.5517241355529132
f1 0.6900264818770844
tgt_res:
===== RES ====
p 569
tp 523
fp 402
recall 0.9191564131473525
precision 0.5654054047941564
f1 0.7001333962618475
save pretrained model to: checkpoint/cameras/bert/1000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/cameras/bert/1000/source-classifier.ptcomputersbest
===== RES ====
p 139
tp 128
fp 105
recall 0.9208633027276022
precision 0.5493562208182137
f1 0.6881715712368773
===== RES ====
p 135
tp 122
fp 82
recall 0.9037036970096023
precision 0.5980392127547097
f1 0.7197635282675635
tgt_res:
===== RES ====
p 571
tp 507
fp 263
recall 0.8879159353976954
precision 0.6584415575864395
f1 0.7561516351630078
save pretrained model to: checkpoint/cameras/bert/1000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/cameras/bert/1000/source-classifier.ptcomputersbest
===== RES ====
p 138
tp 126
fp 80
recall 0.9130434716446125
precision 0.6116504824677161
f1 0.7325576548137276
tgt_res:
===== RES ====
p 571
tp 511
fp 276
recall 0.8949211893258824
precision 0.6493011427581943
f1 0.7525768311292174
save pretrained model to: checkpoint/cameras/bert/1000/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/cameras/bert/1000/source-classifier.ptcomputersbest
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/1000/source-encoder.ptcomputersbest
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/1000/source-classifier.ptcomputersbest
Pretraining time:  73.55725574493408
=== Training F' and A ===
===== RES ====
p 136
tp 117
fp 44
recall 0.8602941113213668
precision 0.7267080700204468
f1 0.7878782861162421
======== tgt result =======
===== RES ====
p 570
tp 489
fp 170
recall 0.8578947353370268
precision 0.7420333827890238
f1 0.7957684191467721
===== RES ====
p 140
tp 115
fp 28
recall 0.8214285655612246
precision 0.8041957985720574
f1 0.8127203423694219
======== tgt result =======
===== RES ====
p 569
tp 477
fp 116
recall 0.8383128280521743
precision 0.8043844843096383
f1 0.8209977776301254
===== RES ====
p 141
tp 119
fp 36
recall 0.8439716252200594
precision 0.7677419305306973
f1 0.8040535497400831
===== RES ====
p 142
tp 119
fp 32
recall 0.8380281631124777
precision 0.7880794649796062
f1 0.8122861843472421
===== RES ====
p 137
tp 116
fp 29
recall 0.8467153222867495
precision 0.7999999944827587
f1 0.8226945300289741
======== tgt result =======
===== RES ====
p 568
tp 480
fp 129
recall 0.8450704210474113
precision 0.7881773386072621
f1 0.8156324643867441
===== RES ====
p 139
tp 117
fp 40
recall 0.8417266126494488
precision 0.7452229251896629
f1 0.7905400370483419
===== RES ====
p 139
tp 117
fp 30
recall 0.8417266126494488
precision 0.7959183619325282
f1 0.8181813128517893
===== RES ====
p 134
tp 112
fp 27
recall 0.8358208892849187
precision 0.8057553898866519
f1 0.8205123146697622
===== RES ====
p 139
tp 118
fp 29
recall 0.8489208572020083
precision 0.8027210829746865
f1 0.8251743197958916
======== tgt result =======
===== RES ====
p 574
tp 495
fp 130
recall 0.8623693364767084
precision 0.7919999987328
f1 0.8256875729221347
===== RES ====
p 142
tp 118
fp 30
recall 0.8309859096409443
precision 0.7972972919101534
f1 0.8137925980502476
===== RES ====
p 139
tp 115
fp 27
recall 0.82733812354433
precision 0.809859149226344
f1 0.8185048323099267
===== RES ====
p 138
tp 116
fp 23
recall 0.8405797040537702
precision 0.8345323680968895
f1 0.8375446203133484
======== tgt result =======
===== RES ====
p 573
tp 487
fp 111
recall 0.8499127384818277
precision 0.8143812695411684
f1 0.8317672187051087
===== RES ====
p 140
tp 115
fp 27
recall 0.8214285655612246
precision 0.809859149226344
f1 0.8156023311204715
===== RES ====
p 141
tp 120
fp 31
recall 0.8510638237513204
precision 0.7947019814920399
f1 0.8219173031763214
===== RES ====
p 139
tp 115
fp 31
recall 0.82733812354433
precision 0.7876712274817039
f1 0.8070170384983089
===== RES ====
p 139
tp 116
fp 35
recall 0.8345323680968895
precision 0.7682119154423052
f1 0.7999994953391937
===== RES ====
p 139
tp 112
fp 23
recall 0.8057553898866519
precision 0.829629623484225
f1 0.8175177423147604
===== RES ====
p 140
tp 119
fp 33
recall 0.8499999939285715
precision 0.782894731691482
f1 0.815067988412766
===== RES ====
p 138
tp 115
fp 37
recall 0.833333327294686
precision 0.756578942390928
f1 0.7931029439717764
===== RES ====
p 136
tp 112
fp 26
recall 0.8235294057093426
precision 0.8115941970174334
f1 0.8175177422348415
===== RES ====
p 139
tp 120
fp 35
recall 0.863309346307127
precision 0.7741935433922997
f1 0.8163260265401723
===== RES ====
p 140
tp 117
fp 33
recall 0.835714279744898
precision 0.7799999948
f1 0.8068960467541736
===== RES ====
p 138
tp 112
fp 36
recall 0.8115941970174334
precision 0.7567567516435355
f1 0.7832162783513381
===== RES ====
p 137
tp 116
fp 30
recall 0.8467153222867495
precision 0.794520542503284
f1 0.8197874805781615
===== RES ====
p 138
tp 117
fp 36
recall 0.8478260808128545
precision 0.7647058773548635
f1 0.8041232071424074
===== RES ====
p 138
tp 114
fp 31
recall 0.8260869505356018
precision 0.7862068911296076
f1 0.8056532048599038
===== RES ====
p 140
tp 116
fp 39
recall 0.8285714226530613
precision 0.7483870919458897
f1 0.7864401739273488
===== RES ====
p 138
tp 114
fp 28
recall 0.8260869505356018
precision 0.8028168957548106
f1 0.8142852085717356
===== RES ====
p 139
tp 117
fp 42
recall 0.8417266126494488
precision 0.7358490519757921
f1 0.7852343963112924
===== RES ====
p 138
tp 115
fp 27
recall 0.833333327294686
precision 0.809859149226344
f1 0.8214280656635696
===== RES ====
p 138
tp 113
fp 29
recall 0.8188405737765176
precision 0.7957746422832772
f1 0.8071423514799017
===== RES ====
p 135
tp 107
fp 24
recall 0.7925925867215364
precision 0.8167938868947031
f1 0.804510772259907
===== RES ====
p 138
tp 113
fp 27
recall 0.8188405737765176
precision 0.807142851377551
f1 0.8129491344653974
===== RES ====
p 141
tp 117
fp 29
recall 0.8297872281575374
precision 0.801369857524864
f1 0.8153305049232752
===== RES ====
p 138
tp 116
fp 26
recall 0.8405797040537702
precision 0.8169014026978775
f1 0.8285709227554037
===== RES ====
p 141
tp 114
fp 21
recall 0.8085106325637544
precision 0.8444444381893005
f1 0.8260864507721991
===== RES ====
p 138
tp 113
fp 25
recall 0.8188405737765176
precision 0.8188405737765176
f1 0.8188400737768229
===== RES ====
p 137
tp 112
fp 35
recall 0.817518242207896
precision 0.7619047567217364
f1 0.7887318894319767
===== RES ====
p 142
tp 120
fp 34
recall 0.8450704165840112
precision 0.7792207741609041
f1 0.8108103061544345
===== RES ====
p 139
tp 119
fp 32
recall 0.8561151017545676
precision 0.7880794649796062
f1 0.8206891503689123
===== RES ====
p 300
tp 270
fp 119
recall 0.899999997
precision 0.6940874018146853
f1 0.783744063397546
=== Result of InvGAN+KD: ===
0.783744063397546
The source-target datasets are: cameras_computers with seed 1000
The F1 score is: 0.783744063397546
The training time is: 739.2571194171906
The inference time is: 0.00027050822973251343
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
usage: main_invgan_kd.py [-h] [--src SRC] [--tgt TGT] [--srcfix SRCFIX]
                         [--tgtfix TGTFIX] [--pretrain] [--adapt]
                         [--seed SEED] [--train_seed TRAIN_SEED] [--load]
                         [--model {bert}] [--max_seq_length MAX_SEQ_LENGTH]
                         [--alpha ALPHA] [--beta BETA]
                         [--temperature TEMPERATURE]
                         [--max_grad_norm MAX_GRAD_NORM]
                         [--clip_value CLIP_VALUE] [--batch_size BATCH_SIZE]
                         [--pre_epochs PRE_EPOCHS] [--epoch EPOCH]
                         [--pre_log_step PRE_LOG_STEP]
                         [--num_epochs NUM_EPOCHS] [--log_step LOG_STEP]
                         [--model_index MODEL_INDEX] [--out_file OUT_FILE]
                         [--d_learning_rate D_LEARNING_RATE]
                         [--rec_epoch REC_EPOCH] [--rec_lr REC_LR]
                         [--epoch_path EPOCH_PATH] [--adda ADDA]
                         [--seed_list SEED_LIST]
                         [--need_kd_model NEED_KD_MODEL]
                         [--need_pred_res NEED_PRED_RES]
main_invgan_kd.py: error: argument --src: expected one argument
usage: main_invgan_kd.py [-h] [--src SRC] [--tgt TGT] [--srcfix SRCFIX]
                         [--tgtfix TGTFIX] [--pretrain] [--adapt]
                         [--seed SEED] [--train_seed TRAIN_SEED] [--load]
                         [--model {bert}] [--max_seq_length MAX_SEQ_LENGTH]
                         [--alpha ALPHA] [--beta BETA]
                         [--temperature TEMPERATURE]
                         [--max_grad_norm MAX_GRAD_NORM]
                         [--clip_value CLIP_VALUE] [--batch_size BATCH_SIZE]
                         [--pre_epochs PRE_EPOCHS] [--epoch EPOCH]
                         [--pre_log_step PRE_LOG_STEP]
                         [--num_epochs NUM_EPOCHS] [--log_step LOG_STEP]
                         [--model_index MODEL_INDEX] [--out_file OUT_FILE]
                         [--d_learning_rate D_LEARNING_RATE]
                         [--rec_epoch REC_EPOCH] [--rec_lr REC_LR]
                         [--epoch_path EPOCH_PATH] [--adda ADDA]
                         [--seed_list SEED_LIST]
                         [--need_kd_model NEED_KD_MODEL]
                         [--need_pred_res NEED_PRED_RES]
main_invgan_kd.py: error: argument --src: expected one argument
=== Argument Setting ===
src: computers
tgt: watches
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 116
tp 100
fp 64
recall 0.8620689580856125
precision 0.6097560938429507
f1 0.7142852238778807
tgt_res:
===== RES ====
p 461
tp 391
fp 317
recall 0.8481561803727632
precision 0.5522598862256216
f1 0.6689473398263714
save pretrained model to: checkpoint/computers/bert/42/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/computers/bert/42/source-classifier.ptwatchesbest
===== RES ====
p 115
tp 107
fp 86
recall 0.9304347745179585
precision 0.5544041422051599
f1 0.6948047223607465
===== RES ====
p 115
tp 94
fp 39
recall 0.8173912972400756
precision 0.7067669119791962
f1 0.7580640126499102
tgt_res:
===== RES ====
p 461
tp 366
fp 174
recall 0.7939262455663205
precision 0.6777777765226337
f1 0.731268232922261
save pretrained model to: checkpoint/computers/bert/42/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/computers/bert/42/source-classifier.ptwatchesbest
===== RES ====
p 116
tp 107
fp 80
recall 0.9224137851516053
precision 0.572192510309131
f1 0.7062701498549052
===== RES ====
p 116
tp 105
fp 66
recall 0.9051724059898931
precision 0.6140350841284498
f1 0.7317068303369726
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/42/source-encoder.ptwatchesbest
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/42/source-classifier.ptwatchesbest
Pretraining time:  86.19098973274231
=== Training F' and A ===
===== RES ====
p 114
tp 82
fp 24
recall 0.7192982393044014
precision 0.7735848983624066
f1 0.7454540393391774
======== tgt result =======
===== RES ====
p 463
tp 324
fp 100
recall 0.6997840157672052
precision 0.7641509415939837
f1 0.7305519232204983
===== RES ====
p 116
tp 82
fp 23
recall 0.7068965456302022
precision 0.7809523735147393
f1 0.7420809424871804
===== RES ====
p 115
tp 82
fp 24
recall 0.7130434720604916
precision 0.7735848983624066
f1 0.7420809420776889
===== RES ====
p 116
tp 83
fp 24
recall 0.7155172352110584
precision 0.7757009273298979
f1 0.7443941129726448
===== RES ====
p 115
tp 81
fp 24
recall 0.7043478199621929
precision 0.7714285640816327
f1 0.7363631307028176
===== RES ====
p 114
tp 80
fp 25
recall 0.7017543798091721
precision 0.7619047546485261
f1 0.7305931014786257
===== RES ====
p 115
tp 83
fp 27
recall 0.7217391241587903
precision 0.7545454476859504
f1 0.7377772714670053
===== RES ====
p 116
tp 83
fp 27
recall 0.7155172352110584
precision 0.7545454476859504
f1 0.7345127681889217
===== RES ====
p 116
tp 83
fp 25
recall 0.7155172352110584
precision 0.7685185114026064
f1 0.7410709225928113
===== RES ====
p 116
tp 85
fp 29
recall 0.7327586143727706
precision 0.7456140285472453
f1 0.739129928393533
===== RES ====
p 115
tp 84
fp 27
recall 0.730434776257089
precision 0.7567567499391284
f1 0.7433623254369406
===== RES ====
p 114
tp 84
fp 27
recall 0.7368420987996307
precision 0.7567567499391284
f1 0.7466661601188533
======== tgt result =======
===== RES ====
p 462
tp 335
fp 116
recall 0.7251082235387268
precision 0.7427937899272865
f1 0.7338439672496025
===== RES ====
p 116
tp 86
fp 28
recall 0.7413793039536267
precision 0.75438595829486
f1 0.7478255804918277
======== tgt result =======
===== RES ====
p 457
tp 337
fp 118
recall 0.7374179414936151
precision 0.7406593390315179
f1 0.7390345861013502
===== RES ====
p 116
tp 84
fp 27
recall 0.7241379247919144
precision 0.7567567499391284
f1 0.7400875994491918
===== RES ====
p 115
tp 86
fp 27
recall 0.7478260804536863
precision 0.7610619401675934
f1 0.7543854583336649
======== tgt result =======
===== RES ====
p 461
tp 339
fp 125
recall 0.7353579159753625
precision 0.7306034467012856
f1 0.732972471393767
===== RES ====
p 115
tp 88
fp 29
recall 0.7652173846502837
precision 0.752136745708233
f1 0.7586201831528268
======== tgt result =======
===== RES ====
p 461
tp 348
fp 137
recall 0.7548806925056818
precision 0.7175257717164417
f1 0.7357288856588817
===== RES ====
p 116
tp 84
fp 29
recall 0.7241379247919144
precision 0.743362825279975
f1 0.7336239478274226
===== RES ====
p 115
tp 83
fp 30
recall 0.7217391241587903
precision 0.7345132678361658
f1 0.7280696690908328
===== RES ====
p 114
tp 83
fp 28
recall 0.728070169052016
precision 0.7477477410112816
f1 0.7377772713089809
===== RES ====
p 115
tp 83
fp 28
recall 0.7217391241587903
precision 0.7477477410112816
f1 0.7345127679931353
===== RES ====
p 116
tp 87
fp 27
recall 0.7499999935344829
precision 0.7631578880424746
f1 0.7565212325901226
===== RES ====
p 116
tp 84
fp 28
recall 0.7241379247919144
precision 0.7499999933035715
f1 0.7368415989538633
===== RES ====
p 114
tp 82
fp 29
recall 0.7192982393044014
precision 0.7387387320834349
f1 0.7288883824991085
===== RES ====
p 115
tp 84
fp 28
recall 0.730434776257089
precision 0.7499999933035715
f1 0.7400875992939395
===== RES ====
p 115
tp 84
fp 26
recall 0.730434776257089
precision 0.7636363566942149
f1 0.7466661602768778
===== RES ====
p 115
tp 87
fp 30
recall 0.756521732551985
precision 0.7435897372342758
f1 0.7499994935719744
===== RES ====
p 115
tp 83
fp 28
recall 0.7217391241587903
precision 0.7477477410112816
f1 0.7345127679931353
===== RES ====
p 114
tp 82
fp 27
recall 0.7192982393044014
precision 0.7522935710798755
f1 0.735425502624563
===== RES ====
p 115
tp 83
fp 27
recall 0.7217391241587903
precision 0.7545454476859504
f1 0.7377772714670053
===== RES ====
p 114
tp 83
fp 27
recall 0.728070169052016
precision 0.7545454476859504
f1 0.7410709221144954
===== RES ====
p 114
tp 82
fp 27
recall 0.7192982393044014
precision 0.7522935710798755
f1 0.735425502624563
===== RES ====
p 115
tp 87
fp 29
recall 0.756521732551985
precision 0.7499999935344829
f1 0.7532462467348385
===== RES ====
p 115
tp 83
fp 25
recall 0.7217391241587903
precision 0.7685185114026064
f1 0.7443941126509015
===== RES ====
p 116
tp 84
fp 27
recall 0.7241379247919144
precision 0.7567567499391284
f1 0.7400875994491918
===== RES ====
p 116
tp 84
fp 27
recall 0.7241379247919144
precision 0.7567567499391284
f1 0.7400875994491918
===== RES ====
p 116
tp 84
fp 29
recall 0.7241379247919144
precision 0.743362825279975
f1 0.7336239478274226
===== RES ====
p 116
tp 87
fp 30
recall 0.7499999935344829
precision 0.7435897372342758
f1 0.7467806094792349
===== RES ====
p 115
tp 86
fp 30
recall 0.7478260804536863
precision 0.7413793039536267
f1 0.7445882381517949
===== RES ====
p 116
tp 83
fp 27
recall 0.7155172352110584
precision 0.7545454476859504
f1 0.7345127681889217
===== RES ====
p 115
tp 88
fp 29
recall 0.7652173846502837
precision 0.752136745708233
f1 0.7586201831528268
===== RES ====
p 300
tp 256
fp 109
recall 0.8533333304888889
precision 0.7013698610921374
f1 0.7699243144918104
=== Result of InvGAN+KD: ===
0.7699243144918104
The source-target datasets are: computers_watches with seed 42
The F1 score is: 0.7699243144918104
The training time is: 591.502370595932
The inference time is: 0.00026977062225341797
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: watches
tgt: computers
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 139
tp 117
fp 65
recall 0.8417266126494488
precision 0.6428571393249608
f1 0.7289714670474284
tgt_res:
===== RES ====
p 572
tp 496
fp 272
recall 0.8671328656169006
precision 0.6458333324924045
f1 0.7402980170553469
save pretrained model to: checkpoint/watches/bert/42/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/watches/bert/42/source-classifier.ptcomputersbest
===== RES ====
p 137
tp 107
fp 27
recall 0.7810218921093293
precision 0.7985074567275563
f1 0.7896673909127497
tgt_res:
===== RES ====
p 569
tp 456
fp 111
recall 0.8014059739869842
precision 0.8042328028144042
f1 0.8028163999969019
save pretrained model to: checkpoint/watches/bert/42/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/watches/bert/42/source-classifier.ptcomputersbest
===== RES ====
p 137
tp 114
fp 81
recall 0.8321167822473228
precision 0.584615381617357
f1 0.6867464990749538
===== RES ====
p 140
tp 120
fp 80
recall 0.8571428510204082
precision 0.5999999970000001
f1 0.705881864360194
===== RES ====
p 139
tp 109
fp 30
recall 0.7841726562289737
precision 0.7841726562289737
f1 0.7841721562292925
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/42/source-encoder.ptcomputersbest
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/42/source-classifier.ptcomputersbest
Pretraining time:  76.70137405395508
=== Training F' and A ===
===== RES ====
p 140
tp 114
fp 31
recall 0.8142857084693879
precision 0.7862068911296076
f1 0.7999994945401709
======== tgt result =======
===== RES ====
p 571
tp 460
fp 112
recall 0.8056042017414988
precision 0.8041958027898675
f1 0.8048988861688484
===== RES ====
p 137
tp 112
fp 42
recall 0.817518242207896
precision 0.7272727225501772
f1 0.769758946588105
===== RES ====
p 141
tp 116
fp 31
recall 0.8226950296262764
precision 0.7891156408903698
f1 0.8055550501787437
======== tgt result =======
===== RES ====
p 569
tp 477
fp 138
recall 0.8383128280521743
precision 0.7756097548364069
f1 0.8057427426372137
===== RES ====
p 137
tp 85
fp 5
recall 0.6204379516756354
precision 0.9444444339506174
f1 0.7488981932507082
===== RES ====
p 133
tp 104
fp 20
recall 0.7819548813386851
precision 0.8387096706555672
f1 0.8093380157159134
======== tgt result =======
===== RES ====
p 574
tp 460
fp 82
recall 0.8013937268268402
precision 0.848708485518988
f1 0.8243722587906575
===== RES ====
p 137
tp 109
fp 28
recall 0.795620432148756
precision 0.795620432148756
f1 0.7956199321490702
===== RES ====
p 140
tp 114
fp 34
recall 0.8142857084693879
precision 0.7702702650657415
f1 0.7916661615550993
===== RES ====
p 137
tp 105
fp 19
recall 0.7664233520699025
precision 0.8467741867195631
f1 0.8045971962246748
===== RES ====
p 138
tp 112
fp 28
recall 0.8115941970174334
precision 0.7999999942857143
f1 0.8057548899128407
===== RES ====
p 139
tp 109
fp 23
recall 0.7841726562289737
precision 0.8257575695018367
f1 0.804427538677616
===== RES ====
p 140
tp 111
fp 23
recall 0.7928571371938776
precision 0.8283582027734463
f1 0.810218472428248
======== tgt result =======
===== RES ====
p 570
tp 466
fp 107
recall 0.8175438582148353
precision 0.813263523886102
f1 0.8153975738175777
===== RES ====
p 139
tp 111
fp 22
recall 0.7985611453340925
precision 0.8345864598903274
f1 0.8161759648305397
======== tgt result =======
===== RES ====
p 571
tp 468
fp 98
recall 0.8196147095978726
precision 0.8268551222140369
f1 0.823218495923396
===== RES ====
p 140
tp 107
fp 20
recall 0.7642857088265307
precision 0.8425196784053569
f1 0.8014976225227194
===== RES ====
p 141
tp 107
fp 21
recall 0.7588652428449274
precision 0.8359374934692384
f1 0.7955385287105298
===== RES ====
p 138
tp 113
fp 27
recall 0.8188405737765176
precision 0.807142851377551
f1 0.8129491344653974
===== RES ====
p 141
tp 106
fp 20
recall 0.7517730443136663
precision 0.8412698345930966
f1 0.7940069862674789
===== RES ====
p 137
tp 109
fp 26
recall 0.795620432148756
precision 0.8074074014266118
f1 0.8014700823694729
===== RES ====
p 137
tp 109
fp 24
recall 0.795620432148756
precision 0.8195488660184296
f1 0.8074069015366608
===== RES ====
p 137
tp 109
fp 29
recall 0.795620432148756
precision 0.7898550667401807
f1 0.7927267669689105
===== RES ====
p 141
tp 112
fp 35
recall 0.7943262355012324
precision 0.7619047567217364
f1 0.7777772725938784
===== RES ====
p 135
tp 107
fp 23
recall 0.7925925867215364
precision 0.8230769167455622
f1 0.8075466638949338
===== RES ====
p 137
tp 107
fp 22
recall 0.7810218921093293
precision 0.8294573579111832
f1 0.8045107725991005
===== RES ====
p 133
tp 105
fp 22
recall 0.789473678274634
precision 0.8267716470332941
f1 0.8076918017458714
===== RES ====
p 137
tp 108
fp 25
recall 0.7883211621290426
precision 0.8120300690824807
f1 0.7999994941841259
===== RES ====
p 141
tp 113
fp 32
recall 0.8014184340324934
precision 0.7793103394530322
f1 0.7902092847819682
===== RES ====
p 142
tp 114
fp 31
recall 0.8028168957548106
precision 0.7862068911296076
f1 0.7944245816268976
===== RES ====
p 136
tp 107
fp 27
recall 0.7867647000973184
precision 0.7985074567275563
f1 0.7925920867492866
===== RES ====
p 138
tp 109
fp 26
recall 0.7898550667401807
precision 0.8074074014266118
f1 0.798534292745419
===== RES ====
p 137
tp 105
fp 22
recall 0.7664233520699025
precision 0.8267716470332941
f1 0.7954540401460894
===== RES ====
p 142
tp 117
fp 35
recall 0.8239436561694109
precision 0.769736837041205
f1 0.7959178625113041
===== RES ====
p 138
tp 102
fp 11
recall 0.7391304294265911
precision 0.902654859268541
f1 0.8127485024685164
===== RES ====
p 140
tp 114
fp 28
recall 0.8142857084693879
precision 0.8028168957548106
f1 0.8085101325892132
===== RES ====
p 137
tp 111
fp 30
recall 0.8102189721881827
precision 0.7872340369699714
f1 0.7985606454379198
===== RES ====
p 140
tp 111
fp 26
recall 0.7928571371938776
precision 0.8102189721881827
f1 0.8014435375936599
===== RES ====
p 136
tp 109
fp 30
recall 0.8014705823421281
precision 0.7841726562289737
f1 0.7927267670218031
===== RES ====
p 138
tp 110
fp 22
recall 0.7971014434992649
precision 0.8333333270202021
f1 0.8148143090263698
===== RES ====
p 141
tp 111
fp 17
recall 0.7872340369699714
precision 0.8671874932250977
f1 0.8252783054410777
======== tgt result =======
===== RES ====
p 573
tp 470
fp 111
recall 0.8202443266662403
precision 0.8089500846661789
f1 0.8145575575381002
===== RES ====
p 136
tp 102
fp 15
recall 0.7499999944852942
precision 0.8717948643436336
f1 0.8063236071180556
===== RES ====
p 132
tp 105
fp 20
recall 0.7954545394283747
precision 0.8399999932800001
f1 0.8171201165804203
===== RES ====
p 138
tp 105
fp 18
recall 0.7608695597038437
precision 0.8536585296450526
f1 0.8045971966357075
===== RES ====
p 300
tp 262
fp 89
recall 0.8733333304222223
precision 0.7464387443121403
f1 0.8049150151890393
=== Result of InvGAN+KD: ===
0.8049150151890393
The source-target datasets are: watches_computers with seed 42
The F1 score is: 0.8049150151890393
The training time is: 738.7189910411835
The inference time is: 0.0002667233347892761
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: cameras
tgt: watches
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 116
tp 92
fp 76
recall 0.7931034414387634
precision 0.5476190443594104
f1 0.6478868361439826
tgt_res:
===== RES ====
p 462
tp 368
fp 325
recall 0.7965367948126909
precision 0.5310245302582619
f1 0.6372289561263712
save pretrained model to: checkpoint/cameras/bert/42/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/cameras/bert/42/source-classifier.ptwatchesbest
===== RES ====
p 113
tp 96
fp 89
recall 0.8495575146056857
precision 0.5189189161139518
f1 0.6442948268775114
===== RES ====
p 115
tp 96
fp 83
recall 0.834782601436673
precision 0.5363128461658501
f1 0.6530607437413811
tgt_res:
===== RES ====
p 461
tp 386
fp 367
recall 0.8373101934114746
precision 0.5126162011784645
f1 0.6359138606635615
save pretrained model to: checkpoint/cameras/bert/42/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/cameras/bert/42/source-classifier.ptwatchesbest
===== RES ====
p 116
tp 105
fp 104
recall 0.9051724059898931
precision 0.5023923420938166
f1 0.6461533831198524
===== RES ====
p 115
tp 100
fp 64
recall 0.8695652098298677
precision 0.6097560938429507
f1 0.7168453884203116
tgt_res:
===== RES ====
p 463
tp 382
fp 281
recall 0.8250539938983715
precision 0.5761689282410725
f1 0.6785075074647978
save pretrained model to: checkpoint/cameras/bert/42/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/cameras/bert/42/source-classifier.ptwatchesbest
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/42/source-encoder.ptwatchesbest
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/42/source-classifier.ptwatchesbest
Pretraining time:  66.64022183418274
=== Training F' and A ===
===== RES ====
p 116
tp 92
fp 31
recall 0.7931034414387634
precision 0.7479674735937604
f1 0.7698739709742217
======== tgt result =======
===== RES ====
p 461
tp 341
fp 132
recall 0.7396963107598779
precision 0.7209302310339741
f1 0.7301922180053769
===== RES ====
p 116
tp 85
fp 25
recall 0.7327586143727706
precision 0.7727272657024794
f1 0.7522118830765321
===== RES ====
p 116
tp 90
fp 33
recall 0.7758620622770512
precision 0.7317073111243309
f1 0.7531375694406425
===== RES ====
p 115
tp 88
fp 31
recall 0.7652173846502837
precision 0.7394957921050774
f1 0.7521362458546679
===== RES ====
p 115
tp 89
fp 33
recall 0.7739130367485824
precision 0.7295081907417361
f1 0.7510543464191755
===== RES ====
p 114
tp 90
fp 34
recall 0.7894736772853186
precision 0.7258064457596255
f1 0.7563020155359556
===== RES ====
p 115
tp 85
fp 28
recall 0.7391304283553876
precision 0.7522123827237842
f1 0.745613528586054
===== RES ====
p 114
tp 86
fp 29
recall 0.75438595829486
precision 0.7478260804536863
f1 0.7510911965068832
===== RES ====
p 115
tp 82
fp 24
recall 0.7130434720604916
precision 0.7735848983624066
f1 0.7420809420776889
===== RES ====
p 116
tp 90
fp 32
recall 0.7758620622770512
precision 0.7377049119860253
f1 0.7563020149710243
===== RES ====
p 116
tp 88
fp 28
recall 0.7586206831153389
precision 0.7586206831153389
f1 0.7586201831156686
===== RES ====
p 116
tp 90
fp 31
recall 0.7758620622770512
precision 0.7438016467454409
f1 0.7594931646997183
===== RES ====
p 115
tp 87
fp 28
recall 0.756521732551985
precision 0.756521732551985
f1 0.7565212325523154
===== RES ====
p 114
tp 88
fp 34
recall 0.7719298177900893
precision 0.721311469497447
f1 0.7457622061192659
===== RES ====
p 116
tp 87
fp 27
recall 0.7499999935344829
precision 0.7631578880424746
f1 0.7565212325901226
===== RES ====
p 115
tp 87
fp 27
recall 0.756521732551985
precision 0.7631578880424746
f1 0.7598248208847517
===== RES ====
p 114
tp 86
fp 36
recall 0.75438595829486
precision 0.7049180270088686
f1 0.7288130537205376
===== RES ====
p 116
tp 89
fp 30
recall 0.7672413726961951
precision 0.7478991533789987
f1 0.7574463021460973
===== RES ====
p 116
tp 88
fp 27
recall 0.7586206831153389
precision 0.7652173846502837
f1 0.7619042553178823
===== RES ====
p 116
tp 89
fp 28
recall 0.7672413726961951
precision 0.7606837541821901
f1 0.7639479913061167
===== RES ====
p 115
tp 87
fp 25
recall 0.756521732551985
precision 0.7767857073501276
f1 0.7665193171227233
===== RES ====
p 115
tp 89
fp 32
recall 0.7739130367485824
precision 0.735537184003825
f1 0.7542367820672661
===== RES ====
p 116
tp 90
fp 35
recall 0.7758620622770512
precision 0.71999999424
f1 0.7468874613043749
===== RES ====
p 114
tp 84
fp 26
recall 0.7368420987996307
precision 0.7636363566942149
f1 0.7499994934633435
===== RES ====
p 116
tp 88
fp 29
recall 0.7586206831153389
precision 0.752136745708233
f1 0.7553643003926757
===== RES ====
p 115
tp 91
fp 37
recall 0.7913043409451797
precision 0.7109374944458008
f1 0.7489706886826127
===== RES ====
p 115
tp 88
fp 39
recall 0.7652173846502837
precision 0.6929133803707608
f1 0.7272722224919752
===== RES ====
p 116
tp 86
fp 30
recall 0.7413793039536267
precision 0.7413793039536267
f1 0.7413788039539639
===== RES ====
p 116
tp 90
fp 38
recall 0.7758620622770512
precision 0.703124994506836
f1 0.7377044131957149
===== RES ====
p 116
tp 91
fp 42
recall 0.7844827518579073
precision 0.6842105211713495
f1 0.7309231912391898
===== RES ====
p 116
tp 89
fp 40
recall 0.7672413726961951
precision 0.6899224752719189
f1 0.7265301077221249
===== RES ====
p 114
tp 88
fp 39
recall 0.7719298177900893
precision 0.6929133803707608
f1 0.7302899518262388
===== RES ====
p 116
tp 94
fp 45
recall 0.8103448206004757
precision 0.6762589879405828
f1 0.7372544002463928
===== RES ====
p 116
tp 89
fp 43
recall 0.7672413726961951
precision 0.6742424191345271
f1 0.7177414317771404
===== RES ====
p 115
tp 87
fp 33
recall 0.756521732551985
precision 0.7249999939583334
f1 0.7404250258400839
===== RES ====
p 114
tp 87
fp 42
recall 0.7631578880424746
precision 0.6744185994231117
f1 0.7160488787281828
===== RES ====
p 115
tp 90
fp 40
recall 0.782608688846881
precision 0.6923076869822485
f1 0.7346933734280762
===== RES ====
p 116
tp 86
fp 27
recall 0.7413793039536267
precision 0.7610619401675934
f1 0.7510911965831593
===== RES ====
p 116
tp 88
fp 35
recall 0.7586206831153389
precision 0.7154471486549012
f1 0.7364011679070633
===== RES ====
p 115
tp 88
fp 34
recall 0.7652173846502837
precision 0.721311469497447
f1 0.7426155279249921
===== RES ====
p 300
tp 261
fp 140
recall 0.8699999971
precision 0.650872816331988
f1 0.7446500075420242
=== Result of InvGAN+KD: ===
0.7446500075420242
The source-target datasets are: cameras_watches with seed 42
The F1 score is: 0.7446500075420242
The training time is: 582.5452134609222
The inference time is: 0.0002683475613594055
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: watches
tgt: cameras
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 92
tp 71
fp 38
recall 0.7717391220463139
precision 0.6513761408130629
f1 0.7064671582390063
tgt_res:
===== RES ====
p 389
tp 301
fp 144
recall 0.7737789183193344
precision 0.6764044928620124
f1 0.7218220424900925
save pretrained model to: checkpoint/watches/bert/42/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/watches/bert/42/source-classifier.ptcamerasbest
===== RES ====
p 90
tp 68
fp 26
recall 0.7555555471604939
precision 0.7234042476233591
f1 0.7391299269852152
tgt_res:
===== RES ====
p 389
tp 290
fp 79
recall 0.745501283430588
precision 0.7859078569487592
f1 0.7651710022872777
save pretrained model to: checkpoint/watches/bert/42/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/watches/bert/42/source-classifier.ptcamerasbest
===== RES ====
p 89
tp 59
fp 24
recall 0.6629213408660524
precision 0.710843364929598
f1 0.6860460042594225
===== RES ====
p 93
tp 69
fp 26
recall 0.741935475893167
precision 0.726315781828255
f1 0.7340420454394534
===== RES ====
p 89
tp 69
fp 29
recall 0.7752808901653833
precision 0.7040816254685549
f1 0.7379674077043037
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/42/source-encoder.ptcamerasbest
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/42/source-classifier.ptcamerasbest
Pretraining time:  73.81829166412354
=== Training F' and A ===
===== RES ====
p 94
tp 69
fp 23
recall 0.7340425453825261
precision 0.7499999918478262
f1 0.741934975951314
======== tgt result =======
===== RES ====
p 388
tp 275
fp 78
recall 0.7087628847712297
precision 0.7790368249885642
f1 0.7422397150369147
===== RES ====
p 89
tp 68
fp 30
recall 0.7640449352354501
precision 0.6938775439400251
f1 0.7272722206529203
===== RES ====
p 89
tp 72
fp 36
recall 0.8089887549551825
precision 0.6666666604938273
f1 0.7309639642354359
===== RES ====
p 89
tp 58
fp 25
recall 0.6516853859361192
precision 0.6987951723036726
f1 0.6744180974178928
===== RES ====
p 87
tp 70
fp 39
recall 0.804597691901176
precision 0.6422018289706254
f1 0.7142852132968844
===== RES ====
p 94
tp 61
fp 22
recall 0.6489361633091898
precision 0.7349397501814489
f1 0.6892650308663308
===== RES ====
p 88
tp 64
fp 31
recall 0.7272727190082646
precision 0.6736842034349031
f1 0.6994530450002072
===== RES ====
p 90
tp 71
fp 36
recall 0.7888888801234569
precision 0.663551395667744
f1 0.7208116791469314
===== RES ====
p 91
tp 68
fp 30
recall 0.7472527390411787
precision 0.6938775439400251
f1 0.7195762126483687
===== RES ====
p 95
tp 77
fp 40
recall 0.8105263072576179
precision 0.6581196524947038
f1 0.7264145928714654
===== RES ====
p 89
tp 58
fp 28
recall 0.6516853859361192
precision 0.6744185968090861
f1 0.6628566354289485
===== RES ====
p 88
tp 65
fp 34
recall 0.7386363552427687
precision 0.6565656499336803
f1 0.6951866600707053
===== RES ====
p 88
tp 65
fp 31
recall 0.7386363552427687
precision 0.677083326280382
f1 0.7065212323963829
===== RES ====
p 90
tp 68
fp 32
recall 0.7555555471604939
precision 0.6799999932
f1 0.7157889675349733
===== RES ====
p 91
tp 60
fp 25
recall 0.6593406520951577
precision 0.7058823446366783
f1 0.681817674651709
===== RES ====
p 85
tp 64
fp 31
recall 0.7529411676124569
precision 0.6736842034349031
f1 0.7111106047534359
===== RES ====
p 89
tp 64
fp 29
recall 0.7191011155157179
precision 0.6881720356110534
f1 0.7032961958100401
===== RES ====
p 88
tp 70
fp 33
recall 0.7954545364152894
precision 0.6796116438872656
f1 0.732983788602623
===== RES ====
p 92
tp 69
fp 31
recall 0.7499999918478262
precision 0.6899999931
f1 0.7187494933814231
===== RES ====
p 90
tp 71
fp 30
recall 0.7888888801234569
precision 0.7029702900696011
f1 0.7434549912560562
======== tgt result =======
===== RES ====
p 388
tp 287
fp 116
recall 0.7396907197430651
precision 0.712158807165859
f1 0.7256632151595068
===== RES ====
p 92
tp 69
fp 31
recall 0.7499999918478262
precision 0.6899999931
f1 0.7187494933814231
===== RES ====
p 87
tp 62
fp 30
recall 0.712643669969613
precision 0.6739130361531191
f1 0.6927369228180004
===== RES ====
p 92
tp 72
fp 33
recall 0.7826086871455578
precision 0.6857142791836736
f1 0.7309639617617861
===== RES ====
p 90
tp 64
fp 29
recall 0.7111111032098767
precision 0.6881720356110534
f1 0.699453044402997
===== RES ====
p 90
tp 61
fp 27
recall 0.6777777702469137
precision 0.6931818103047521
f1 0.6853927507894065
===== RES ====
p 90
tp 63
fp 26
recall 0.6999999922222223
precision 0.7078651605857847
f1 0.7039101066761769
===== RES ====
p 90
tp 69
fp 36
recall 0.7666666581481483
precision 0.6571428508843539
f1 0.7076918033928541
===== RES ====
p 92
tp 68
fp 31
recall 0.7391304267485823
precision 0.6868686799306194
f1 0.7120413780327507
===== RES ====
p 93
tp 64
fp 26
recall 0.6881720356110534
precision 0.7111111032098767
f1 0.699453044402997
===== RES ====
p 93
tp 74
fp 31
recall 0.7956989161752805
precision 0.7047618980498866
f1 0.7474742417613769
======== tgt result =======
===== RES ====
p 389
tp 290
fp 122
recall 0.745501283430588
precision 0.703883493437176
f1 0.7240943800028696
===== RES ====
p 91
tp 70
fp 30
recall 0.7692307607776839
precision 0.699999993
f1 0.7329837866289958
===== RES ====
p 92
tp 60
fp 31
recall 0.6521739059546314
precision 0.6593406520951577
f1 0.655737197766812
===== RES ====
p 92
tp 59
fp 25
recall 0.6413043408553876
precision 0.7023809440192745
f1 0.6704540388691731
===== RES ====
p 92
tp 66
fp 32
recall 0.7173912965500946
precision 0.6734693808829655
f1 0.6947363352912179
===== RES ====
p 95
tp 65
fp 26
recall 0.6842105191135736
precision 0.7142857064364209
f1 0.6989242238990742
===== RES ====
p 89
tp 62
fp 27
recall 0.6966292056558516
precision 0.6966292056558516
f1 0.6966287056562106
===== RES ====
p 89
tp 47
fp 26
recall 0.5280898817068552
precision 0.6438356076186903
f1 0.5802464112944326
===== RES ====
p 91
tp 66
fp 31
recall 0.7252747173046734
precision 0.6804123641194602
f1 0.7021271526146605
===== RES ====
p 90
tp 66
fp 28
recall 0.7333333251851853
precision 0.702127652105025
f1 0.7173907967867377
===== RES ====
p 82
tp 61
fp 31
recall 0.7439024299524094
precision 0.6630434710538753
f1 0.701148918879995
===== RES ====
p 300
tp 263
fp 109
recall 0.8766666637444445
precision 0.7069892454113192
f1 0.7827375986486256
=== Result of InvGAN+KD: ===
0.7827375986486256
The source-target datasets are: watches_cameras with seed 42
The F1 score is: 0.7827375986486256
The training time is: 491.5094828605652
The inference time is: 0.00027042627334594727
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: shoes
tgt: watches
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 116
tp 83
fp 30
recall 0.7155172352110584
precision 0.7345132678361658
f1 0.7248903234495544
tgt_res:
===== RES ====
p 460
tp 306
fp 114
recall 0.6652173898582231
precision 0.7285714268367347
f1 0.6954540449073829
save pretrained model to: checkpoint/shoes/bert/42/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/shoes/bert/42/source-classifier.ptwatchesbest
===== RES ====
p 115
tp 87
fp 35
recall 0.756521732551985
precision 0.7131147482531578
f1 0.7341767094308088
tgt_res:
===== RES ====
p 462
tp 337
fp 146
recall 0.729437227858361
precision 0.6977225658432245
f1 0.7132270119653008
save pretrained model to: checkpoint/shoes/bert/42/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/shoes/bert/42/source-classifier.ptwatchesbest
===== RES ====
p 116
tp 91
fp 33
recall 0.7844827518579073
precision 0.7338709618236213
f1 0.7583328275697735
tgt_res:
===== RES ====
p 461
tp 331
fp 141
recall 0.7180043368373008
precision 0.7012711849549339
f1 0.7095386196635531
save pretrained model to: checkpoint/shoes/bert/42/source-encoder.ptwatchesbest
save pretrained model to: checkpoint/shoes/bert/42/source-classifier.ptwatchesbest
===== RES ====
p 115
tp 87
fp 34
recall 0.756521732551985
precision 0.7190082585205929
f1 0.7372876296685376
===== RES ====
p 115
tp 85
fp 28
recall 0.7391304283553876
precision 0.7522123827237842
f1 0.745613528586054
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/42/source-encoder.ptwatchesbest
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/42/source-classifier.ptwatchesbest
Pretraining time:  74.91273784637451
=== Training F' and A ===
===== RES ====
p 116
tp 88
fp 29
recall 0.7586206831153389
precision 0.752136745708233
f1 0.7553643003926757
======== tgt result =======
===== RES ====
p 462
tp 321
fp 128
recall 0.6948051933012874
precision 0.7149220474055188
f1 0.7047195863706236
===== RES ====
p 115
tp 87
fp 29
recall 0.756521732551985
precision 0.7499999935344829
f1 0.7532462467348385
===== RES ====
p 114
tp 84
fp 29
recall 0.7368420987996307
precision 0.743362825279975
f1 0.7400875992163133
===== RES ====
p 114
tp 85
fp 27
recall 0.7456140285472453
precision 0.7589285646524235
f1 0.7522118827632739
===== RES ====
p 115
tp 87
fp 31
recall 0.756521732551985
precision 0.7372881293450159
f1 0.7467806095529145
===== RES ====
p 115
tp 88
fp 31
recall 0.7652173846502837
precision 0.7394957921050774
f1 0.7521362458546679
===== RES ====
p 115
tp 87
fp 30
recall 0.756521732551985
precision 0.7435897372342758
f1 0.7499994935719744
===== RES ====
p 115
tp 87
fp 31
recall 0.756521732551985
precision 0.7372881293450159
f1 0.7467806095529145
===== RES ====
p 115
tp 87
fp 36
recall 0.756521732551985
precision 0.7073170674201864
f1 0.731091931396429
===== RES ====
p 116
tp 89
fp 32
recall 0.7672413726961951
precision 0.735537184003825
f1 0.7510543462055348
===== RES ====
p 114
tp 85
fp 31
recall 0.7456140285472453
precision 0.7327586143727706
f1 0.739129928393533
===== RES ====
p 115
tp 85
fp 32
recall 0.7391304283553876
precision 0.7264957202863613
f1 0.7327581144102698
===== RES ====
p 116
tp 88
fp 31
recall 0.7586206831153389
precision 0.7394957921050774
f1 0.7489356639206598
===== RES ====
p 116
tp 86
fp 30
recall 0.7413793039536267
precision 0.7413793039536267
f1 0.7413788039539639
===== RES ====
p 115
tp 84
fp 27
recall 0.730434776257089
precision 0.7567567499391284
f1 0.7433623254369406
===== RES ====
p 116
tp 89
fp 32
recall 0.7672413726961951
precision 0.735537184003825
f1 0.7510543462055348
===== RES ====
p 116
tp 88
fp 29
recall 0.7586206831153389
precision 0.752136745708233
f1 0.7553643003926757
===== RES ====
p 116
tp 88
fp 32
recall 0.7586206831153389
precision 0.7333333272222223
f1 0.7457622056883558
===== RES ====
p 114
tp 87
fp 36
recall 0.7631578880424746
precision 0.7073170674201864
f1 0.7341767097156631
===== RES ====
p 116
tp 91
fp 42
recall 0.7844827518579073
precision 0.6842105211713495
f1 0.7309231912391898
===== RES ====
p 116
tp 87
fp 33
recall 0.7499999935344829
precision 0.7249999939583334
f1 0.7372876294889917
===== RES ====
p 114
tp 89
fp 34
recall 0.7807017475377039
precision 0.723577229889616
f1 0.7510543467040298
===== RES ====
p 115
tp 85
fp 30
recall 0.7391304283553876
precision 0.7391304283553876
f1 0.7391299283557259
===== RES ====
p 114
tp 84
fp 26
recall 0.7368420987996307
precision 0.7636363566942149
f1 0.7499994934633435
===== RES ====
p 116
tp 89
fp 33
recall 0.7672413726961951
precision 0.7295081907417361
f1 0.7478986536971066
===== RES ====
p 116
tp 85
fp 29
recall 0.7327586143727706
precision 0.7456140285472453
f1 0.739129928393533
===== RES ====
p 115
tp 88
fp 36
recall 0.7652173846502837
precision 0.7096774136316337
f1 0.73640116818717
===== RES ====
p 114
tp 84
fp 28
recall 0.7368420987996307
precision 0.7499999933035715
f1 0.7433623253194687
===== RES ====
p 115
tp 88
fp 33
recall 0.7652173846502837
precision 0.7272727212622089
f1 0.7457622058679015
===== RES ====
p 115
tp 87
fp 34
recall 0.756521732551985
precision 0.7190082585205929
f1 0.7372876296685376
===== RES ====
p 116
tp 85
fp 29
recall 0.7327586143727706
precision 0.7456140285472453
f1 0.739129928393533
===== RES ====
p 115
tp 86
fp 30
recall 0.7478260804536863
precision 0.7413793039536267
f1 0.7445882381517949
===== RES ====
p 116
tp 85
fp 28
recall 0.7327586143727706
precision 0.7522123827237842
f1 0.7423575722052909
===== RES ====
p 116
tp 88
fp 33
recall 0.7586206831153389
precision 0.7272727212622089
f1 0.7426155277113513
===== RES ====
p 116
tp 89
fp 31
recall 0.7672413726961951
precision 0.7416666604861112
f1 0.75423678188772
===== RES ====
p 116
tp 88
fp 30
recall 0.7586206831153389
precision 0.7457627055443838
f1 0.7521362457450909
===== RES ====
p 116
tp 88
fp 33
recall 0.7586206831153389
precision 0.7272727212622089
f1 0.7426155277113513
===== RES ====
p 115
tp 89
fp 35
recall 0.7739130367485824
precision 0.7177419296956297
f1 0.7447693689539596
===== RES ====
p 116
tp 85
fp 25
recall 0.7327586143727706
precision 0.7727272657024794
f1 0.7522118830765321
===== RES ====
p 115
tp 87
fp 30
recall 0.756521732551985
precision 0.7435897372342758
f1 0.7499994935719744
===== RES ====
p 300
tp 260
fp 124
recall 0.8666666637777778
precision 0.6770833315700955
f1 0.7602334234468496
=== Result of InvGAN+KD: ===
0.7602334234468496
The source-target datasets are: shoes_watches with seed 42
The F1 score is: 0.7602334234468496
The training time is: 582.8963258266449
The inference time is: 0.00027158111333847046
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: watches
tgt: shoes
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 100
tp 86
fp 80
recall 0.8599999914
precision 0.5180722860357091
f1 0.646616067273787
tgt_res:
===== RES ====
p 418
tp 375
fp 350
recall 0.8971291844566287
precision 0.5172413785969084
f1 0.6561675139254793
save pretrained model to: checkpoint/watches/bert/42/source-encoder.ptshoesbest
save pretrained model to: checkpoint/watches/bert/42/source-classifier.ptshoesbest
===== RES ====
p 97
tp 67
fp 22
recall 0.6907216423636944
precision 0.7528089803055171
f1 0.7204296007056297
tgt_res:
===== RES ====
p 417
tp 296
fp 131
recall 0.7098321325903306
precision 0.69320842928991
f1 0.7014212993562783
save pretrained model to: checkpoint/watches/bert/42/source-encoder.ptshoesbest
save pretrained model to: checkpoint/watches/bert/42/source-classifier.ptshoesbest
===== RES ====
p 99
tp 92
fp 119
recall 0.9292929199061322
precision 0.436018955279531
f1 0.5935479485330959
===== RES ====
p 101
tp 88
fp 74
recall 0.8712871200862662
precision 0.5432098731900625
f1 0.6692010427217845
===== RES ====
p 101
tp 92
fp 87
recall 0.9108910800901874
precision 0.5139664775756063
f1 0.6571423912503231
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/42/source-encoder.ptshoesbest
Restore model from: /home/derossi/DADER/main/checkpoint/watches/bert/42/source-classifier.ptshoesbest
Pretraining time:  72.81721305847168
=== Training F' and A ===
===== RES ====
p 95
tp 46
fp 12
recall 0.4842105212188366
precision 0.793103434601665
f1 0.6013067109235172
======== tgt result =======
===== RES ====
p 417
tp 187
fp 42
recall 0.4484412459269994
precision 0.81659388289697
f1 0.5789469089758142
===== RES ====
p 97
tp 61
fp 19
recall 0.6288659728982889
precision 0.7624999904687502
f1 0.6892650335475489
======== tgt result =======
===== RES ====
p 419
tp 282
fp 95
recall 0.6730310246467041
precision 0.7480106080954626
f1 0.7085422131799424
===== RES ====
p 98
tp 60
fp 22
recall 0.6122448917117869
precision 0.7317073081499109
f1 0.6666661632102457
===== RES ====
p 99
tp 63
fp 20
recall 0.6363636299357209
precision 0.7590361354332996
f1 0.6923071885645388
======== tgt result =======
===== RES ====
p 418
tp 297
fp 101
recall 0.71052631408965
precision 0.7462311539039418
f1 0.727940674987127
===== RES ====
p 100
tp 67
fp 22
recall 0.6699999933
precision 0.7528089803055171
f1 0.7089942031861515
======== tgt result =======
===== RES ====
p 417
tp 295
fp 98
recall 0.7074340510613092
precision 0.750636130405506
f1 0.7283945603691889
===== RES ====
p 98
tp 62
fp 18
recall 0.6326530547688464
precision 0.7749999903125001
f1 0.696628710769194
===== RES ====
p 98
tp 64
fp 19
recall 0.653061217825906
precision 0.771084328059225
f1 0.7071818160621295
===== RES ====
p 100
tp 66
fp 22
recall 0.6599999934
precision 0.7499999914772728
f1 0.7021271541424989
===== RES ====
p 100
tp 69
fp 20
recall 0.6899999931
precision 0.7752808901653833
f1 0.7301582241262043
======== tgt result =======
===== RES ====
p 418
tp 297
fp 98
recall 0.71052631408965
precision 0.7518987322736741
f1 0.7306268048762133
===== RES ====
p 103
tp 64
fp 16
recall 0.6213592172683571
precision 0.7999999900000001
f1 0.6994530521667292
===== RES ====
p 99
tp 73
fp 21
recall 0.7373737299255179
precision 0.7765957364191943
f1 0.756476176434597
======== tgt result =======
===== RES ====
p 421
tp 312
fp 107
recall 0.7410926348192574
precision 0.7446300698218853
f1 0.7428566410916064
===== RES ====
p 95
tp 60
fp 20
recall 0.6315789407202217
precision 0.7499999906250001
f1 0.6857137815513797
===== RES ====
p 100
tp 68
fp 21
recall 0.6799999932
precision 0.7640449352354501
f1 0.7195762136561779
===== RES ====
p 97
tp 62
fp 17
recall 0.6391752511425232
precision 0.7848101166479732
f1 0.704544951769459
===== RES ====
p 99
tp 67
fp 17
recall 0.6767676699316397
precision 0.7976190381235829
f1 0.7322399325154911
===== RES ====
p 100
tp 70
fp 24
recall 0.699999993
precision 0.7446808431416931
f1 0.7216489775750085
===== RES ====
p 98
tp 66
fp 18
recall 0.6734693808829655
precision 0.7857142763605444
f1 0.725274220263594
===== RES ====
p 99
tp 62
fp 13
recall 0.6262626199367413
precision 0.8266666556444446
f1 0.7126431794824355
===== RES ====
p 92
tp 64
fp 22
recall 0.6956521663516069
precision 0.744186037858302
f1 0.7191006160841747
===== RES ====
p 100
tp 69
fp 22
recall 0.6899999931
precision 0.7582417499094314
f1 0.722512582550165
===== RES ====
p 100
tp 65
fp 18
recall 0.6499999935
precision 0.7831325206851504
f1 0.7103820102126545
===== RES ====
p 96
tp 58
fp 17
recall 0.6041666603732639
precision 0.7733333230222224
f1 0.6783620727064893
===== RES ====
p 98
tp 66
fp 18
recall 0.6734693808829655
precision 0.7857142763605444
f1 0.725274220263594
===== RES ====
p 94
tp 61
fp 21
recall 0.6489361633091898
precision 0.7439024299524094
f1 0.6931813126294896
===== RES ====
p 96
tp 63
fp 16
recall 0.6562499931640626
precision 0.7974683443358438
f1 0.7199994964901366
===== RES ====
p 102
tp 68
fp 19
recall 0.6666666601307191
precision 0.7816091864182853
f1 0.7195762151119022
===== RES ====
p 98
tp 65
fp 12
recall 0.6632652993544358
precision 0.8441558331927814
f1 0.742856641567674
===== RES ====
p 100
tp 65
fp 16
recall 0.6499999935
precision 0.802469125895443
f1 0.7182315417725696
===== RES ====
p 100
tp 58
fp 10
recall 0.5799999942
precision 0.8529411639273359
f1 0.6904757003971613
===== RES ====
p 103
tp 70
fp 19
recall 0.6796116438872656
precision 0.7865168450953164
f1 0.7291661617299401
===== RES ====
p 99
tp 63
fp 17
recall 0.6363636299357209
precision 0.7874999901562502
f1 0.703910112293971
===== RES ====
p 95
tp 63
fp 19
recall 0.6631578877562327
precision 0.7682926735574065
f1 0.7118639014335245
===== RES ====
p 98
tp 66
fp 18
recall 0.6734693808829655
precision 0.7857142763605444
f1 0.725274220263594
===== RES ====
p 98
tp 64
fp 18
recall 0.653061217825906
precision 0.780487795359905
f1 0.71111060716084
===== RES ====
p 100
tp 66
fp 16
recall 0.6599999934
precision 0.804878038964902
f1 0.725274222195725
===== RES ====
p 95
tp 69
fp 24
recall 0.726315781828255
precision 0.741935475893167
f1 0.7340420454394534
===== RES ====
p 103
tp 69
fp 21
recall 0.6699029061174475
precision 0.7666666581481483
f1 0.7150254015950203
===== RES ====
p 100
tp 68
fp 22
recall 0.6799999932
precision 0.7555555471604939
f1 0.7157889675349733
===== RES ====
p 100
tp 61
fp 14
recall 0.6099999939
precision 0.813333322488889
f1 0.6971423593799358
===== RES ====
p 96
tp 68
fp 17
recall 0.7083333259548612
precision 0.7999999905882355
f1 0.7513807090140968
===== RES ====
p 300
tp 222
fp 69
recall 0.7399999975333333
precision 0.7628865953165409
f1 0.7512685331069145
=== Result of InvGAN+KD: ===
0.7512685331069145
The source-target datasets are: watches_shoes with seed 42
The F1 score is: 0.7512685331069145
The training time is: 539.4588119983673
The inference time is: 0.0002689957618713379
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: computers
tgt: shoes
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 92
tp 84
fp 115
recall 0.9130434683364841
precision 0.42211055064266056
f1 0.5773191512621173
tgt_res:
===== RES ====
p 423
tp 392
fp 446
recall 0.9267139457997307
precision 0.46778042903606154
f1 0.6217283398461553
save pretrained model to: checkpoint/computers/bert/42/source-encoder.ptshoesbest
save pretrained model to: checkpoint/computers/bert/42/source-classifier.ptshoesbest
===== RES ====
p 102
tp 93
fp 96
recall 0.9117646969434834
precision 0.4920634894599815
f1 0.6391747980305784
tgt_res:
===== RES ====
p 421
tp 388
fp 386
recall 0.9216151997111278
precision 0.5012919890164186
f1 0.6493719274805818
save pretrained model to: checkpoint/computers/bert/42/source-encoder.ptshoesbest
save pretrained model to: checkpoint/computers/bert/42/source-classifier.ptshoesbest
===== RES ====
p 95
tp 92
fp 171
recall 0.9684210424376732
precision 0.3498098846014833
f1 0.5139660876848238
===== RES ====
p 97
tp 92
fp 132
recall 0.9484535984695506
precision 0.4107142838807398
f1 0.573208297435311
===== RES ====
p 104
tp 88
fp 86
recall 0.8461538380177516
precision 0.5057471235301889
f1 0.6330930523268301
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/42/source-encoder.ptshoesbest
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/42/source-classifier.ptshoesbest
Pretraining time:  83.80363893508911
=== Training F' and A ===
===== RES ====
p 99
tp 44
fp 7
recall 0.4444444399551067
precision 0.8627450811226455
f1 0.5866662100447866
======== tgt result =======
===== RES ====
p 420
tp 164
fp 25
recall 0.3904761895464853
precision 0.8677248631337309
f1 0.5385874191024238
===== RES ====
p 95
tp 44
fp 8
recall 0.4631578898614959
precision 0.8461538298816571
f1 0.5986389904210063
======== tgt result =======
===== RES ====
p 419
tp 186
fp 39
recall 0.44391408008612393
precision 0.8266666629925926
f1 0.5776392951326828
===== RES ====
p 102
tp 89
fp 51
recall 0.8725490110534411
precision 0.6357142811734694
f1 0.7355366963325409
======== tgt result =======
===== RES ====
p 418
tp 367
fp 250
recall 0.877990428521554
precision 0.5948136132985192
f1 0.7091782610752644
===== RES ====
p 99
tp 21
fp 1
recall 0.21212120997857364
precision 0.9545454111570267
f1 0.34710713475880317
===== RES ====
p 102
tp 85
fp 40
recall 0.8333333251633988
precision 0.67999999456
f1 0.7488981769492297
======== tgt result =======
===== RES ====
p 422
tp 354
fp 181
recall 0.8388625572538801
precision 0.6616822417538649
f1 0.7398114176510548
===== RES ====
p 99
tp 61
fp 8
recall 0.6161616099377615
precision 0.8840579582020586
f1 0.7261899834895513
===== RES ====
p 98
tp 75
fp 27
recall 0.7653061146397335
precision 0.7352941104382931
f1 0.7499994927003332
======== tgt result =======
===== RES ====
p 419
tp 318
fp 135
recall 0.7589498788569216
precision 0.7019867534172478
f1 0.7293572972527828
===== RES ====
p 96
tp 69
fp 22
recall 0.718749992513021
precision 0.7582417499094314
f1 0.7379674069035956
===== RES ====
p 95
tp 69
fp 24
recall 0.726315781828255
precision 0.741935475893167
f1 0.7340420454394534
===== RES ====
p 97
tp 57
fp 9
recall 0.5876288599213519
precision 0.8636363505509644
f1 0.6993860125713732
===== RES ====
p 102
tp 79
fp 30
recall 0.7745097963283354
precision 0.724770635552563
f1 0.7488146593296385
===== RES ====
p 97
tp 69
fp 21
recall 0.7113401988521629
precision 0.7666666581481483
f1 0.7379674072467562
===== RES ====
p 99
tp 69
fp 19
recall 0.6969696899295991
precision 0.7840909001807852
f1 0.737967408276238
===== RES ====
p 102
tp 71
fp 18
recall 0.6960784245482508
precision 0.7977528000252495
f1 0.743454991913932
===== RES ====
p 101
tp 80
fp 28
recall 0.7920792000784238
precision 0.7407407338820302
f1 0.7655497324698206
======== tgt result =======
===== RES ====
p 419
tp 320
fp 133
recall 0.7637231485352669
precision 0.7064017644450292
f1 0.7339444532055623
===== RES ====
p 99
tp 69
fp 16
recall 0.6969696899295991
precision 0.8117646963321801
f1 0.7499994947427682
===== RES ====
p 100
tp 71
fp 20
recall 0.7099999929
precision 0.7802197716459366
f1 0.7434549907078265
===== RES ====
p 100
tp 73
fp 21
recall 0.7299999927
precision 0.7765957364191943
f1 0.752576812307697
===== RES ====
p 101
tp 74
fp 24
recall 0.732673260072542
precision 0.7551020331112037
f1 0.7437180856042348
===== RES ====
p 99
tp 57
fp 8
recall 0.5757575699418428
precision 0.8769230634319529
f1 0.6951214642329288
===== RES ====
p 103
tp 80
fp 30
recall 0.7766990215854465
precision 0.7272727206611571
f1 0.751173202407262
===== RES ====
p 101
tp 69
fp 12
recall 0.6831683100676406
precision 0.8518518413351625
f1 0.7582412559476711
===== RES ====
p 99
tp 70
fp 12
recall 0.7070706999285788
precision 0.853658526174896
f1 0.7734801588477276
======== tgt result =======
===== RES ====
p 420
tp 268
fp 80
recall 0.6380952365759637
precision 0.7701149403157617
f1 0.6979161692440585
===== RES ====
p 96
tp 74
fp 27
recall 0.7708333253038195
precision 0.732673260072542
f1 0.7512685282283208
===== RES ====
p 98
tp 64
fp 10
recall 0.653061217825906
precision 0.8648648531775021
f1 0.7441855475936165
===== RES ====
p 95
tp 67
fp 15
recall 0.7052631504709143
precision 0.8170731607674006
f1 0.7570616410357891
===== RES ====
p 103
tp 75
fp 21
recall 0.728155332736356
precision 0.7812499918619793
f1 0.7537683372645414
===== RES ====
p 100
tp 68
fp 11
recall 0.6799999932
precision 0.8607594827751964
f1 0.7597760347058534
===== RES ====
p 98
tp 71
fp 25
recall 0.7244897885256144
precision 0.7395833256293404
f1 0.7319582553941135
===== RES ====
p 102
tp 73
fp 23
recall 0.7156862674932719
precision 0.7604166587456598
f1 0.737373230384993
===== RES ====
p 97
tp 68
fp 16
recall 0.7010309206079287
precision 0.8095237998866215
f1 0.7513807097466741
===== RES ====
p 96
tp 71
fp 25
recall 0.7395833256293404
precision 0.7395833256293404
f1 0.7395828256296784
===== RES ====
p 98
tp 70
fp 23
recall 0.7142857069970846
precision 0.7526881639495897
f1 0.7329837858614741
===== RES ====
p 100
tp 69
fp 16
recall 0.6899999931
precision 0.8117646963321801
f1 0.7459454411690671
===== RES ====
p 99
tp 76
fp 28
recall 0.767676759922457
precision 0.7307692237426037
f1 0.74876796583304
===== RES ====
p 98
tp 68
fp 16
recall 0.6938775439400251
precision 0.8095237998866215
f1 0.7472522420000892
===== RES ====
p 96
tp 72
fp 24
recall 0.7499999921875001
precision 0.7499999921875001
f1 0.7499994921878335
===== RES ====
p 96
tp 71
fp 22
recall 0.7395833256293404
precision 0.7634408520060124
f1 0.7513222434985551
===== RES ====
p 97
tp 68
fp 11
recall 0.7010309206079287
precision 0.8607594827751964
f1 0.7727267691764531
===== RES ====
p 95
tp 69
fp 23
recall 0.726315781828255
precision 0.7499999918478262
f1 0.737967406674822
===== RES ====
p 300
tp 237
fp 106
recall 0.7899999973666667
precision 0.6909620971108977
f1 0.7371690178284118
=== Result of InvGAN+KD: ===
0.7371690178284118
The source-target datasets are: computers_shoes with seed 42
The F1 score is: 0.7371690178284118
The training time is: 539.9711484909058
The inference time is: 0.0002682432532310486
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: shoes
tgt: computers
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 136
tp 102
fp 29
recall 0.7499999944852942
precision 0.7786259482547637
f1 0.7640444382727113
tgt_res:
===== RES ====
p 568
tp 424
fp 107
recall 0.7464788719252132
precision 0.7984934071591462
f1 0.7716100542128964
save pretrained model to: checkpoint/shoes/bert/42/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/shoes/bert/42/source-classifier.ptcomputersbest
===== RES ====
p 138
tp 66
fp 6
recall 0.4782608660995589
precision 0.9166666539351854
f1 0.6285709719731113
===== RES ====
p 133
tp 98
fp 27
recall 0.7368420997229918
precision 0.7839999937280001
f1 0.7596894170726198
===== RES ====
p 138
tp 115
fp 64
recall 0.833333327294686
precision 0.6424580969695078
f1 0.7255515542600033
===== RES ====
p 136
tp 106
fp 56
recall 0.7794117589749135
precision 0.6543209836153026
f1 0.7114088950050758
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/42/source-encoder.ptcomputersbest
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/42/source-classifier.ptcomputersbest
Pretraining time:  67.32227778434753
=== Training F' and A ===
===== RES ====
p 138
tp 114
fp 67
recall 0.8260869505356018
precision 0.6298342506638992
f1 0.7147330469240113
======== tgt result =======
===== RES ====
p 572
tp 474
fp 274
recall 0.8286713272226026
precision 0.6336898387250136
f1 0.7181813259828888
===== RES ====
p 140
tp 95
fp 14
recall 0.6785714237244899
precision 0.8715596250315631
f1 0.7630517104566006
======== tgt result =======
===== RES ====
p 568
tp 397
fp 63
recall 0.6989436607412963
precision 0.8630434763846881
f1 0.7723730448723054
===== RES ====
p 141
tp 103
fp 20
recall 0.7304964487198834
precision 0.837398367175623
f1 0.7803025267163414
======== tgt result =======
===== RES ====
p 571
tp 430
fp 83
recall 0.7530647972800967
precision 0.8382066260463809
f1 0.7933574335473103
===== RES ====
p 137
tp 55
fp 3
recall 0.40145985108423465
precision 0.9482758457193821
f1 0.5641021403816361
===== RES ====
p 138
tp 99
fp 25
recall 0.7173912991493384
precision 0.798387090335588
f1 0.7557246864987849
===== RES ====
p 138
tp 113
fp 67
recall 0.8188405737765176
precision 0.6277777742901235
f1 0.7106913281519158
===== RES ====
p 138
tp 103
fp 30
recall 0.7463768061856754
precision 0.7744360844027363
f1 0.7601470960366027
===== RES ====
p 134
tp 108
fp 40
recall 0.8059701432390288
precision 0.7297297247991235
f1 0.7659569426088456
===== RES ====
p 139
tp 104
fp 33
recall 0.7482014334661767
precision 0.7591240820501892
f1 0.7536226829713464
===== RES ====
p 140
tp 116
fp 57
recall 0.8285714226530613
precision 0.6705202273380334
f1 0.7412135583300057
===== RES ====
p 140
tp 97
fp 21
recall 0.6928571379081633
precision 0.8220338913386959
f1 0.7519374823030739
===== RES ====
p 141
tp 112
fp 55
recall 0.7943262355012324
precision 0.6706586786188103
f1 0.7272722261135116
===== RES ====
p 134
tp 93
fp 19
recall 0.6940298455669415
precision 0.8303571354432399
f1 0.756097058827743
===== RES ====
p 137
tp 105
fp 31
recall 0.7664233520699025
precision 0.7720588178525087
f1 0.7692302636024129
===== RES ====
p 138
tp 106
fp 32
recall 0.768115936462928
precision 0.768115936462928
f1 0.7681154364632534
===== RES ====
p 140
tp 107
fp 28
recall 0.7642857088265307
precision 0.7925925867215364
f1 0.7781813126879245
===== RES ====
p 133
tp 104
fp 47
recall 0.7819548813386851
precision 0.6887417172931012
f1 0.7323938630483451
===== RES ====
p 139
tp 107
fp 42
recall 0.7697841671238549
precision 0.7181208005495249
f1 0.7430550509985996
===== RES ====
p 137
tp 99
fp 22
recall 0.7226277319516224
precision 0.818181811419985
f1 0.7674413564392392
===== RES ====
p 141
tp 113
fp 57
recall 0.8014184340324934
precision 0.6647058784429066
f1 0.7266876025685497
===== RES ====
p 137
tp 107
fp 36
recall 0.7810218921093293
precision 0.7482517430192186
f1 0.7642852090564494
===== RES ====
p 137
tp 107
fp 35
recall 0.7810218921093293
precision 0.7535211214540766
f1 0.7670245842682567
===== RES ====
p 135
tp 109
fp 38
recall 0.8074074014266118
precision 0.7414965935952613
f1 0.7730491408131587
===== RES ====
p 134
tp 101
fp 26
recall 0.7537313376587214
precision 0.7952755842891687
f1 0.7739458545826101
===== RES ====
p 141
tp 107
fp 33
recall 0.7588652428449274
precision 0.7642857088265307
f1 0.7615653308851956
===== RES ====
p 138
tp 109
fp 46
recall 0.7898550667401807
precision 0.7032258019146722
f1 0.7440268003591033
===== RES ====
p 141
tp 108
fp 36
recall 0.7659574413761884
precision 0.7499999947916667
f1 0.7578942315792773
===== RES ====
p 141
tp 107
fp 33
recall 0.7588652428449274
precision 0.7642857088265307
f1 0.7615653308851956
===== RES ====
p 135
tp 109
fp 43
recall 0.8074074014266118
precision 0.717105258440097
f1 0.7595813779944751
===== RES ====
p 137
tp 104
fp 37
recall 0.7591240820501892
precision 0.7375886472511444
f1 0.748200933570025
===== RES ====
p 142
tp 111
fp 41
recall 0.7816901353402104
precision 0.730263153090374
f1 0.755101536258371
===== RES ====
p 136
tp 95
fp 23
recall 0.6985294066284603
precision 0.8050847389399599
f1 0.7480309926843164
===== RES ====
p 136
tp 110
fp 57
recall 0.8088235234645329
precision 0.6586826307863316
f1 0.7260721077021964
===== RES ====
p 138
tp 98
fp 25
recall 0.7101449223902542
precision 0.7967479610020491
f1 0.7509573503034678
===== RES ====
p 139
tp 101
fp 30
recall 0.7266186998084986
precision 0.7709923605267759
f1 0.7481476430456011
===== RES ====
p 141
tp 101
fp 29
recall 0.7163120516573613
precision 0.7769230709467456
f1 0.7453869491976493
===== RES ====
p 138
tp 107
fp 37
recall 0.7753623132220122
precision 0.7430555503954476
f1 0.7588647430716035
===== RES ====
p 138
tp 100
fp 28
recall 0.7246376759084227
precision 0.7812499938964844
f1 0.751879194301875
===== RES ====
p 140
tp 103
fp 30
recall 0.7357142804591837
precision 0.7744360844027363
f1 0.7545782493797671
===== RES ====
p 137
tp 106
fp 36
recall 0.773722622089616
precision 0.7464788679825431
f1 0.7598561255382846
===== RES ====
p 300
tp 249
fp 152
recall 0.8299999972333334
precision 0.6209476293741955
f1 0.7104132030748124
=== Result of InvGAN+KD: ===
0.7104132030748124
The source-target datasets are: shoes_computers with seed 42
The F1 score is: 0.7104132030748124
The training time is: 731.506097316742
The inference time is: 0.00027092546224594116
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: cameras
tgt: shoes
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 96
tp 86
fp 54
recall 0.8958333240017362
precision 0.6142857098979593
f1 0.7288130705260305
tgt_res:
===== RES ====
p 420
tp 365
fp 270
recall 0.8690476169784581
precision 0.5748031487010974
f1 0.6919426474161581
save pretrained model to: checkpoint/cameras/bert/42/source-encoder.ptshoesbest
save pretrained model to: checkpoint/cameras/bert/42/source-classifier.ptshoesbest
===== RES ====
p 102
tp 87
fp 45
recall 0.8529411681084199
precision 0.6590909040977962
f1 0.7435892454528784
tgt_res:
===== RES ====
p 422
tp 372
fp 209
recall 0.8815165855888233
precision 0.6402753861613161
f1 0.741774187058289
save pretrained model to: checkpoint/cameras/bert/42/source-encoder.ptshoesbest
save pretrained model to: checkpoint/cameras/bert/42/source-classifier.ptshoesbest
===== RES ====
p 100
tp 91
fp 62
recall 0.9099999909000002
precision 0.5947712379426716
f1 0.7193671051886503
===== RES ====
p 99
tp 97
fp 117
recall 0.9797979699010306
precision 0.4532710259192943
f1 0.6198078702449703
===== RES ====
p 94
tp 83
fp 58
recall 0.8829787140108648
precision 0.5886524780946633
f1 0.7063824927119602
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/42/source-encoder.ptshoesbest
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/42/source-classifier.ptshoesbest
Pretraining time:  66.80483388900757
=== Training F' and A ===
===== RES ====
p 98
tp 68
fp 23
recall 0.6938775439400251
precision 0.7472527390411787
f1 0.7195762126483687
======== tgt result =======
===== RES ====
p 420
tp 287
fp 107
recall 0.6833333317063492
precision 0.7284263940902883
f1 0.7051592039375947
===== RES ====
p 96
tp 34
fp 12
recall 0.3541666629774306
precision 0.7391304187145562
f1 0.47887279468399363
===== RES ====
p 99
tp 71
fp 21
recall 0.7171717099275585
precision 0.7717391220463139
f1 0.7434549902692426
======== tgt result =======
===== RES ====
p 416
tp 295
fp 141
recall 0.7091346136799649
precision 0.6766055030353085
f1 0.692487761561118
===== RES ====
p 99
tp 73
fp 32
recall 0.7373737299255179
precision 0.6952380886167802
f1 0.7156857679261467
===== RES ====
p 99
tp 51
fp 16
recall 0.5151515099479645
precision 0.7611940184896415
f1 0.6144573425029171
===== RES ====
p 94
tp 82
fp 43
recall 0.8723404162516977
precision 0.6559999947520001
f1 0.7488579506685719
======== tgt result =======
===== RES ====
p 419
tp 364
fp 199
recall 0.8687350814588661
precision 0.6465364109297754
f1 0.7413437047613781
===== RES ====
p 98
tp 61
fp 23
recall 0.6224489732403166
precision 0.7261904675453515
f1 0.6703291659223587
===== RES ====
p 96
tp 71
fp 24
recall 0.7395833256293404
precision 0.7473684131855957
f1 0.7434549896113668
===== RES ====
p 99
tp 52
fp 19
recall 0.5252525199469442
precision 0.7323943558817697
f1 0.6117642122495216
===== RES ====
p 97
tp 64
fp 24
recall 0.6597938076309917
precision 0.7272727190082646
f1 0.6918913855956849
===== RES ====
p 100
tp 60
fp 19
recall 0.599999994
precision 0.7594936612722322
f1 0.6703905608442815
===== RES ====
p 98
tp 55
fp 21
recall 0.5612244840691379
precision 0.7236842010041553
f1 0.632183408773008
===== RES ====
p 100
tp 64
fp 25
recall 0.6399999936
precision 0.7191011155157179
f1 0.6772481717760729
===== RES ====
p 102
tp 73
fp 24
recall 0.7156862674932719
precision 0.7525773118290999
f1 0.7336678346509806
===== RES ====
p 100
tp 57
fp 24
recall 0.5699999943
precision 0.7037036950160038
f1 0.62983375269414
===== RES ====
p 96
tp 59
fp 24
recall 0.6145833269314237
precision 0.710843364929598
f1 0.6592173723670306
===== RES ====
p 99
tp 60
fp 23
recall 0.6060605999387818
precision 0.7228915575555235
f1 0.6593401559597986
===== RES ====
p 96
tp 65
fp 24
recall 0.677083326280382
precision 0.7303370704456509
f1 0.7027021958221226
===== RES ====
p 97
tp 43
fp 20
recall 0.44329896450207257
precision 0.6825396717057195
f1 0.5374995158597986
===== RES ====
p 101
tp 66
fp 22
recall 0.6534653400646996
precision 0.7499999914772728
f1 0.6984121933879978
===== RES ====
p 99
tp 51
fp 21
recall 0.5151515099479645
precision 0.7083333234953705
f1 0.596490733559442
===== RES ====
p 99
tp 68
fp 23
recall 0.6868686799306194
precision 0.7472527390411787
f1 0.7157889670363593
===== RES ====
p 99
tp 62
fp 24
recall 0.6262626199367413
precision 0.72093022417523
f1 0.6702697654934301
===== RES ====
p 99
tp 57
fp 22
recall 0.5757575699418428
precision 0.7215189782086205
f1 0.640448937318901
===== RES ====
p 101
tp 70
fp 27
recall 0.6930693000686208
precision 0.7216494770963972
f1 0.707070200132993
===== RES ====
p 95
tp 61
fp 24
recall 0.642105256398892
precision 0.7176470503806229
f1 0.6777772717904901
===== RES ====
p 96
tp 67
fp 27
recall 0.6979166593967014
precision 0.7127659498641921
f1 0.7052626505266704
===== RES ====
p 100
tp 64
fp 24
recall 0.6399999936
precision 0.7272727190082646
f1 0.680850558624176
===== RES ====
p 98
tp 71
fp 22
recall 0.7244897885256144
precision 0.7634408520060124
f1 0.7434549899403047
===== RES ====
p 98
tp 39
fp 19
recall 0.3979591796126614
precision 0.6724137815101072
f1 0.4999995264632891
===== RES ====
p 101
tp 69
fp 25
recall 0.6831683100676406
precision 0.7340425453825261
f1 0.7076918010785905
===== RES ====
p 95
tp 50
fp 23
recall 0.5263157839335181
precision 0.6849314974666918
f1 0.595237596726596
===== RES ====
p 95
tp 61
fp 24
recall 0.642105256398892
precision 0.7176470503806229
f1 0.6777772717904901
===== RES ====
p 99
tp 42
fp 19
recall 0.4242424199571473
precision 0.6885245788766463
f1 0.5249995216410484
===== RES ====
p 99
tp 64
fp 22
recall 0.6464646399347006
precision 0.744186037858302
f1 0.6918913868812929
===== RES ====
p 100
tp 69
fp 25
recall 0.6899999931
precision 0.7340425453825261
f1 0.7113396993307792
===== RES ====
p 103
tp 59
fp 22
recall 0.5728155284192668
precision 0.7283950527358636
f1 0.6413038480036869
===== RES ====
p 100
tp 51
fp 23
recall 0.5099999949
precision 0.689189179875822
f1 0.5862064009780796
===== RES ====
p 98
tp 65
fp 23
recall 0.6632652993544358
precision 0.7386363552427687
f1 0.6989242251130855
===== RES ====
p 97
tp 54
fp 23
recall 0.5567010251886493
precision 0.7012986921909261
f1 0.6206891546443346
===== RES ====
p 300
tp 222
fp 179
recall 0.7399999975333333
precision 0.5536159587191622
f1 0.6333803930235917
=== Result of InvGAN+KD: ===
0.6333803930235917
The source-target datasets are: cameras_shoes with seed 42
The F1 score is: 0.6333803930235917
The training time is: 532.2081933021545
The inference time is: 0.00026864558458328247
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: shoes
tgt: cameras
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 90
tp 64
fp 7
recall 0.7111111032098767
precision 0.9014084380083319
f1 0.7950305529882306
tgt_res:
===== RES ====
p 388
tp 254
fp 48
recall 0.6546391735705176
precision 0.8410595998640411
f1 0.7362313896915705
save pretrained model to: checkpoint/shoes/bert/42/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/shoes/bert/42/source-classifier.ptcamerasbest
===== RES ====
p 88
tp 60
fp 16
recall 0.681818174070248
precision 0.7894736738227148
f1 0.7317068108272269
===== RES ====
p 90
tp 48
fp 4
recall 0.5333333274074075
precision 0.9230769053254442
f1 0.6760558643129543
===== RES ====
p 87
tp 58
fp 7
recall 0.6666666590038315
precision 0.8923076785798819
f1 0.7631573951699814
===== RES ====
p 93
tp 65
fp 19
recall 0.6989247236674762
precision 0.7738095145975058
f1 0.734462769830209
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/42/source-encoder.ptcamerasbest
Restore model from: /home/derossi/DADER/main/checkpoint/shoes/bert/42/source-classifier.ptcamerasbest
Pretraining time:  66.12447953224182
=== Training F' and A ===
===== RES ====
p 93
tp 77
fp 44
recall 0.8279569803445487
precision 0.6363636311044328
f1 0.719625670058856
======== tgt result =======
===== RES ====
p 388
tp 309
fp 218
recall 0.7963917505247635
precision 0.5863377597982206
f1 0.6754093461283355
===== RES ====
p 90
tp 49
fp 5
recall 0.5444444383950618
precision 0.9074073906035669
f1 0.6805550773537172
===== RES ====
p 89
tp 65
fp 18
recall 0.7303370704456509
precision 0.7831325206851504
f1 0.7558134453086048
======== tgt result =======
===== RES ====
p 388
tp 290
fp 99
recall 0.7474226784860241
precision 0.745501283430588
f1 0.7464602445405177
===== RES ====
p 93
tp 70
fp 36
recall 0.7526881639495897
precision 0.660377352260591
f1 0.7035170830033068
===== RES ====
p 91
tp 55
fp 10
recall 0.6043955977538945
precision 0.8461538331360949
f1 0.7051277099773238
===== RES ====
p 92
tp 68
fp 28
recall 0.7391304267485823
precision 0.7083333259548612
f1 0.7234037478500511
===== RES ====
p 89
tp 59
fp 13
recall 0.6629213408660524
precision 0.8194444330632717
f1 0.7329187511287623
===== RES ====
p 91
tp 62
fp 19
recall 0.6813186738316629
precision 0.7654320893156532
f1 0.720929725865677
===== RES ====
p 94
tp 63
fp 16
recall 0.6702127588275238
precision 0.7974683443358438
f1 0.7283231947612724
===== RES ====
p 90
tp 58
fp 11
recall 0.6444444372839507
precision 0.8405796979626131
f1 0.7295592479731168
===== RES ====
p 92
tp 56
fp 10
recall 0.6086956455576561
precision 0.8484848356290177
f1 0.7088602640605804
===== RES ====
p 94
tp 64
fp 18
recall 0.6808510565866909
precision 0.780487795359905
f1 0.7272722213329853
===== RES ====
p 89
tp 57
fp 11
recall 0.6404494310061862
precision 0.8382352817906576
f1 0.7261141493775888
===== RES ====
p 85
tp 45
fp 4
recall 0.5294117584775088
precision 0.9183673281965852
f1 0.671641317108807
===== RES ====
p 89
tp 62
fp 20
recall 0.6966292056558516
precision 0.7560975517549079
f1 0.7251456911873756
===== RES ====
p 90
tp 57
fp 15
recall 0.6333333262962964
precision 0.7916666556712965
f1 0.7037032011891897
===== RES ====
p 89
tp 62
fp 16
recall 0.6966292056558516
precision 0.794871784681131
f1 0.7425144633371333
===== RES ====
p 89
tp 58
fp 15
recall 0.6516853859361192
precision 0.7945205370613625
f1 0.7160488787535811
===== RES ====
p 90
tp 59
fp 15
recall 0.655555548271605
precision 0.7972972865230097
f1 0.7195116911068251
===== RES ====
p 89
tp 60
fp 16
recall 0.6741572957959855
precision 0.7894736738227148
f1 0.7272722215614048
===== RES ====
p 87
tp 57
fp 17
recall 0.6551724062623862
precision 0.7702702598612128
f1 0.7080740286257875
===== RES ====
p 86
tp 57
fp 14
recall 0.6627906899675501
precision 0.8028168901011706
f1 0.7261141449960783
===== RES ====
p 87
tp 60
fp 21
recall 0.6896551644867223
precision 0.7407407315957935
f1 0.7142852064204174
===== RES ====
p 92
tp 63
fp 15
recall 0.684782601252363
precision 0.8076922973372783
f1 0.7411759652598484
===== RES ====
p 90
tp 66
fp 24
recall 0.7333333251851853
precision 0.7333333251851853
f1 0.7333328251855262
===== RES ====
p 92
tp 59
fp 15
recall 0.6413043408553876
precision 0.7972972865230097
f1 0.7108428708088789
===== RES ====
p 91
tp 64
fp 23
recall 0.7032966955681682
precision 0.7356321754525037
f1 0.7191006157685585
===== RES ====
p 86
tp 50
fp 12
recall 0.5813953420767983
precision 0.806451599895942
f1 0.6756751796935573
===== RES ====
p 91
tp 61
fp 16
recall 0.6703296629634103
precision 0.7922077819193795
f1 0.7261899710179133
===== RES ====
p 92
tp 67
fp 24
recall 0.7282608616493385
precision 0.7362637281729261
f1 0.732239929171114
===== RES ====
p 89
tp 55
fp 13
recall 0.61797752114632
precision 0.8088235175173012
f1 0.7006364426957882
===== RES ====
p 91
tp 58
fp 15
recall 0.6373626303586525
precision 0.7945205370613625
f1 0.7073165705684592
===== RES ====
p 90
tp 61
fp 18
recall 0.6777777702469137
precision 0.7721518889601027
f1 0.7218929846997588
===== RES ====
p 91
tp 66
fp 18
recall 0.7252747173046734
precision 0.7857142763605444
f1 0.7542852064656366
===== RES ====
p 91
tp 52
fp 9
recall 0.5714285651491366
precision 0.8524590024187049
f1 0.6842100367905033
===== RES ====
p 89
tp 58
fp 15
recall 0.6516853859361192
precision 0.7945205370613625
f1 0.7160488787535811
===== RES ====
p 89
tp 56
fp 16
recall 0.6292134760762531
precision 0.7777777669753089
f1 0.6956516708463836
===== RES ====
p 90
tp 54
fp 12
recall 0.5999999933333334
precision 0.8181818057851242
f1 0.6923071952666162
===== RES ====
p 89
tp 58
fp 15
recall 0.6516853859361192
precision 0.7945205370613625
f1 0.7160488787535811
===== RES ====
p 86
tp 55
fp 13
recall 0.6395348762844782
precision 0.8088235175173012
f1 0.7142852118404486
===== RES ====
p 300
tp 217
fp 84
recall 0.7233333309222222
precision 0.7209302301630225
f1 0.7221292812924799
=== Result of InvGAN+KD: ===
0.7221292812924799
The source-target datasets are: shoes_cameras with seed 42
The F1 score is: 0.7221292812924799
The training time is: 489.1114501953125
The inference time is: 0.00027085840702056885
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: computers
tgt: cameras
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 91
tp 80
fp 33
recall 0.8791208694602103
precision 0.7079645955047381
f1 0.7843132236162283
tgt_res:
===== RES ====
p 388
tp 320
fp 151
recall 0.8247422659156127
precision 0.6794055187273769
f1 0.7450518894296547
save pretrained model to: checkpoint/computers/bert/42/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/computers/bert/42/source-classifier.ptcamerasbest
===== RES ====
p 89
tp 82
fp 38
recall 0.9213483042545134
precision 0.683333327638889
f1 0.7846884987068361
tgt_res:
===== RES ====
p 389
tp 327
fp 141
recall 0.8406169644200078
precision 0.6987179472249616
f1 0.7631266903328026
save pretrained model to: checkpoint/computers/bert/42/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/computers/bert/42/source-classifier.ptcamerasbest
===== RES ====
p 87
tp 74
fp 25
recall 0.8505747028669575
precision 0.7474747399244976
f1 0.7956984182567575
tgt_res:
===== RES ====
p 389
tp 312
fp 102
recall 0.8020565532080809
precision 0.7536231865854512
f1 0.7770854263203633
save pretrained model to: checkpoint/computers/bert/42/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/computers/bert/42/source-classifier.ptcamerasbest
===== RES ====
p 92
tp 78
fp 30
recall 0.8478260777410209
precision 0.7222222155349795
f1 0.7799994954003165
===== RES ====
p 92
tp 73
fp 16
recall 0.7934782522448016
precision 0.8202247098851156
f1 0.8066293254787751
tgt_res:
===== RES ====
p 387
tp 266
fp 64
recall 0.6873384995159212
precision 0.8060606036179981
f1 0.7419799752886582
save pretrained model to: checkpoint/computers/bert/42/source-encoder.ptcamerasbest
save pretrained model to: checkpoint/computers/bert/42/source-classifier.ptcamerasbest
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/42/source-encoder.ptcamerasbest
Restore model from: /home/derossi/DADER/main/checkpoint/computers/bert/42/source-classifier.ptcamerasbest
Pretraining time:  84.756516456604
=== Training F' and A ===
===== RES ====
p 88
tp 72
fp 12
recall 0.8181818088842977
precision 0.8571428469387756
f1 0.8372087928613043
======== tgt result =======
===== RES ====
p 389
tp 278
fp 64
recall 0.7146529544610464
precision 0.8128654946992238
f1 0.7606014151709692
===== RES ====
p 91
tp 72
fp 9
recall 0.7912087825141892
precision 0.8888888779149522
f1 0.8372087942809889
======== tgt result =======
===== RES ====
p 389
tp 266
fp 48
recall 0.6838046254915048
precision 0.8471337552639052
f1 0.7567562602950565
===== RES ====
p 88
tp 64
fp 8
recall 0.7272727190082646
precision 0.88888887654321
f1 0.7999994950003063
===== RES ====
p 90
tp 75
fp 19
recall 0.8333333240740742
precision 0.7978723319375284
f1 0.8152168826798907
===== RES ====
p 92
tp 73
fp 10
recall 0.7934782522448016
precision 0.8795180616925535
f1 0.8342852060737677
===== RES ====
p 91
tp 70
fp 10
recall 0.7692307607776839
precision 0.8749999890625002
f1 0.8187129427861175
===== RES ====
p 87
tp 70
fp 13
recall 0.804597691901176
precision 0.8433734838147773
f1 0.8235289023532446
===== RES ====
p 91
tp 75
fp 17
recall 0.8241758151189471
precision 0.8152173824432893
f1 0.819671622204611
===== RES ====
p 90
tp 74
fp 11
recall 0.8222222130864199
precision 0.8705882250519033
f1 0.8457137764574382
======== tgt result =======
===== RES ====
p 386
tp 287
fp 64
recall 0.74352331413595
precision 0.8176638153342911
f1 0.7788326062057547
===== RES ====
p 94
tp 74
fp 14
recall 0.7872340341783614
precision 0.8409090813533059
f1 0.8131863047944139
===== RES ====
p 91
tp 74
fp 12
recall 0.8131868042506945
precision 0.8604651062736616
f1 0.8361576830415705
===== RES ====
p 92
tp 73
fp 14
recall 0.7934782522448016
precision 0.8390804501255121
f1 0.8156419493776664
===== RES ====
p 91
tp 74
fp 19
recall 0.8131868042506945
precision 0.7956989161752805
f1 0.80434731740343
===== RES ====
p 90
tp 73
fp 28
recall 0.8111111020987656
precision 0.7227722700715618
f1 0.7643973994137182
===== RES ====
p 90
tp 75
fp 31
recall 0.8333333240740742
precision 0.7075471631363476
f1 0.7653056179720009
===== RES ====
p 90
tp 73
fp 24
recall 0.8111111020987656
precision 0.7525773118290999
f1 0.7807481554522911
===== RES ====
p 92
tp 76
fp 34
recall 0.8260869475425332
precision 0.6909090846280993
f1 0.7524747440450286
===== RES ====
p 87
tp 67
fp 29
recall 0.7701149336768399
precision 0.6979166593967014
f1 0.7322399303655345
===== RES ====
p 94
tp 70
fp 6
recall 0.7446808431416931
precision 0.921052619459834
f1 0.8235289076819576
===== RES ====
p 93
tp 80
fp 18
recall 0.8602150445138167
precision 0.8163265222823824
f1 0.8376958266497869
===== RES ====
p 91
tp 72
fp 10
recall 0.7912087825141892
precision 0.8780487697798931
f1 0.8323694339272593
===== RES ====
p 91
tp 70
fp 10
recall 0.7692307607776839
precision 0.8749999890625002
f1 0.8187129427861175
===== RES ====
p 87
tp 73
fp 22
recall 0.8390804501255121
precision 0.7684210445429364
f1 0.8021972943488193
===== RES ====
p 88
tp 73
fp 15
recall 0.8295454451188018
precision 0.8295454451188018
f1 0.8295449451191033
===== RES ====
p 89
tp 73
fp 16
recall 0.8202247098851156
precision 0.8202247098851156
f1 0.8202242098854206
===== RES ====
p 93
tp 78
fp 16
recall 0.8387096684009713
precision 0.8297872252150296
f1 0.8342240900228911
===== RES ====
p 88
tp 68
fp 13
recall 0.7727272639462811
precision 0.8395061624752326
f1 0.8047332191452977
===== RES ====
p 91
tp 74
fp 17
recall 0.8131868042506945
precision 0.8131868042506945
f1 0.8131863042510019
===== RES ====
p 91
tp 74
fp 19
recall 0.8131868042506945
precision 0.7956989161752805
f1 0.80434731740343
===== RES ====
p 90
tp 74
fp 22
recall 0.8222222130864199
precision 0.7708333253038195
f1 0.7956984166958854
===== RES ====
p 92
tp 78
fp 30
recall 0.8478260777410209
precision 0.7222222155349795
f1 0.7799994954003165
===== RES ====
p 92
tp 74
fp 23
recall 0.8043478173440455
precision 0.7628865900733341
f1 0.7830682751325941
===== RES ====
p 92
tp 74
fp 23
recall 0.8043478173440455
precision 0.7628865900733341
f1 0.7830682751325941
===== RES ====
p 91
tp 75
fp 24
recall 0.8241758151189471
precision 0.7575757499234773
f1 0.7894731767870193
===== RES ====
p 94
tp 77
fp 20
recall 0.8191489274558625
precision 0.7938144248060369
f1 0.806282214194
===== RES ====
p 89
tp 72
fp 17
recall 0.8089887549551825
precision 0.8089887549551825
f1 0.8089882549554915
===== RES ====
p 90
tp 73
fp 16
recall 0.8111111020987656
precision 0.8202247098851156
f1 0.8156419490031468
===== RES ====
p 90
tp 75
fp 27
recall 0.8333333240740742
precision 0.7352941104382931
f1 0.7812494938154219
===== RES ====
p 92
tp 76
fp 25
recall 0.8260869475425332
precision 0.7524752400745026
f1 0.787564259765679
===== RES ====
p 92
tp 76
fp 19
recall 0.8260869475425332
precision 0.7999999915789475
f1 0.8128337160345093
===== RES ====
p 300
tp 260
fp 69
recall 0.8666666637777778
precision 0.7902735538289558
f1 0.8267085604376674
=== Result of InvGAN+KD: ===
0.8267085604376674
The source-target datasets are: computers_cameras with seed 42
The F1 score is: 0.8267085604376674
The training time is: 492.53423166275024
The inference time is: 0.00027145445346832275
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: cameras
tgt: computers
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 40
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 139
tp 120
fp 100
recall 0.863309346307127
precision 0.5454545429752067
f1 0.6685231986099068
tgt_res:
===== RES ====
p 573
tp 505
fp 402
recall 0.8813263509924496
precision 0.5567805947554789
f1 0.6824319569753117
save pretrained model to: checkpoint/cameras/bert/42/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/cameras/bert/42/source-classifier.ptcomputersbest
===== RES ====
p 137
tp 125
fp 77
recall 0.9124087524641697
precision 0.6188118781246936
f1 0.7374626408753503
tgt_res:
===== RES ====
p 573
tp 508
fp 283
recall 0.8865619530775534
precision 0.6422250307936472
f1 0.7448675468706156
save pretrained model to: checkpoint/cameras/bert/42/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/cameras/bert/42/source-classifier.ptcomputersbest
===== RES ====
p 139
tp 126
fp 80
recall 0.9064748136224834
precision 0.6116504824677161
f1 0.7304342972319909
===== RES ====
p 140
tp 127
fp 66
recall 0.9071428506632654
precision 0.6580310846734141
f1 0.7627622708477354
tgt_res:
===== RES ====
p 570
tp 505
fp 249
recall 0.8859649107263774
precision 0.6697612723212715
f1 0.7628393876587886
save pretrained model to: checkpoint/cameras/bert/42/source-encoder.ptcomputersbest
save pretrained model to: checkpoint/cameras/bert/42/source-classifier.ptcomputersbest
===== RES ====
p 139
tp 122
fp 60
recall 0.8776978354122458
precision 0.6703296666465403
f1 0.7601241148283953
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/42/source-encoder.ptcomputersbest
Restore model from: /home/derossi/DADER/main/checkpoint/cameras/bert/42/source-classifier.ptcomputersbest
Pretraining time:  74.24307680130005
=== Training F' and A ===
===== RES ====
p 141
tp 120
fp 37
recall 0.8510638237513204
precision 0.7643312053227311
f1 0.8053686235533017
======== tgt result =======
===== RES ====
p 567
tp 478
fp 135
recall 0.8430335082133448
precision 0.7797716137360985
f1 0.8101689909124019
===== RES ====
p 138
tp 123
fp 49
recall 0.8913043413673598
precision 0.7151162749121147
f1 0.7935478879919828
===== RES ====
p 136
tp 115
fp 33
recall 0.8455882290765572
precision 0.7770270217768445
f1 0.8098586501193316
======== tgt result =======
===== RES ====
p 571
tp 487
fp 124
recall 0.8528896657567606
precision 0.7970540085154599
f1 0.824026571936652
===== RES ====
p 135
tp 114
fp 30
recall 0.8444444381893005
precision 0.7916666611689815
f1 0.8172037957377702
======== tgt result =======
===== RES ====
p 568
tp 482
fp 120
recall 0.8485915478017755
precision 0.8006644504972351
f1 0.8239311229457336
===== RES ====
p 138
tp 113
fp 27
recall 0.8188405737765176
precision 0.807142851377551
f1 0.8129491344653974
===== RES ====
p 141
tp 119
fp 33
recall 0.8439716252200594
precision 0.782894731691482
f1 0.812286184580209
===== RES ====
p 137
tp 112
fp 31
recall 0.817518242207896
precision 0.7832167777397429
f1 0.7999994945156185
===== RES ====
p 137
tp 120
fp 49
recall 0.875912402365603
precision 0.7100591673960996
f1 0.7843132258322569
===== RES ====
p 138
tp 118
fp 33
recall 0.8550724575719387
precision 0.7814569484671725
f1 0.8166084919005456
===== RES ====
p 143
tp 122
fp 38
recall 0.8531468471807913
precision 0.7624999952343751
f1 0.8052800243116506
===== RES ====
p 139
tp 118
fp 29
recall 0.8489208572020083
precision 0.8027210829746865
f1 0.8251743197958916
======== tgt result =======
===== RES ====
p 568
tp 484
fp 125
recall 0.8521126745561397
precision 0.7947454830956561
f1 0.8224294057515725
===== RES ====
p 138
tp 114
fp 28
recall 0.8260869505356018
precision 0.8028168957548106
f1 0.8142852085717356
===== RES ====
p 138
tp 114
fp 26
recall 0.8260869505356018
precision 0.8142857084693879
f1 0.8201433790179541
===== RES ====
p 137
tp 112
fp 27
recall 0.817518242207896
precision 0.8057553898866519
f1 0.8115936970439963
===== RES ====
p 138
tp 114
fp 26
recall 0.8260869505356018
precision 0.8142857084693879
f1 0.8201433790179541
===== RES ====
p 142
tp 120
fp 27
recall 0.8450704165840112
precision 0.8163265250590033
f1 0.8304493213925256
======== tgt result =======
===== RES ====
p 571
tp 483
fp 127
recall 0.8458844118285738
precision 0.7918032773904864
f1 0.8179503882374295
===== RES ====
p 135
tp 113
fp 26
recall 0.8370370308367627
precision 0.8129496344392113
f1 0.8248170123344711
===== RES ====
p 138
tp 117
fp 30
recall 0.8478260808128545
precision 0.7959183619325282
f1 0.8210521263160935
===== RES ====
p 136
tp 111
fp 24
recall 0.8161764645869378
precision 0.8222222161316873
f1 0.8191876858433631
===== RES ====
p 140
tp 118
fp 24
recall 0.8428571368367348
precision 0.8309859096409443
f1 0.8368789267142468
======== tgt result =======
===== RES ====
p 574
tp 487
fp 124
recall 0.8484320542710243
precision 0.7970540085154599
f1 0.8219404273705613
===== RES ====
p 138
tp 112
fp 22
recall 0.8115941970174334
precision 0.8358208892849187
f1 0.8235289058177776
===== RES ====
p 140
tp 117
fp 24
recall 0.835714279744898
precision 0.8297872281575374
f1 0.8327397076027876
===== RES ====
p 137
tp 115
fp 32
recall 0.8394160522670361
precision 0.7823129198482115
f1 0.8098586498465687
===== RES ====
p 142
tp 119
fp 25
recall 0.8380281631124777
precision 0.8263888831500772
f1 0.8321673263732282
===== RES ====
p 141
tp 116
fp 22
recall 0.8226950296262764
precision 0.8405797040537702
f1 0.8315407127352347
===== RES ====
p 136
tp 112
fp 21
recall 0.8235294057093426
precision 0.8421052568262762
f1 0.8327132485181483
===== RES ====
p 139
tp 119
fp 35
recall 0.8561151017545676
precision 0.7727272677095632
f1 0.8122861851859228
===== RES ====
p 142
tp 119
fp 28
recall 0.8380281631124777
precision 0.8095238040168449
f1 0.8235289062155069
===== RES ====
p 139
tp 119
fp 32
recall 0.8561151017545676
precision 0.7880794649796062
f1 0.8206891503689123
===== RES ====
p 140
tp 117
fp 26
recall 0.835714279744898
precision 0.8181818124602671
f1 0.8268546178879024
===== RES ====
p 137
tp 115
fp 30
recall 0.8394160522670361
precision 0.7931034428061832
f1 0.8156023314977157
===== RES ====
p 136
tp 113
fp 26
recall 0.8308823468317474
precision 0.8129496344392113
f1 0.8218176759011308
===== RES ====
p 138
tp 115
fp 33
recall 0.833333327294686
precision 0.7770270217768445
f1 0.8041952991836443
===== RES ====
p 138
tp 116
fp 29
recall 0.8405797040537702
precision 0.7999999944827587
f1 0.8197874803783841
===== RES ====
p 139
tp 115
fp 24
recall 0.82733812354433
precision 0.82733812354433
f1 0.8273376235446321
===== RES ====
p 136
tp 114
fp 24
recall 0.8382352879541524
precision 0.8260869505356018
f1 0.8321162822742629
===== RES ====
p 139
tp 117
fp 27
recall 0.8417266126494488
precision 0.812499994357639
f1 0.8268546179877913
===== RES ====
p 139
tp 118
fp 31
recall 0.8489208572020083
precision 0.7919463034097564
f1 0.8194439393569788
===== RES ====
p 139
tp 116
fp 25
recall 0.8345323680968895
precision 0.8226950296262764
f1 0.8285709226788732
===== RES ====
p 138
tp 116
fp 28
recall 0.8405797040537702
precision 0.8055555499614198
f1 0.8226945298529268
===== RES ====
p 300
tp 270
fp 122
recall 0.899999997
precision 0.6887755084470013
f1 0.7803463273918023
=== Result of InvGAN+KD: ===
0.7803463273918023
The source-target datasets are: cameras_computers with seed 42
The F1 score is: 0.7803463273918023
The training time is: 739.7403609752655
The inference time is: 0.00026983022689819336
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
usage: main_invgan_kd.py [-h] [--src SRC] [--tgt TGT] [--srcfix SRCFIX]
                         [--tgtfix TGTFIX] [--pretrain] [--adapt]
                         [--seed SEED] [--train_seed TRAIN_SEED] [--load]
                         [--model {bert}] [--max_seq_length MAX_SEQ_LENGTH]
                         [--alpha ALPHA] [--beta BETA]
                         [--temperature TEMPERATURE]
                         [--max_grad_norm MAX_GRAD_NORM]
                         [--clip_value CLIP_VALUE] [--batch_size BATCH_SIZE]
                         [--pre_epochs PRE_EPOCHS] [--epoch EPOCH]
                         [--pre_log_step PRE_LOG_STEP]
                         [--num_epochs NUM_EPOCHS] [--log_step LOG_STEP]
                         [--model_index MODEL_INDEX] [--out_file OUT_FILE]
                         [--d_learning_rate D_LEARNING_RATE]
                         [--rec_epoch REC_EPOCH] [--rec_lr REC_LR]
                         [--epoch_path EPOCH_PATH] [--adda ADDA]
                         [--seed_list SEED_LIST]
                         [--need_kd_model NEED_KD_MODEL]
                         [--need_pred_res NEED_PRED_RES]
main_invgan_kd.py: error: argument --src: expected one argument
usage: main_invgan_kd.py [-h] [--src SRC] [--tgt TGT] [--srcfix SRCFIX]
                         [--tgtfix TGTFIX] [--pretrain] [--adapt]
                         [--seed SEED] [--train_seed TRAIN_SEED] [--load]
                         [--model {bert}] [--max_seq_length MAX_SEQ_LENGTH]
                         [--alpha ALPHA] [--beta BETA]
                         [--temperature TEMPERATURE]
                         [--max_grad_norm MAX_GRAD_NORM]
                         [--clip_value CLIP_VALUE] [--batch_size BATCH_SIZE]
                         [--pre_epochs PRE_EPOCHS] [--epoch EPOCH]
                         [--pre_log_step PRE_LOG_STEP]
                         [--num_epochs NUM_EPOCHS] [--log_step LOG_STEP]
                         [--model_index MODEL_INDEX] [--out_file OUT_FILE]
                         [--d_learning_rate D_LEARNING_RATE]
                         [--rec_epoch REC_EPOCH] [--rec_lr REC_LR]
                         [--epoch_path EPOCH_PATH] [--adda ADDA]
                         [--seed_list SEED_LIST]
                         [--need_kd_model NEED_KD_MODEL]
                         [--need_pred_res NEED_PRED_RES]
main_invgan_kd.py: error: argument --src: expected one argument
