=== Argument Setting ===
src: wa1
tgt: ab
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 203
tp 132
fp 33
recall 0.6502463022155355
precision 0.7999999951515152
f1 0.717390805780705
tgt_res:
===== RES ====
p 614
tp 442
fp 120
recall 0.7198697056679646
precision 0.7864768669279771
f1 0.7517001799716396
save pretrained model to: checkpoint/wa1/bert/3000/source-encoder.ptabbest
save pretrained model to: checkpoint/wa1/bert/3000/source-classifier.ptabbest
===== RES ====
p 203
tp 46
fp 3
recall 0.22660098410541388
precision 0.9387754910453981
f1 0.3650790489105708
===== RES ====
p 202
tp 72
fp 5
recall 0.35643564179982357
precision 0.9350649229212348
f1 0.5161286289233691
===== RES ====
p 203
tp 41
fp 1
recall 0.20197044235482542
precision 0.9761904529478465
f1 0.3346935907374305
===== RES ====
p 200
tp 45
fp 1
recall 0.224999998875
precision 0.9782608482986772
f1 0.36585335151059395
Restore model from: /home/derossi/DADER/main/checkpoint/wa1/bert/3000/source-encoder.ptabbest
Restore model from: /home/derossi/DADER/main/checkpoint/wa1/bert/3000/source-classifier.ptabbest
Pretraining time:  212.50272297859192
=== Training F' and A ===
===== RES ====
p 204
tp 153
fp 80
recall 0.7499999963235294
precision 0.656652357696771
f1 0.7002283319495185
======== tgt result =======
===== RES ====
p 614
tp 497
fp 205
recall 0.8094462527533449
precision 0.7079772069686935
f1 0.7553186500243486
===== RES ====
p 202
tp 136
fp 62
recall 0.6732673233996668
precision 0.6868686833996531
f1 0.6799994966503676
===== RES ====
p 204
tp 149
fp 88
recall 0.7303921532823914
precision 0.6286919804696541
f1 0.6757364611868056
===== RES ====
p 199
tp 142
fp 84
recall 0.7135678356102119
precision 0.6283185812906258
f1 0.6682347929913747
===== RES ====
p 202
tp 132
fp 58
recall 0.6534653432996765
precision 0.6947368384487534
f1 0.6734688847879591
===== RES ====
p 204
tp 143
fp 74
recall 0.7009803887206844
precision 0.6589861720784047
f1 0.6793344141144833
===== RES ====
p 204
tp 145
fp 69
recall 0.7107843102412534
precision 0.6775700902917285
f1 0.6937794012732247
===== RES ====
p 204
tp 135
fp 68
recall 0.6617647026384084
precision 0.6650246272658885
f1 0.6633901601341536
===== RES ====
p 203
tp 132
fp 57
recall 0.6502463022155355
precision 0.6984126947173932
f1 0.6734688849571592
===== RES ====
p 203
tp 136
fp 83
recall 0.6699507356160063
precision 0.6210045633744083
f1 0.6445492606975859
===== RES ====
p 204
tp 132
fp 59
recall 0.6470588203575548
precision 0.6910994728214688
f1 0.6683539275376269
===== RES ====
p 204
tp 125
fp 61
recall 0.6127450950355633
precision 0.6720430071395538
f1 0.6410251388038072
===== RES ====
p 203
tp 139
fp 80
recall 0.6847290606663593
precision 0.6347031934488438
f1 0.6587672701088667
===== RES ====
p 205
tp 151
fp 85
recall 0.7365853622605593
precision 0.63983050576343
f1 0.6848067556011657
===== RES ====
p 200
tp 139
fp 82
recall 0.694999996525
precision 0.62895927317213
f1 0.6603320396751698
===== RES ====
p 206
tp 138
fp 77
recall 0.6699029093694033
precision 0.6418604621308815
f1 0.6555814448579479
===== RES ====
p 202
tp 128
fp 60
recall 0.6336633631996863
precision 0.680851060208239
f1 0.6564097536887429
===== RES ====
p 203
tp 128
fp 70
recall 0.6305418688150647
precision 0.6464646431996736
f1 0.6384034869190053
===== RES ====
p 205
tp 150
fp 99
recall 0.7317073135038668
precision 0.6024096361349011
f1 0.6607924533276238
===== RES ====
p 205
tp 132
fp 65
recall 0.6439024358834028
precision 0.6700507580200469
f1 0.6567159148415972
===== RES ====
p 206
tp 146
fp 51
recall 0.7087378606371949
precision 0.7411167475070216
f1 0.7245652534776768
=== Result of InvGAN+KD: ===
0.7245652534776768
The source-target datasets are: wa1_ab with seed 3000
The F1 score is: 0.7245652534776768
The training time is: 943.5232434272766
The inference time is: 0.00026822835206985474
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ab
tgt: wa1
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 193
tp 180
fp 419
recall 0.9326424822142877
precision 0.30050083422287005
f1 0.45454508479075334
tgt_res:
===== RES ====
p 576
tp 537
fp 1289
recall 0.9322916650481048
precision 0.29408543247859503
f1 0.4471270288743758
save pretrained model to: checkpoint/ab/bert/3000/source-encoder.ptwa1best
save pretrained model to: checkpoint/ab/bert/3000/source-classifier.ptwa1best
===== RES ====
p 193
tp 182
fp 581
recall 0.943005176461113
precision 0.23853210977911912
f1 0.3807528150267071
===== RES ====
p 193
tp 178
fp 474
recall 0.9222797879674622
precision 0.2730061345506041
f1 0.4213014216816079
===== RES ====
p 193
tp 168
fp 351
recall 0.8704663167333352
precision 0.3236994213416196
f1 0.471909715854547
tgt_res:
===== RES ====
p 576
tp 511
fp 1000
recall 0.887152776237582
precision 0.33818663114613723
f1 0.4896977311771252
save pretrained model to: checkpoint/ab/bert/3000/source-encoder.ptwa1best
save pretrained model to: checkpoint/ab/bert/3000/source-classifier.ptwa1best
===== RES ====
p 193
tp 170
fp 442
recall 0.8808290109801605
precision 0.2777777773238925
f1 0.42235988285668535
Restore model from: /home/derossi/DADER/main/checkpoint/ab/bert/3000/source-encoder.ptwa1best
Restore model from: /home/derossi/DADER/main/checkpoint/ab/bert/3000/source-classifier.ptwa1best
Pretraining time:  204.92191219329834
=== Training F' and A ===
===== RES ====
p 192
tp 121
fp 52
recall 0.6302083300509983
precision 0.6994219612750175
f1 0.6630131963524111
======== tgt result =======
===== RES ====
p 576
tp 385
fp 129
recall 0.6684027766173563
precision 0.7490272358968342
f1 0.7064215186704972
===== RES ====
p 193
tp 152
fp 99
recall 0.7875647627587318
precision 0.6055776868303678
f1 0.684684190133053
======== tgt result =======
===== RES ====
p 576
tp 441
fp 217
recall 0.7656249986707899
precision 0.6702127649388864
f1 0.7147482854906027
===== RES ====
p 193
tp 122
fp 65
recall 0.6321243490563505
precision 0.6524064136234952
f1 0.6421047599034363
===== RES ====
p 193
tp 150
fp 92
recall 0.7772020685119064
precision 0.6198347081825012
f1 0.6896546755876172
======== tgt result =======
===== RES ====
p 576
tp 445
fp 254
recall 0.7725694431031781
precision 0.6366237473009675
f1 0.6980387192449499
===== RES ====
p 192
tp 131
fp 66
recall 0.6822916631130642
precision 0.6649746159138344
f1 0.6735213475198826
===== RES ====
p 193
tp 141
fp 70
recall 0.730569944401192
precision 0.6682464423305856
f1 0.698019299517561
======== tgt result =======
===== RES ====
p 576
tp 426
fp 212
recall 0.7395833320493345
precision 0.6677115976995116
f1 0.7018116912520609
===== RES ====
p 193
tp 157
fp 107
recall 0.8134714983757954
precision 0.5946969674443296
f1 0.6870892245980222
===== RES ====
p 193
tp 155
fp 88
recall 0.8031088041289699
precision 0.6378600796795881
f1 0.7110086776263839
======== tgt result =======
===== RES ====
p 576
tp 450
fp 265
recall 0.7812499986436632
precision 0.6293706284903907
f1 0.69713350936417
===== RES ====
p 193
tp 152
fp 80
recall 0.7875647627587318
precision 0.6551724109690844
f1 0.7152936184916932
======== tgt result =======
===== RES ====
p 576
tp 437
fp 227
recall 0.7586805542384019
precision 0.6581325291293185
f1 0.7048382110591441
===== RES ====
p 193
tp 124
fp 43
recall 0.642487043303176
precision 0.7425149656136828
f1 0.6888883876701122
===== RES ====
p 193
tp 154
fp 82
recall 0.7979274570055572
precision 0.6525423701163459
f1 0.7179482196253162
======== tgt result =======
===== RES ====
p 576
tp 430
fp 221
recall 0.7465277764817226
precision 0.6605222724108721
f1 0.700895996243535
===== RES ====
p 193
tp 138
fp 70
recall 0.7150259030309539
precision 0.6634615352718195
f1 0.6882787990128062
===== RES ====
p 193
tp 162
fp 118
recall 0.8393782339928589
precision 0.5785714265051021
f1 0.6849889431950092
===== RES ====
p 193
tp 134
fp 56
recall 0.694300514537303
precision 0.7052631541828255
f1 0.6997383997713013
===== RES ====
p 193
tp 138
fp 64
recall 0.7150259030309539
precision 0.6831683134496618
f1 0.6987336739372265
===== RES ====
p 193
tp 148
fp 78
recall 0.766839374265081
precision 0.6548672537395255
f1 0.7064434138109339
===== RES ====
p 193
tp 152
fp 95
recall 0.7875647627587318
precision 0.6153846128931797
f1 0.6909085952999379
===== RES ====
p 193
tp 144
fp 75
recall 0.7461139857714302
precision 0.657534243572903
f1 0.6990286248118357
===== RES ====
p 193
tp 152
fp 77
recall 0.7875647627587318
precision 0.6637554556167884
f1 0.7203786471443863
======== tgt result =======
===== RES ====
p 576
tp 434
fp 204
recall 0.7534722209141107
precision 0.6802507826328358
f1 0.7149912628942627
===== RES ====
p 193
tp 151
fp 88
recall 0.7823834156353191
precision 0.6317991605364052
f1 0.6990735765071224
===== RES ====
p 193
tp 156
fp 84
recall 0.8082901512523827
precision 0.6499999972916667
f1 0.7205537750804769
=== Result of InvGAN+KD: ===
0.7205537750804769
The source-target datasets are: ab_wa1 with seed 3000
The F1 score is: 0.7205537750804769
The training time is: 1064.3613379001617
The inference time is: 0.0002708733081817627
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ds
tgt: da
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 443
tp 441
fp 34
recall 0.9954853250666246
precision 0.9284210506770083
f1 0.9607838122400915
tgt_res:
===== RES ====
p 1329
tp 1319
fp 99
recall 0.9924755447761658
precision 0.9301833561846379
f1 0.9603198492980807
save pretrained model to: checkpoint/ds/bert/3000/source-encoder.ptdabest
save pretrained model to: checkpoint/ds/bert/3000/source-classifier.ptdabest
===== RES ====
p 440
tp 439
fp 47
recall 0.9977272704597108
precision 0.903292179211333
f1 0.948163646054496
===== RES ====
p 441
tp 439
fp 30
recall 0.9954648503504199
precision 0.9360341131427844
f1 0.9648346631882795
tgt_res:
===== RES ====
p 1328
tp 1314
fp 67
recall 0.9894578305802274
precision 0.9514844308823428
f1 0.9700991672495219
save pretrained model to: checkpoint/ds/bert/3000/source-encoder.ptdabest
save pretrained model to: checkpoint/ds/bert/3000/source-classifier.ptdabest
===== RES ====
p 443
tp 440
fp 39
recall 0.993227988728605
precision 0.9185803738651767
f1 0.9544463533559295
===== RES ====
p 444
tp 443
fp 36
recall 0.9977477455005681
precision 0.9248434218688029
f1 0.95991282474974
Restore model from: /home/derossi/DADER/main/checkpoint/ds/bert/3000/source-encoder.ptdabest
Restore model from: /home/derossi/DADER/main/checkpoint/ds/bert/3000/source-classifier.ptdabest
Pretraining time:  551.3261585235596
=== Training F' and A ===
===== RES ====
p 442
tp 440
fp 32
recall 0.9954751108699659
precision 0.9322033878555013
f1 0.9628003737056635
======== tgt result =======
===== RES ====
p 1329
tp 1316
fp 73
recall 0.9902182084347492
precision 0.9474442037815376
f1 0.9683585870957447
===== RES ====
p 443
tp 440
fp 42
recall 0.993227988728605
precision 0.9128630686455124
f1 0.9513508501834613
===== RES ====
p 443
tp 441
fp 44
recall 0.9954853250666246
precision 0.9092783486406633
f1 0.9504305334588494
===== RES ====
p 442
tp 440
fp 38
recall 0.9954751108699659
precision 0.9205020901244726
f1 0.9565212378168958
===== RES ====
p 444
tp 440
fp 32
recall 0.9909909887590294
precision 0.9322033878555013
f1 0.960698188326188
===== RES ====
p 442
tp 440
fp 31
recall 0.9954751108699659
precision 0.9341825882501431
f1 0.9638549200800604
======== tgt result =======
===== RES ====
p 1329
tp 1317
fp 69
recall 0.9909706538818881
precision 0.9502164495308683
f1 0.9701652453623242
===== RES ====
p 443
tp 409
fp 27
recall 0.9232505622499987
precision 0.9380733923438683
f1 0.9306024558212773
===== RES ====
p 444
tp 442
fp 30
recall 0.9954954932533885
precision 0.9364406759821172
f1 0.9650650005437265
======== tgt result =======
===== RES ====
p 1328
tp 1318
fp 69
recall 0.9924698787707305
precision 0.9502523425016205
f1 0.9709018936279786
===== RES ====
p 444
tp 441
fp 29
recall 0.993243241006209
precision 0.9382978703440471
f1 0.9649885573742466
===== RES ====
p 444
tp 442
fp 35
recall 0.9954954932533885
precision 0.9266247360028832
f1 0.9598257743450509
===== RES ====
p 442
tp 439
fp 29
recall 0.9932126674361704
precision 0.9380341860298415
f1 0.96483466312307
===== RES ====
p 441
tp 439
fp 30
recall 0.9954648503504199
precision 0.9360341131427844
f1 0.9648346631882795
===== RES ====
p 442
tp 439
fp 29
recall 0.9932126674361704
precision 0.9380341860298415
f1 0.96483466312307
===== RES ====
p 441
tp 437
fp 28
recall 0.9909297029684134
precision 0.9397849442155163
f1 0.9646794099213638
===== RES ====
p 443
tp 433
fp 25
recall 0.9774266343624681
precision 0.9454148450973475
f1 0.9611537710352798
===== RES ====
p 442
tp 439
fp 29
recall 0.9932126674361704
precision 0.9380341860298415
f1 0.96483466312307
===== RES ====
p 443
tp 440
fp 28
recall 0.993227988728605
precision 0.9401709381620279
f1 0.9659709581902548
======== tgt result =======
===== RES ====
p 1322
tp 1305
fp 68
recall 0.9871406951685773
precision 0.9504734151853798
f1 0.9684596107778609
===== RES ====
p 442
tp 439
fp 31
recall 0.9932126674361704
precision 0.9340425512041648
f1 0.9627187966059455
===== RES ====
p 442
tp 440
fp 33
recall 0.9954751108699659
precision 0.9302325561728698
f1 0.9617481323517778
===== RES ====
p 442
tp 440
fp 37
recall 0.9954751108699659
precision 0.9224318638942728
f1 0.9575620666502714
===== RES ====
p 444
tp 438
fp 30
recall 0.9864864842646701
precision 0.935897433897655
f1 0.9605258140295766
=== Result of InvGAN+KD: ===
0.9605258140295766
The source-target datasets are: ds_da with seed 3000
The F1 score is: 0.9605258140295766
The training time is: 1246.034507036209
The inference time is: 0.0002686604857444763
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: da
tgt: ds
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 1068
tp 577
fp 20
recall 0.5402621717787807
precision 0.9664991608601354
f1 0.6930926322722168
tgt_res:
===== RES ====
p 3204
tp 1773
fp 74
recall 0.5533707863441415
precision 0.9599350292582918
f1 0.7020387359697203
save pretrained model to: checkpoint/da/bert/3000/source-encoder.ptdsbest
save pretrained model to: checkpoint/da/bert/3000/source-classifier.ptdsbest
===== RES ====
p 1069
tp 641
fp 24
recall 0.599625817961061
precision 0.9639097729866019
f1 0.7393305528173585
tgt_res:
===== RES ====
p 3206
tp 1938
fp 80
recall 0.6044915781021548
precision 0.9603567884240055
f1 0.7419597093415692
save pretrained model to: checkpoint/da/bert/3000/source-encoder.ptdsbest
save pretrained model to: checkpoint/da/bert/3000/source-classifier.ptdsbest
===== RES ====
p 1067
tp 555
fp 13
recall 0.5201499526521556
precision 0.9771126743360692
f1 0.678898628311799
===== RES ====
p 1065
tp 464
fp 5
recall 0.435680750764619
precision 0.9893390170803006
f1 0.6049539423543108
===== RES ====
p 1067
tp 449
fp 6
recall 0.4208059977312033
precision 0.9868131846443666
f1 0.5900127206726613
Restore model from: /home/derossi/DADER/main/checkpoint/da/bert/3000/source-encoder.ptdsbest
Restore model from: /home/derossi/DADER/main/checkpoint/da/bert/3000/source-classifier.ptdsbest
Pretraining time:  297.19905972480774
=== Training F' and A ===
===== RES ====
p 1067
tp 912
fp 70
recall 0.8547328951689476
precision 0.9287169033312456
f1 0.8901898367414582
======== tgt result =======
===== RES ====
p 3205
tp 2769
fp 187
recall 0.863962558232773
precision 0.9367388359483293
f1 0.8988795524648077
===== RES ====
p 1068
tp 884
fp 57
recall 0.827715355030229
precision 0.9394261414033729
f1 0.8800393219286574
===== RES ====
p 1063
tp 874
fp 49
recall 0.8222013162538087
precision 0.9469122416609835
f1 0.880160629493846
===== RES ====
p 1069
tp 941
fp 79
recall 0.8802619262111674
precision 0.9225490187033834
f1 0.9009090255018847
======== tgt result =======
===== RES ====
p 3207
tp 2866
fp 244
recall 0.8936700963848861
precision 0.9215434080638124
f1 0.9073922495538532
===== RES ====
p 1067
tp 857
fp 45
recall 0.8031865034646799
precision 0.9501108636916731
f1 0.8704921384829762
===== RES ====
p 1068
tp 907
fp 51
recall 0.8492509355344091
precision 0.9467640908697661
f1 0.8953598164837223
===== RES ====
p 1069
tp 943
fp 92
recall 0.8821328335995015
precision 0.9111111102308104
f1 0.8963873319783867
===== RES ====
p 1067
tp 947
fp 138
recall 0.8875351444352997
precision 0.8728110591033998
f1 0.8801110233808861
===== RES ====
p 1067
tp 951
fp 83
recall 0.8912839729228828
precision 0.9197292060737627
f1 0.9052826977387796
======== tgt result =======
===== RES ====
p 3206
tp 2861
fp 262
recall 0.8923892698401781
precision 0.9161063077438021
f1 0.9040917734614957
===== RES ====
p 1066
tp 980
fp 109
recall 0.9193245769987574
precision 0.8999081718090833
f1 0.909512260234016
======== tgt result =======
===== RES ====
p 3206
tp 2949
fp 306
recall 0.9198378038303687
precision 0.9059907831318001
f1 0.9128612858476789
===== RES ====
p 1070
tp 873
fp 81
recall 0.8158878497047777
precision 0.9150943386634232
f1 0.8626477221340894
===== RES ====
p 1065
tp 942
fp 103
recall 0.884507041422998
precision 0.9014354058359468
f1 0.8928904944595238
===== RES ====
p 1066
tp 923
fp 84
recall 0.8658536577243399
precision 0.9165839117015055
f1 0.8904963639938215
===== RES ====
p 1068
tp 962
fp 130
recall 0.900749062827014
precision 0.8809523801456479
f1 0.8907402399779898
===== RES ====
p 1066
tp 933
fp 107
recall 0.8752345207549393
precision 0.8971153837527736
f1 0.8860393852749325
===== RES ====
p 1068
tp 957
fp 91
recall 0.8960674148913226
precision 0.9131679380599541
f1 0.9045363611937753
===== RES ====
p 1068
tp 975
fp 89
recall 0.9129213474598113
precision 0.9163533825974122
f1 0.914633645485491
======== tgt result =======
===== RES ====
p 3205
tp 2922
fp 309
recall 0.9117004677342588
precision 0.9043639737219549
f1 0.9080169018393809
===== RES ====
p 1063
tp 915
fp 53
recall 0.860771400883564
precision 0.9452479329078017
f1 0.9010334736190571
===== RES ====
p 1068
tp 969
fp 91
recall 0.9073033699369818
precision 0.9141509425338198
f1 0.9107137848656922
===== RES ====
p 1067
tp 850
fp 30
recall 0.7966260536114095
precision 0.965909089811467
f1 0.8731376649894694
===== RES ====
p 1070
tp 859
fp 27
recall 0.802803737567473
precision 0.9695259582736727
f1 0.878322611911195
=== Result of InvGAN+KD: ===
0.878322611911195
The source-target datasets are: da_ds with seed 3000
The F1 score is: 0.878322611911195
The training time is: 2151.067756652832
The inference time is: 0.0002694353461265564
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: wa1
tgt: ab
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 201
tp 103
fp 12
recall 0.5124378083958318
precision 0.8956521661247638
f1 0.6518982670849301
tgt_res:
===== RES ====
p 614
tp 347
fp 48
recall 0.5651465788841261
precision 0.8784810104342253
f1 0.6878092347783094
save pretrained model to: checkpoint/wa1/bert/1000/source-encoder.ptabbest
save pretrained model to: checkpoint/wa1/bert/1000/source-classifier.ptabbest
===== RES ====
p 203
tp 115
fp 11
recall 0.5665024602635347
precision 0.9126984054547745
f1 0.6990876690351582
tgt_res:
===== RES ====
p 614
tp 393
fp 40
recall 0.6400651455373532
precision 0.907621245017041
f1 0.750715845887366
save pretrained model to: checkpoint/wa1/bert/1000/source-encoder.ptabbest
save pretrained model to: checkpoint/wa1/bert/1000/source-classifier.ptabbest
===== RES ====
p 203
tp 102
fp 10
recall 0.5024630517120047
precision 0.9107142775829082
f1 0.6476185852358995
===== RES ====
p 202
tp 84
fp 3
recall 0.41584158209979416
precision 0.9655172302814111
f1 0.5813144540418022
===== RES ====
p 203
tp 54
fp 1
recall 0.2660098509063554
precision 0.9818181639669425
f1 0.41860431245144125
Restore model from: /home/derossi/DADER/main/checkpoint/wa1/bert/1000/source-encoder.ptabbest
Restore model from: /home/derossi/DADER/main/checkpoint/wa1/bert/1000/source-classifier.ptabbest
Pretraining time:  217.98787212371826
=== Training F' and A ===
===== RES ====
p 205
tp 140
fp 31
recall 0.6829268259369423
precision 0.8187134455045998
f1 0.7446803511914801
======== tgt result =======
===== RES ====
p 615
tp 460
fp 116
recall 0.7479674784585895
precision 0.7986111097246335
f1 0.7724596167875765
===== RES ====
p 205
tp 126
fp 32
recall 0.6146341433432481
precision 0.7974683493831117
f1 0.6942143805906236
===== RES ====
p 205
tp 145
fp 55
recall 0.7073170697204045
precision 0.724999996375
f1 0.71604887925656
===== RES ====
p 203
tp 154
fp 59
recall 0.7586206859181247
precision 0.7230046914412925
f1 0.7403841121143351
===== RES ====
p 203
tp 140
fp 46
recall 0.689655169016477
precision 0.7526881679963002
f1 0.7197938417275352
===== RES ====
p 201
tp 138
fp 47
recall 0.6865671607633475
precision 0.7459459419138057
f1 0.7150254038903857
===== RES ====
p 201
tp 138
fp 51
recall 0.6865671607633475
precision 0.7301587262954565
f1 0.7076918045368419
===== RES ====
p 204
tp 141
fp 49
recall 0.6911764672001154
precision 0.7421052592520776
f1 0.7157355376076044
===== RES ====
p 203
tp 128
fp 42
recall 0.6305418688150647
precision 0.7529411720415226
f1 0.6863265779819441
===== RES ====
p 204
tp 147
fp 53
recall 0.7205882317618224
precision 0.734999996325
f1 0.727722268723998
===== RES ====
p 202
tp 124
fp 38
recall 0.6138613830996962
precision 0.7654320940405427
f1 0.6813181836134483
===== RES ====
p 203
tp 132
fp 42
recall 0.6502463022155355
precision 0.7586206852952835
f1 0.7002647512333876
===== RES ====
p 203
tp 141
fp 50
recall 0.6945812773665947
precision 0.7382198914229325
f1 0.7157355374401179
===== RES ====
p 204
tp 142
fp 57
recall 0.6960784279603999
precision 0.7135678356102119
f1 0.7047141367784889
===== RES ====
p 203
tp 133
fp 44
recall 0.6551724105656531
precision 0.7514124251332631
f1 0.6999994986568635
===== RES ====
p 204
tp 145
fp 58
recall 0.7107843102412534
precision 0.7142857107670655
f1 0.7125302090327026
===== RES ====
p 203
tp 142
fp 45
recall 0.6995073857167124
precision 0.759358284709314
f1 0.7282046253126367
===== RES ====
p 204
tp 136
fp 46
recall 0.6666666633986928
precision 0.747252743146963
f1 0.7046627104086858
===== RES ====
p 201
tp 141
fp 62
recall 0.7014925338234203
precision 0.6945812773665947
f1 0.6980192985372663
===== RES ====
p 203
tp 137
fp 45
recall 0.6748768439661239
precision 0.7527472486112788
f1 0.7116878094791821
===== RES ====
p 206
tp 151
fp 37
recall 0.7330097051795645
precision 0.803191485089407
f1 0.7664969590819812
=== Result of InvGAN+KD: ===
0.7664969590819812
The source-target datasets are: wa1_ab with seed 1000
The F1 score is: 0.7664969590819812
The training time is: 946.2820162773132
The inference time is: 0.0002709329128265381
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ab
tgt: wa1
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 193
tp 191
fp 1058
recall 0.9896373005718274
precision 0.1529223377478604
f1 0.26490961521042994
tgt_res:
===== RES ====
p 576
tp 564
fp 3085
recall 0.9791666649667246
precision 0.15456289390118857
f1 0.26698201290429346
save pretrained model to: checkpoint/ab/bert/1000/source-encoder.ptwa1best
save pretrained model to: checkpoint/ab/bert/1000/source-classifier.ptwa1best
===== RES ====
p 192
tp 189
fp 1026
recall 0.9843749948730469
precision 0.15555555542752628
f1 0.2686564803580876
tgt_res:
===== RES ====
p 576
tp 566
fp 3034
recall 0.9826388871829186
precision 0.1572222221785494
f1 0.27107255899312177
save pretrained model to: checkpoint/ab/bert/1000/source-encoder.ptwa1best
save pretrained model to: checkpoint/ab/bert/1000/source-classifier.ptwa1best
===== RES ====
p 193
tp 186
fp 677
recall 0.963730564954764
precision 0.2155272303412199
f1 0.3522724278816948
tgt_res:
===== RES ====
p 576
tp 553
fp 1954
recall 0.9600694427776573
precision 0.22058236927778926
f1 0.3587411814831761
save pretrained model to: checkpoint/ab/bert/1000/source-encoder.ptwa1best
save pretrained model to: checkpoint/ab/bert/1000/source-classifier.ptwa1best
===== RES ====
p 193
tp 169
fp 333
recall 0.8756476638567479
precision 0.33665338578355897
f1 0.4863305326891146
tgt_res:
===== RES ====
p 576
tp 522
fp 991
recall 0.9062499984266493
precision 0.3450099138499604
f1 0.49976025114510375
save pretrained model to: checkpoint/ab/bert/1000/source-encoder.ptwa1best
save pretrained model to: checkpoint/ab/bert/1000/source-classifier.ptwa1best
===== RES ====
p 193
tp 185
fp 834
recall 0.9585492178313512
precision 0.1815505395666825
f1 0.3052802597827718
Restore model from: /home/derossi/DADER/main/checkpoint/ab/bert/1000/source-encoder.ptwa1best
Restore model from: /home/derossi/DADER/main/checkpoint/ab/bert/1000/source-classifier.ptwa1best
Pretraining time:  222.47093319892883
=== Training F' and A ===
===== RES ====
p 193
tp 149
fp 73
recall 0.7720207213884936
precision 0.6711711681478776
f1 0.7180717881379554
======== tgt result =======
===== RES ====
p 576
tp 428
fp 196
recall 0.7430555542655286
precision 0.6858974347982413
f1 0.7133328329447939
===== RES ====
p 193
tp 137
fp 77
recall 0.7098445559075411
precision 0.6401869128963228
f1 0.6732181712419706
===== RES ====
p 193
tp 158
fp 93
recall 0.818652845499208
precision 0.6294820692052507
f1 0.7117112170383233
===== RES ====
p 193
tp 154
fp 83
recall 0.7979274570055572
precision 0.6497890267941391
f1 0.7162785716715153
===== RES ====
p 193
tp 156
fp 59
recall 0.8082901512523827
precision 0.72558139197404
f1 0.7647053800584758
======== tgt result =======
===== RES ====
p 576
tp 412
fp 202
recall 0.7152777765359761
precision 0.671009770894121
f1 0.6924364741363678
===== RES ====
p 192
tp 159
fp 88
recall 0.8281249956868489
precision 0.6437246937501024
f1 0.7243730808581547
===== RES ====
p 193
tp 161
fp 78
recall 0.8341968868694462
precision 0.6736401645454386
f1 0.7453698725890625
===== RES ====
p 193
tp 153
fp 66
recall 0.7927461098821446
precision 0.6986301337962094
f1 0.7427179449880802
===== RES ====
p 193
tp 158
fp 79
recall 0.818652845499208
precision 0.6666666638537272
f1 0.7348832227477641
===== RES ====
p 193
tp 139
fp 38
recall 0.7202072501543666
precision 0.7853107300264931
f1 0.7513508482253132
===== RES ====
p 193
tp 157
fp 59
recall 0.8134714983757954
precision 0.726851848486797
f1 0.7677256591965264
======== tgt result =======
===== RES ====
p 576
tp 416
fp 190
recall 0.7222222209683642
precision 0.6864686457318999
f1 0.7038912080992781
===== RES ====
p 193
tp 161
fp 64
recall 0.8341968868694462
precision 0.7155555523753087
f1 0.7703344274745086
======== tgt result =======
===== RES ====
p 576
tp 426
fp 208
recall 0.7395833320493345
precision 0.6719242891610027
f1 0.7041317313902857
===== RES ====
p 193
tp 161
fp 74
recall 0.8341968868694462
precision 0.6851063800633771
f1 0.7523359498976969
===== RES ====
p 193
tp 153
fp 54
recall 0.7927461098821446
precision 0.7391304312119303
f1 0.7649994967878261
===== RES ====
p 193
tp 156
fp 65
recall 0.8082901512523827
precision 0.7058823497471387
f1 0.7536226870525354
===== RES ====
p 193
tp 156
fp 66
recall 0.8082901512523827
precision 0.7027026995373752
f1 0.7518067277343982
===== RES ====
p 193
tp 153
fp 64
recall 0.7927461098821446
precision 0.7050691211747967
f1 0.7463409614875427
===== RES ====
p 192
tp 154
fp 70
recall 0.802083329155816
precision 0.6874999969308035
f1 0.7403841147839876
===== RES ====
p 192
tp 158
fp 74
recall 0.8229166623806424
precision 0.6810344798231273
f1 0.7452825198027504
===== RES ====
p 193
tp 159
fp 69
recall 0.8238341926226208
precision 0.6973684179939982
f1 0.7553439179199951
===== RES ====
p 193
tp 163
fp 77
recall 0.8445595811162716
precision 0.6791666638368056
f1 0.7528863384415129
=== Result of InvGAN+KD: ===
0.7528863384415129
The source-target datasets are: ab_wa1 with seed 1000
The F1 score is: 0.7528863384415129
The training time is: 1034.8862109184265
The inference time is: 0.00026712566614151
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ds
tgt: da
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 443
tp 443
fp 77
recall 0.9999999977426637
precision 0.8519230752847633
f1 0.9200410381501309
tgt_res:
===== RES ====
p 1328
tp 1324
fp 202
recall 0.9969879510564849
precision 0.8676277844904142
f1 0.9278201044195429
save pretrained model to: checkpoint/ds/bert/1000/source-encoder.ptdabest
save pretrained model to: checkpoint/ds/bert/1000/source-classifier.ptdabest
===== RES ====
p 443
tp 439
fp 29
recall 0.9909706523905855
precision 0.9380341860298415
f1 0.9637755685134067
tgt_res:
===== RES ====
p 1328
tp 1309
fp 61
recall 0.9856927703420988
precision 0.9554744518573178
f1 0.9703479056289498
save pretrained model to: checkpoint/ds/bert/1000/source-encoder.ptdabest
save pretrained model to: checkpoint/ds/bert/1000/source-classifier.ptdabest
===== RES ====
p 443
tp 440
fp 33
recall 0.993227988728605
precision 0.9302325561728698
f1 0.9606981883953132
===== RES ====
p 442
tp 441
fp 32
recall 0.9977375543037612
precision 0.9323467210732628
f1 0.9639339246967263
tgt_res:
===== RES ====
p 1327
tp 1313
fp 71
recall 0.9894498862174453
precision 0.9486994212798414
f1 0.9686457555007864
save pretrained model to: checkpoint/ds/bert/1000/source-encoder.ptdabest
save pretrained model to: checkpoint/ds/bert/1000/source-classifier.ptdabest
===== RES ====
p 443
tp 442
fp 46
recall 0.9977426614046441
precision 0.905737703062013
f1 0.9495161478933953
Restore model from: /home/derossi/DADER/main/checkpoint/ds/bert/1000/source-encoder.ptdabest
Restore model from: /home/derossi/DADER/main/checkpoint/ds/bert/1000/source-classifier.ptdabest
Pretraining time:  559.2939298152924
=== Training F' and A ===
===== RES ====
p 443
tp 442
fp 38
recall 0.9977426614046441
precision 0.9208333314149305
f1 0.95774597760168
======== tgt result =======
===== RES ====
p 1327
tp 1311
fp 87
recall 0.9879427272133061
precision 0.9377682396725548
f1 0.9622013344958743
===== RES ====
p 443
tp 442
fp 40
recall 0.9977426614046441
precision 0.9170124462302647
f1 0.9556751744984345
===== RES ====
p 443
tp 441
fp 41
recall 0.9954853250666246
precision 0.9149377574378885
f1 0.9535130123409479
===== RES ====
p 442
tp 441
fp 42
recall 0.9977375543037612
precision 0.9130434763705104
f1 0.9535130124344467
===== RES ====
p 443
tp 441
fp 36
recall 0.9954853250666246
precision 0.924528299948578
f1 0.9586951507729444
======== tgt result =======
===== RES ====
p 1326
tp 1308
fp 77
recall 0.9864253386226053
precision 0.9444043314480834
f1 0.9649570797538917
===== RES ====
p 442
tp 438
fp 37
recall 0.9909502240023751
precision 0.9221052612166205
f1 0.9552884843876178
===== RES ====
p 444
tp 444
fp 42
recall 0.9999999977477477
precision 0.9135802450337855
f1 0.9548382086440347
===== RES ====
p 441
tp 441
fp 42
recall 0.9999999977324263
precision 0.9130434763705104
f1 0.9545449535126576
===== RES ====
p 443
tp 439
fp 45
recall 0.9909706523905855
precision 0.9070247915144116
f1 0.9471408150082518
===== RES ====
p 439
tp 438
fp 40
recall 0.9977220933992663
precision 0.9163179897148159
f1 0.9552884846444885
===== RES ====
p 444
tp 444
fp 46
recall 0.9999999977477477
precision 0.9061224471303624
f1 0.9507489638452975
===== RES ====
p 444
tp 443
fp 46
recall 0.9977477455005681
precision 0.9059304684950297
f1 0.9496243651513437
===== RES ====
p 442
tp 440
fp 42
recall 0.9954751108699659
precision 0.9128630686455124
f1 0.9523804512567974
===== RES ====
p 443
tp 442
fp 46
recall 0.9977426614046441
precision 0.905737703062013
f1 0.9495161478933953
===== RES ====
p 442
tp 441
fp 40
recall 0.9977375543037612
precision 0.9168399149338048
f1 0.9555791304583153
===== RES ====
p 442
tp 437
fp 40
recall 0.9886877805685796
precision 0.9161425557313573
f1 0.9510332309735134
===== RES ====
p 442
tp 440
fp 46
recall 0.9954751108699659
precision 0.9053497923758235
f1 0.948275361149564
===== RES ====
p 442
tp 438
fp 41
recall 0.9909502240023751
precision 0.9144050085294259
f1 0.9511395638883552
===== RES ====
p 442
tp 437
fp 41
recall 0.9886877805685796
precision 0.9142259395099875
f1 0.9499994987006404
===== RES ====
p 443
tp 436
fp 40
recall 0.9841986433765267
precision 0.9159663846303228
f1 0.9488569523340789
===== RES ====
p 444
tp 437
fp 30
recall 0.9842342320174905
precision 0.9357601693024408
f1 0.9593847891018736
=== Result of InvGAN+KD: ===
0.9593847891018736
The source-target datasets are: ds_da with seed 1000
The F1 score is: 0.9593847891018736
The training time is: 1231.1440925598145
The inference time is: 0.0002696588635444641
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: da
tgt: ds
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 1065
tp 733
fp 17
recall 0.6882629101518657
precision 0.9773333320302222
f1 0.8077130127932848
tgt_res:
===== RES ====
p 3205
tp 2235
fp 72
recall 0.6973478936981754
precision 0.9687906367712221
f1 0.8109574229915729
save pretrained model to: checkpoint/da/bert/1000/source-encoder.ptdsbest
save pretrained model to: checkpoint/da/bert/1000/source-classifier.ptdsbest
===== RES ====
p 1067
tp 604
fp 11
recall 0.5660731016250485
precision 0.9821138195412783
f1 0.7181921630776337
===== RES ====
p 1067
tp 543
fp 5
recall 0.5089034671894063
precision 0.9908759106005914
f1 0.6724453712378262
===== RES ====
p 1068
tp 471
fp 3
recall 0.4410112355421243
precision 0.9936708839795972
f1 0.6108945150371128
===== RES ====
p 1067
tp 509
fp 3
recall 0.47703842504494987
precision 0.9941406230583191
f1 0.6447114038942654
Restore model from: /home/derossi/DADER/main/checkpoint/da/bert/1000/source-encoder.ptdsbest
Restore model from: /home/derossi/DADER/main/checkpoint/da/bert/1000/source-classifier.ptdsbest
Pretraining time:  289.0335576534271
=== Training F' and A ===
===== RES ====
p 1066
tp 909
fp 51
recall 0.8527204494815004
precision 0.9468749990136719
f1 0.8973341500389159
======== tgt result =======
===== RES ====
p 3206
tp 2749
fp 149
recall 0.8574547720344807
precision 0.9485852308666027
f1 0.9007203397724257
===== RES ====
p 1066
tp 956
fp 72
recall 0.8968105057253184
precision 0.9299610885895319
f1 0.9130845040683847
======== tgt result =======
===== RES ====
p 3207
tp 2902
fp 211
recall 0.904895540721891
precision 0.9322197234396982
f1 0.9183539302000098
===== RES ====
p 1067
tp 980
fp 86
recall 0.9184629794578603
precision 0.9193245769987574
f1 0.9188930762602097
======== tgt result =======
===== RES ====
p 3207
tp 2957
fp 245
recall 0.9220455251256484
precision 0.923485321385545
f1 0.9227643616255512
===== RES ====
p 1068
tp 999
fp 90
recall 0.9353932575511298
precision 0.9173553710584431
f1 0.9262860082291303
======== tgt result =======
===== RES ====
p 3207
tp 2990
fp 257
recall 0.9323355157679029
precision 0.9208500151152295
f1 0.9265566735780262
===== RES ====
p 1069
tp 971
fp 78
recall 0.908325537036178
precision 0.9256434690889956
f1 0.9169022376115205
===== RES ====
p 1066
tp 885
fp 46
recall 0.8302063782080614
precision 0.950590761599795
f1 0.8863289956389552
===== RES ====
p 1066
tp 965
fp 73
recall 0.905253282452858
precision 0.9296724461178492
f1 0.917299879445002
===== RES ====
p 1067
tp 1016
fp 115
recall 0.9522024358461082
precision 0.8983200699395931
f1 0.9244762966712446
===== RES ====
p 1070
tp 940
fp 68
recall 0.8785046720761638
precision 0.9325396816145439
f1 0.904715572721881
===== RES ====
p 1067
tp 971
fp 84
recall 0.9100281153607983
precision 0.9203791460470339
f1 0.915173862961433
===== RES ====
p 1070
tp 984
fp 96
recall 0.9196261673648353
precision 0.9111111102674896
f1 0.9153483363689048
===== RES ====
p 1067
tp 944
fp 73
recall 0.8847235230696123
precision 0.9282202547411796
f1 0.9059495953879473
===== RES ====
p 1066
tp 910
fp 64
recall 0.8536585357845604
precision 0.934291580149598
f1 0.8921563628876301
===== RES ====
p 1065
tp 941
fp 63
recall 0.8835680742877294
precision 0.9372509950824194
f1 0.9096176725860599
===== RES ====
p 1069
tp 996
fp 119
recall 0.9317118793903536
precision 0.8932735417997546
f1 0.9120874114747501
===== RES ====
p 1070
tp 858
fp 43
recall 0.8018691581290942
precision 0.9522752486656213
f1 0.8706235514990445
===== RES ====
p 1067
tp 942
fp 71
recall 0.8828491088258208
precision 0.929911154067215
f1 0.9057687302355752
===== RES ====
p 1069
tp 983
fp 102
recall 0.9195509813661823
precision 0.9059907825751236
f1 0.9127200191432558
===== RES ====
p 1069
tp 931
fp 52
recall 0.8709073892694972
precision 0.9471007111423188
f1 0.9074069074015072
===== RES ====
p 1068
tp 990
fp 76
recall 0.9269662912668853
precision 0.928705440029357
f1 0.9278345506775266
======== tgt result =======
===== RES ====
p 3207
tp 2937
fp 260
recall 0.9158091671606458
precision 0.9186737563595015
f1 0.9172387251991061
===== RES ====
p 1070
tp 988
fp 91
recall 0.9233644851183509
precision 0.915662649753788
f1 0.9194969398233761
=== Result of InvGAN+KD: ===
0.9194969398233761
The source-target datasets are: da_ds with seed 1000
The F1 score is: 0.9194969398233761
The training time is: 2154.015390396118
The inference time is: 0.0002688989043235779
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: wa1
tgt: ab
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 205
tp 110
fp 18
recall 0.536585363236169
precision 0.8593749932861329
f1 0.6606601834270086
tgt_res:
===== RES ====
p 615
tp 390
fp 58
recall 0.6341463404322824
precision 0.8705357123425542
f1 0.7337718533874524
save pretrained model to: checkpoint/wa1/bert/42/source-encoder.ptabbest
save pretrained model to: checkpoint/wa1/bert/42/source-classifier.ptabbest
===== RES ====
p 203
tp 60
fp 3
recall 0.29556650100706156
precision 0.9523809372637947
f1 0.45112745466137644
===== RES ====
p 202
tp 49
fp 0
recall 0.24257425622487994
precision 0.9999999795918372
f1 0.3904379296839058
===== RES ====
p 202
tp 79
fp 3
recall 0.3910891069748064
precision 0.9634146223973827
f1 0.5563376135194451
===== RES ====
p 202
tp 96
fp 10
recall 0.47524752239976475
precision 0.9056603688145248
f1 0.6233761679038514
Restore model from: /home/derossi/DADER/main/checkpoint/wa1/bert/42/source-encoder.ptabbest
Restore model from: /home/derossi/DADER/main/checkpoint/wa1/bert/42/source-classifier.ptabbest
Pretraining time:  209.750097990036
=== Training F' and A ===
===== RES ====
p 204
tp 145
fp 55
recall 0.7107843102412534
precision 0.724999996375
f1 0.7178212786740077
======== tgt result =======
===== RES ====
p 615
tp 488
fp 187
recall 0.7934959336691123
precision 0.7229629618919067
f1 0.7565886471958101
===== RES ====
p 204
tp 130
fp 27
recall 0.6372548988369858
precision 0.8280254724329588
f1 0.7202211111336141
======== tgt result =======
===== RES ====
p 615
tp 436
fp 98
recall 0.7089430882781413
precision 0.8164793992200761
f1 0.7589203018604218
===== RES ====
p 203
tp 124
fp 31
recall 0.6108374354145939
precision 0.7999999948387098
f1 0.6927369352863878
===== RES ====
p 200
tp 127
fp 75
recall 0.6349999968250001
precision 0.6287128681746887
f1 0.6318402928891856
===== RES ====
p 206
tp 134
fp 76
recall 0.6504854337355076
precision 0.6380952350566894
f1 0.6442302661801218
===== RES ====
p 203
tp 123
fp 56
recall 0.6059113270644763
precision 0.6871508341499953
f1 0.6439785561940203
===== RES ====
p 202
tp 122
fp 42
recall 0.6039603930497011
precision 0.7439024344883998
f1 0.6666661684138758
===== RES ====
p 203
tp 120
fp 46
recall 0.5911330020141231
precision 0.7228915619102918
f1 0.6504060055673158
===== RES ====
p 204
tp 119
fp 52
recall 0.5833333304738563
precision 0.6959064286789098
f1 0.6346661671541658
===== RES ====
p 202
tp 129
fp 61
recall 0.6386138582246839
precision 0.6789473648476455
f1 0.6581627624170805
===== RES ====
p 201
tp 118
fp 50
recall 0.5870646736961956
precision 0.7023809482001134
f1 0.6395658961967993
===== RES ====
p 204
tp 124
fp 52
recall 0.6078431342752788
precision 0.7045454505423554
f1 0.6526310782275259
===== RES ====
p 202
tp 121
fp 59
recall 0.5990098980247035
precision 0.6722222184876544
f1 0.633507351745136
===== RES ====
p 202
tp 117
fp 50
recall 0.5792079179247133
precision 0.7005987982000073
f1 0.6341458425250455
===== RES ====
p 205
tp 116
fp 51
recall 0.5658536557763236
precision 0.6946107742837678
f1 0.6236554158432647
===== RES ====
p 204
tp 121
fp 45
recall 0.5931372519944252
precision 0.7289156582595442
f1 0.6540535557929236
===== RES ====
p 203
tp 134
fp 72
recall 0.6600985189157709
precision 0.6504854337355076
f1 0.6552562205394744
===== RES ====
p 204
tp 122
fp 44
recall 0.5980392127547097
precision 0.7349397546087967
f1 0.6594589611691074
===== RES ====
p 202
tp 129
fp 68
recall 0.6386138582246839
precision 0.6548223317014095
f1 0.6466160381911015
===== RES ====
p 202
tp 101
fp 31
recall 0.4999999975247525
precision 0.7651515093549128
f1 0.6047899375026188
===== RES ====
p 206
tp 116
fp 44
recall 0.5631067933829768
precision 0.72499999546875
f1 0.633879285855439
=== Result of InvGAN+KD: ===
0.633879285855439
The source-target datasets are: wa1_ab with seed 42
The F1 score is: 0.633879285855439
The training time is: 948.9878087043762
The inference time is: 0.0002668127417564392
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ab
tgt: wa1
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 193
tp 181
fp 533
recall 0.9378238293377004
precision 0.25350140020518014
f1 0.3991176354341047
tgt_res:
===== RES ====
p 576
tp 525
fp 1510
recall 0.9114583317509404
precision 0.25798525785848386
f1 0.40214442793320176
save pretrained model to: checkpoint/ab/bert/42/source-encoder.ptwa1best
save pretrained model to: checkpoint/ab/bert/42/source-classifier.ptwa1best
===== RES ====
p 193
tp 174
fp 398
recall 0.9015543994738114
precision 0.30419580366399335
f1 0.45490158231822414
tgt_res:
===== RES ====
p 576
tp 518
fp 1136
recall 0.8993055539942612
precision 0.31318016909723084
f1 0.46457360745672954
save pretrained model to: checkpoint/ab/bert/42/source-encoder.ptwa1best
save pretrained model to: checkpoint/ab/bert/42/source-classifier.ptwa1best
===== RES ====
p 193
tp 183
fp 566
recall 0.9481865235845258
precision 0.24432576736405104
f1 0.38853470521031036
===== RES ====
p 193
tp 182
fp 469
recall 0.943005176461113
precision 0.27956989204367144
f1 0.4312792670680761
===== RES ====
p 193
tp 175
fp 514
recall 0.9067357465972241
precision 0.2539912913585032
f1 0.3968250540492755
Restore model from: /home/derossi/DADER/main/checkpoint/ab/bert/42/source-encoder.ptwa1best
Restore model from: /home/derossi/DADER/main/checkpoint/ab/bert/42/source-classifier.ptwa1best
Pretraining time:  207.6567668914795
=== Training F' and A ===
===== RES ====
p 193
tp 140
fp 82
recall 0.7253885972777793
precision 0.6306306277899522
f1 0.674698294371103
======== tgt result =======
===== RES ====
p 576
tp 414
fp 232
recall 0.7187499987521702
precision 0.6408668720729614
f1 0.6775772419396092
===== RES ====
p 193
tp 149
fp 109
recall 0.7720207213884936
precision 0.5775193776065141
f1 0.6607533877221538
===== RES ====
p 193
tp 146
fp 104
recall 0.7564766800182555
precision 0.583999997664
f1 0.659141717491921
===== RES ====
p 192
tp 121
fp 66
recall 0.6302083300509983
precision 0.6470588200692042
f1 0.6385219241585357
===== RES ====
p 193
tp 116
fp 47
recall 0.6010362663158743
precision 0.7116564373518011
f1 0.6516848931483393
===== RES ====
p 193
tp 155
fp 86
recall 0.8031088041289699
precision 0.6431535243022676
f1 0.7142852171104946
======== tgt result =======
===== RES ====
p 576
tp 442
fp 239
recall 0.7673611097788869
precision 0.6490455203391402
f1 0.7032612366582011
===== RES ====
p 193
tp 149
fp 78
recall 0.7720207213884936
precision 0.6563876623066622
f1 0.7095233094221164
===== RES ====
p 193
tp 162
fp 94
recall 0.8393782339928589
precision 0.6328124975280762
f1 0.7216030701041518
======== tgt result =======
===== RES ====
p 576
tp 451
fp 260
recall 0.7829861097517602
precision 0.6343178612738145
f1 0.700854205267412
===== RES ====
p 192
tp 153
fp 84
recall 0.7968749958496094
precision 0.6455696175292421
f1 0.7132862154632017
===== RES ====
p 193
tp 154
fp 86
recall 0.7979274570055572
precision 0.6416666639930556
f1 0.7113158998344669
===== RES ====
p 193
tp 149
fp 84
recall 0.7720207213884936
precision 0.6394849757962018
f1 0.6995300175563917
===== RES ====
p 193
tp 158
fp 105
recall 0.818652845499208
precision 0.600760453989504
f1 0.6929819648837695
===== RES ====
p 193
tp 145
fp 74
recall 0.7512953328948428
precision 0.6621004535977149
f1 0.7038829937203072
===== RES ====
p 193
tp 142
fp 59
recall 0.7357512915246047
precision 0.7064676581767778
f1 0.7208116792886561
===== RES ====
p 193
tp 161
fp 111
recall 0.8341968868694462
precision 0.5919117625297362
f1 0.6924726297332575
===== RES ====
p 193
tp 148
fp 68
recall 0.766839374265081
precision 0.6851851820130316
f1 0.7237158794606526
======== tgt result =======
===== RES ====
p 576
tp 408
fp 207
recall 0.708333332103588
precision 0.6634146330676185
f1 0.6851380384287956
===== RES ====
p 193
tp 151
fp 70
recall 0.7823834156353191
precision 0.6832579154603714
f1 0.7294680977972543
======== tgt result =======
===== RES ====
p 576
tp 416
fp 212
recall 0.7222222209683642
precision 0.6624203811107956
f1 0.691029400117357
===== RES ====
p 192
tp 148
fp 77
recall 0.7708333293185764
precision 0.657777774854321
f1 0.7098316340197509
===== RES ====
p 192
tp 157
fp 95
recall 0.8177083290744358
precision 0.6230158705435879
f1 0.7072067131526821
===== RES ====
p 193
tp 140
fp 63
recall 0.7253885972777793
precision 0.689655169016477
f1 0.7070702038188411
===== RES ====
p 193
tp 147
fp 70
recall 0.7616580271416683
precision 0.6774193517169615
f1 0.7170726689474016
=== Result of InvGAN+KD: ===
0.7170726689474016
The source-target datasets are: ab_wa1 with seed 42
The F1 score is: 0.7170726689474016
The training time is: 1039.8752791881561
The inference time is: 0.0002686530351638794
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ds
tgt: da
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 443
tp 442
fp 46
recall 0.9977426614046441
precision 0.905737703062013
f1 0.9495161478933953
tgt_res:
===== RES ====
p 1326
tp 1321
fp 132
recall 0.9962292601838391
precision 0.9091534749420829
f1 0.9507011916161452
save pretrained model to: checkpoint/ds/bert/42/source-encoder.ptdabest
save pretrained model to: checkpoint/ds/bert/42/source-classifier.ptdabest
===== RES ====
p 443
tp 442
fp 50
recall 0.9977426614046441
precision 0.8983739819138741
f1 0.9454540448056621
===== RES ====
p 443
tp 440
fp 28
recall 0.993227988728605
precision 0.9401709381620279
f1 0.9659709581902548
tgt_res:
===== RES ====
p 1330
tp 1309
fp 54
recall 0.9842105255757815
precision 0.9603815106673649
f1 0.9721495179200086
save pretrained model to: checkpoint/ds/bert/42/source-encoder.ptdabest
save pretrained model to: checkpoint/ds/bert/42/source-classifier.ptdabest
===== RES ====
p 442
tp 440
fp 55
recall 0.9954751108699659
precision 0.8888888870931537
f1 0.9391670556252353
===== RES ====
p 443
tp 441
fp 26
recall 0.9954853250666246
precision 0.944325479776605
f1 0.9692302674486335
tgt_res:
===== RES ====
p 1326
tp 1313
fp 52
recall 0.9901960776846183
precision 0.9619047612000697
f1 0.9758449100080292
save pretrained model to: checkpoint/ds/bert/42/source-encoder.ptdabest
save pretrained model to: checkpoint/ds/bert/42/source-classifier.ptdabest
Restore model from: /home/derossi/DADER/main/checkpoint/ds/bert/42/source-encoder.ptdabest
Restore model from: /home/derossi/DADER/main/checkpoint/ds/bert/42/source-classifier.ptdabest
Pretraining time:  551.5052742958069
=== Training F' and A ===
===== RES ====
p 440
tp 437
fp 26
recall 0.9931818159245868
precision 0.9438444904020638
f1 0.9678843265308702
======== tgt result =======
===== RES ====
p 1325
tp 1310
fp 48
recall 0.9886792445368457
precision 0.9646539020878836
f1 0.9765183215619058
===== RES ====
p 444
tp 441
fp 25
recall 0.993243241006209
precision 0.9463519292996739
f1 0.9692302673930846
======== tgt result =======
===== RES ====
p 1329
tp 1316
fp 55
recall 0.9902182084347492
precision 0.9598832961634695
f1 0.9748143142139738
===== RES ====
p 442
tp 438
fp 23
recall 0.9909502240023751
precision 0.9501084578088753
f1 0.9700991658470901
======== tgt result =======
===== RES ====
p 1326
tp 1311
fp 46
recall 0.988687782059813
precision 0.9661016942033148
f1 0.9772637557678897
===== RES ====
p 443
tp 440
fp 24
recall 0.993227988728605
precision 0.9482758600252675
f1 0.9702310306536709
======== tgt result =======
===== RES ====
p 1326
tp 1313
fp 50
recall 0.9901960776846183
precision 0.9633162135265471
f1 0.976570715434028
===== RES ====
p 441
tp 438
fp 28
recall 0.9931972766594166
precision 0.9399141610731456
f1 0.9658208874455724
===== RES ====
p 442
tp 439
fp 24
recall 0.9932126674361704
precision 0.9481641448203798
f1 0.9701652439818208
===== RES ====
p 444
tp 441
fp 24
recall 0.993243241006209
precision 0.9483870947346514
f1 0.9702965278352192
======== tgt result =======
===== RES ====
p 1328
tp 1316
fp 53
recall 0.990963854675479
precision 0.9612856092320776
f1 0.9758986465927085
===== RES ====
p 443
tp 440
fp 24
recall 0.993227988728605
precision 0.9482758600252675
f1 0.9702310306536709
===== RES ====
p 442
tp 439
fp 24
recall 0.9932126674361704
precision 0.9481641448203798
f1 0.9701652439818208
===== RES ====
p 442
tp 440
fp 24
recall 0.9954751108699659
precision 0.9482758600252675
f1 0.9713019264069936
======== tgt result =======
===== RES ====
p 1328
tp 1316
fp 52
recall 0.990963854675479
precision 0.9619883033903593
f1 0.976260626982531
===== RES ====
p 441
tp 439
fp 23
recall 0.9954648503504199
precision 0.9502164481597046
f1 0.9723140053153809
======== tgt result =======
===== RES ====
p 1327
tp 1314
fp 47
recall 0.990203465719515
precision 0.9654665679901053
f1 0.9776780707813837
===== RES ====
p 443
tp 440
fp 25
recall 0.993227988728605
precision 0.9462365571048676
f1 0.9691624937537751
===== RES ====
p 443
tp 441
fp 27
recall 0.9954853250666246
precision 0.9423076902942144
f1 0.9681663478671031
===== RES ====
p 443
tp 440
fp 26
recall 0.993227988728605
precision 0.9442060065574979
f1 0.9680963078713091
===== RES ====
p 443
tp 440
fp 25
recall 0.993227988728605
precision 0.9462365571048676
f1 0.9691624937537751
===== RES ====
p 443
tp 440
fp 25
recall 0.993227988728605
precision 0.9462365571048676
f1 0.9691624937537751
===== RES ====
p 443
tp 441
fp 28
recall 0.9954853250666246
precision 0.9402985054577857
f1 0.9671047614436831
===== RES ====
p 441
tp 439
fp 34
recall 0.9954648503504199
precision 0.928118391272477
f1 0.9606121899772286
===== RES ====
p 443
tp 440
fp 35
recall 0.993227988728605
precision 0.9263157875235457
f1 0.9586051630073658
===== RES ====
p 440
tp 437
fp 31
recall 0.9931818159245868
precision 0.9337606817654687
f1 0.9625545644348497
===== RES ====
p 444
tp 438
fp 25
recall 0.9864864842646701
precision 0.9460043176112218
f1 0.9658208872851153
=== Result of InvGAN+KD: ===
0.9658208872851153
The source-target datasets are: ds_da with seed 42
The F1 score is: 0.9658208872851153
The training time is: 1270.9480307102203
The inference time is: 0.00027094781398773193
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: da
tgt: ds
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 1068
tp 649
fp 31
recall 0.607677902052736
precision 0.9544117633023357
f1 0.7425624528473632
tgt_res:
===== RES ====
p 3207
tp 1869
fp 101
recall 0.5827876518295019
precision 0.9487309639854157
f1 0.722039319652852
save pretrained model to: checkpoint/da/bert/42/source-encoder.ptdsbest
save pretrained model to: checkpoint/da/bert/42/source-classifier.ptdsbest
===== RES ====
p 1068
tp 555
fp 5
recall 0.5196629208617388
precision 0.9910714268016582
f1 0.6818177296651321
===== RES ====
p 1063
tp 501
fp 9
recall 0.4713076195001809
precision 0.9823529392502883
f1 0.6369989252588258
===== RES ====
p 1067
tp 446
fp 3
recall 0.417994376365516
precision 0.9933184833110947
f1 0.588390083633276
===== RES ====
p 1068
tp 391
fp 3
recall 0.36610486857106284
precision 0.9923857842832848
f1 0.5348833264649829
Restore model from: /home/derossi/DADER/main/checkpoint/da/bert/42/source-encoder.ptdsbest
Restore model from: /home/derossi/DADER/main/checkpoint/da/bert/42/source-classifier.ptdsbest
Pretraining time:  287.1630139350891
=== Training F' and A ===
===== RES ====
p 1069
tp 915
fp 81
recall 0.8559401301628249
precision 0.9186746978728165
f1 0.8861980469823239
======== tgt result =======
===== RES ====
p 3204
tp 2765
fp 225
recall 0.8629837700177954
precision 0.924749163570318
f1 0.8927989836798409
===== RES ====
p 1068
tp 936
fp 75
recall 0.8764044935614189
precision 0.9258160228231296
f1 0.900432399942807
======== tgt result =======
===== RES ====
p 3206
tp 2815
fp 208
recall 0.8780411725271238
precision 0.9311941776608686
f1 0.903836392098603
===== RES ====
p 1068
tp 974
fp 127
recall 0.911985017872673
precision 0.8846503170893275
f1 0.898109227273131
===== RES ====
p 1068
tp 975
fp 125
recall 0.9129213474598113
precision 0.8863636355578511
f1 0.8994459937444057
===== RES ====
p 1068
tp 1014
fp 140
recall 0.9494382013582038
precision 0.8786828415262713
f1 0.9126907690546762
======== tgt result =======
===== RES ====
p 3206
tp 3023
fp 368
recall 0.9429195255948474
precision 0.8914774400202072
f1 0.9164766867156018
===== RES ====
p 1066
tp 920
fp 62
recall 0.8630393988151599
precision 0.9368635428341512
f1 0.8984369999640377
===== RES ====
p 1068
tp 941
fp 77
recall 0.8810861414971103
precision 0.9243614922157549
f1 0.9022046767954937
===== RES ====
p 1067
tp 955
fp 91
recall 0.8950328014104659
precision 0.9130019111730382
f1 0.90392756355754
===== RES ====
p 1070
tp 931
fp 74
recall 0.8700934571307537
precision 0.9263681582822207
f1 0.8973488972163617
===== RES ====
p 1066
tp 939
fp 105
recall 0.8808630385732992
precision 0.8994252864948032
f1 0.8900468925759193
===== RES ====
p 1070
tp 1003
fp 144
recall 0.9373831766940344
precision 0.8744550994991672
f1 0.9048258416906296
===== RES ====
p 1068
tp 932
fp 70
recall 0.8726591752128658
precision 0.930139719630599
f1 0.9004825914259814
===== RES ====
p 1068
tp 937
fp 83
recall 0.8773408231485572
precision 0.918627450079777
f1 0.8975090779488917
===== RES ====
p 1068
tp 907
fp 65
recall 0.8492509355344091
precision 0.9331275710564532
f1 0.8892151865102761
===== RES ====
p 1070
tp 932
fp 86
recall 0.8710280365691326
precision 0.9155206277843608
f1 0.8927198059687036
===== RES ====
p 1067
tp 958
fp 91
recall 0.8978444227761532
precision 0.9132507140960431
f1 0.9054815407685158
===== RES ====
p 1068
tp 927
fp 78
recall 0.8679775272771745
precision 0.9223880587836935
f1 0.8943555053879281
===== RES ====
p 1069
tp 812
fp 48
recall 0.7595883996636216
precision 0.9441860454137372
f1 0.8418864930735993
===== RES ====
p 1066
tp 985
fp 121
recall 0.9240150085140572
precision 0.8905967442218835
f1 0.9069976577140544
===== RES ====
p 1065
tp 916
fp 79
recall 0.860093895906015
precision 0.9206030141501478
f1 0.8893198880637173
===== RES ====
p 1070
tp 939
fp 86
recall 0.8775700926377848
precision 0.9160975600818559
f1 0.8964195471078936
=== Result of InvGAN+KD: ===
0.8964195471078936
The source-target datasets are: da_ds with seed 42
The F1 score is: 0.8964195471078936
The training time is: 2115.712265253067
The inference time is: 0.000269964337348938
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: dzy
tgt: fz
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 21
tp 21
fp 0
recall 0.9999999523809546
precision 0.9999999523809546
f1 0.9999994523812045
tgt_res:
===== RES ====
p 63
tp 49
fp 0
recall 0.777777765432099
precision 0.9999999795918372
f1 0.874999492187777
save pretrained model to: checkpoint/dzy/bert/3000/source-encoder.ptfzbest
save pretrained model to: checkpoint/dzy/bert/3000/source-classifier.ptfzbest
===== RES ====
p 16
tp 16
fp 8
recall 0.9999999375000038
precision 0.66666663888889
f1 0.7999994800002881
===== RES ====
p 19
tp 19
fp 1
recall 0.9999999473684238
precision 0.9499999525000024
f1 0.9743584247208373
===== RES ====
p 17
tp 17
fp 5
recall 0.999999941176474
precision 0.7727272376033073
f1 0.8717943353059987
===== RES ====
p 20
tp 20
fp 7
recall 0.9999999500000024
precision 0.7407407133058995
f1 0.8510633046630248
Restore model from: /home/derossi/DADER/main/checkpoint/dzy/bert/3000/source-encoder.ptfzbest
Restore model from: /home/derossi/DADER/main/checkpoint/dzy/bert/3000/source-classifier.ptfzbest
Pretraining time:  25.7577223777771
=== Training F' and A ===
===== RES ====
p 20
tp 18
fp 0
recall 0.8999999550000022
precision 0.9999999444444475
f1 0.9473678725764422
======== tgt result =======
===== RES ====
p 64
tp 45
fp 0
recall 0.703124989013672
precision 0.9999999777777784
f1 0.8256875734368639
===== RES ====
p 15
tp 13
fp 0
recall 0.8666666088888928
precision 0.9999999230769291
f1 0.9285708647961893
===== RES ====
p 20
tp 12
fp 0
recall 0.5999999700000015
precision 0.9999999166666736
f1 0.7499994843752921
===== RES ====
p 17
tp 1
fp 0
recall 0.05882352595155729
precision 0.9999990000010001
f1 0.11111099382717317
===== RES ====
p 19
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 20
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 17
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 20
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 18
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 22
tp 6
fp 0
recall 0.27272726033057904
precision 0.9999998333333611
f1 0.42857106122473326
===== RES ====
p 20
tp 10
fp 0
recall 0.4999999750000012
precision 0.99999990000001
f1 0.6666661777780697
===== RES ====
p 18
tp 10
fp 0
recall 0.5555555246913597
precision 0.99999990000001
f1 0.7142852040819256
===== RES ====
p 21
tp 15
fp 0
recall 0.7142856802721105
precision 0.9999999333333378
f1 0.8333328009262105
===== RES ====
p 22
tp 15
fp 0
recall 0.6818181508264477
precision 0.9999999333333378
f1 0.8108102848797611
===== RES ====
p 17
tp 12
fp 0
recall 0.7058823114186875
precision 0.9999999166666736
f1 0.8275856646851852
===== RES ====
p 19
tp 16
fp 0
recall 0.8421052188365674
precision 0.9999999375000038
f1 0.9142851657145576
===== RES ====
p 19
tp 19
fp 0
recall 0.9999999473684238
precision 0.9999999473684238
f1 0.9999994473686737
======== tgt result =======
===== RES ====
p 62
tp 55
fp 0
recall 0.8870967598855362
precision 0.9999999818181822
f1 0.9401704258896646
===== RES ====
p 19
tp 19
fp 2
recall 0.9999999473684238
precision 0.9047618616780065
f1 0.9499994537502641
===== RES ====
p 14
tp 14
fp 7
recall 0.9999999285714337
precision 0.6666666349206364
f1 0.7999994742860026
===== RES ====
p 19
tp 19
fp 10
recall 0.9999999473684238
precision 0.655172391200952
f1 0.791666155382233
===== RES ====
p 22
tp 22
fp 6
recall 0.9999999545454565
precision 0.7857142576530622
f1 0.8799994720002768
=== Result of InvGAN+KD: ===
0.8799994720002768
The source-target datasets are: dzy_fz with seed 3000
The F1 score is: 0.8799994720002768
The training time is: 89.74156141281128
The inference time is: 0.00026857852935791016
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: fz
tgt: dzy
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 41
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 36
fp 1
recall 0.9473683961218844
precision 0.9729729466764069
f1 0.95999947448915
tgt_res:
===== RES ====
p 125
tp 121
fp 1
recall 0.9679999922560001
precision 0.9918032705589896
f1 0.979756577161002
save pretrained model to: checkpoint/fz/bert/3000/source-encoder.ptdzybest
save pretrained model to: checkpoint/fz/bert/3000/source-classifier.ptdzybest
===== RES ====
p 42
tp 41
fp 1
recall 0.9761904529478465
precision 0.9761904529478465
f1 0.9761899529481026
tgt_res:
===== RES ====
p 120
tp 118
fp 0
recall 0.9833333251388889
precision 0.9999999915254238
f1 0.9915961303582778
save pretrained model to: checkpoint/fz/bert/3000/source-encoder.ptdzybest
save pretrained model to: checkpoint/fz/bert/3000/source-classifier.ptdzybest
===== RES ====
p 38
tp 38
fp 4
recall 0.9999999736842113
precision 0.9047618832199552
f1 0.9499994775002626
===== RES ====
p 38
tp 34
fp 0
recall 0.8947368185595574
precision 0.9999999705882362
f1 0.9444439197533502
Restore model from: /home/derossi/DADER/main/checkpoint/fz/bert/3000/source-encoder.ptdzybest
Restore model from: /home/derossi/DADER/main/checkpoint/fz/bert/3000/source-classifier.ptdzybest
Pretraining time:  27.181705474853516
=== Training F' and A ===
===== RES ====
p 37
tp 36
fp 1
recall 0.9729729466764069
precision 0.9729729466764069
f1 0.9729724466766639
======== tgt result =======
===== RES ====
p 123
tp 119
fp 0
recall 0.9674796669310597
precision 0.9999999915966388
f1 0.983470566389162
===== RES ====
p 38
tp 37
fp 1
recall 0.9736841849030479
precision 0.9736841849030479
f1 0.9736836849033047
======== tgt result =======
===== RES ====
p 119
tp 113
fp 0
recall 0.9495798239531107
precision 0.9999999911504426
f1 0.9741374229714216
===== RES ====
p 42
tp 40
fp 0
recall 0.952380929705216
precision 0.9999999750000007
f1 0.9756092325998997
======== tgt result =======
===== RES ====
p 120
tp 113
fp 0
recall 0.9416666588194446
precision 0.9999999911504426
f1 0.9699565736707985
===== RES ====
p 37
tp 34
fp 0
recall 0.9189188940832732
precision 0.9999999705882362
f1 0.9577459527874063
===== RES ====
p 38
tp 35
fp 0
recall 0.9210526073407209
precision 0.9999999714285723
f1 0.9589035841623923
===== RES ====
p 36
tp 33
fp 0
recall 0.9166666412037044
precision 0.9999999696969707
f1 0.9565212123506078
===== RES ====
p 40
tp 36
fp 0
recall 0.8999999775000006
precision 0.999999972222223
f1 0.9473678975071883
===== RES ====
p 39
tp 35
fp 0
recall 0.8974358744247213
precision 0.9999999714285723
f1 0.9459454218410231
===== RES ====
p 40
tp 30
fp 0
recall 0.7499999812500006
precision 0.9999999666666678
f1 0.857142342857423
===== RES ====
p 38
tp 7
fp 0
recall 0.18421052146814418
precision 0.9999998571428775
f1 0.3111108345681027
===== RES ====
p 39
tp 5
fp 0
recall 0.12820512491781733
precision 0.99999980000004
f1 0.2272725154960197
===== RES ====
p 40
tp 3
fp 0
recall 0.07499999812500005
precision 0.9999996666667778
f1 0.13953474743113042
===== RES ====
p 36
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 39
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 36
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 40
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 37
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 43
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
=== Result of InvGAN+KD: ===
0.0
The source-target datasets are: fz_dzy with seed 3000
The F1 score is: 0.0
The training time is: 85.3011691570282
The inference time is: 0.0002664327621459961
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: dzy
tgt: fz
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 18
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 21
tp 21
fp 6
recall 0.9999999523809546
precision 0.7777777489711944
f1 0.8749994713544443
tgt_res:
===== RES ====
p 63
tp 63
fp 28
recall 0.9999999841269844
precision 0.6923076846999155
f1 0.8181813240852912
save pretrained model to: checkpoint/dzy/bert/1000/source-encoder.ptfzbest
save pretrained model to: checkpoint/dzy/bert/1000/source-classifier.ptfzbest
===== RES ====
p 18
tp 18
fp 11
recall 0.9999999444444475
precision 0.620689633769323
f1 0.7659569416028258
===== RES ====
p 16
tp 16
fp 63
recall 0.9999999375000038
precision 0.20253164300592857
f1 0.33684181806116564
===== RES ====
p 18
tp 18
fp 20
recall 0.9999999444444475
precision 0.4736841980609422
f1 0.6428566836737617
Restore model from: /home/derossi/DADER/main/checkpoint/dzy/bert/1000/source-encoder.ptfzbest
Restore model from: /home/derossi/DADER/main/checkpoint/dzy/bert/1000/source-classifier.ptfzbest
Pretraining time:  27.13695979118347
=== Training F' and A ===
===== RES ====
p 20
tp 20
fp 3
recall 0.9999999500000024
precision 0.8695651795841226
f1 0.9302320173069202
======== tgt result =======
===== RES ====
p 65
tp 65
fp 16
recall 0.9999999846153849
precision 0.802469125895443
f1 0.8904104527118522
===== RES ====
p 18
tp 18
fp 2
recall 0.9999999444444475
precision 0.8999999550000022
f1 0.9473678725764422
======== tgt result =======
===== RES ====
p 63
tp 63
fp 5
recall 0.9999999841269844
precision 0.9264705746107269
f1 0.9618315471128986
===== RES ====
p 19
tp 19
fp 1
recall 0.9999999473684238
precision 0.9499999525000024
f1 0.9743584247208373
======== tgt result =======
===== RES ====
p 63
tp 62
fp 4
recall 0.9841269685059212
precision 0.9393939251606981
f1 0.9612397954452453
===== RES ====
p 16
tp 16
fp 1
recall 0.9999999375000038
precision 0.9411764152249167
f1 0.969696411386854
===== RES ====
p 16
tp 16
fp 0
recall 0.9999999375000038
precision 0.9999999375000038
f1 0.9999994375002539
======== tgt result =======
===== RES ====
p 63
tp 61
fp 1
recall 0.9682539528848579
precision 0.9838709518730492
f1 0.9759994844162564
===== RES ====
p 19
tp 19
fp 1
recall 0.9999999473684238
precision 0.9499999525000024
f1 0.9743584247208373
===== RES ====
p 19
tp 19
fp 1
recall 0.9999999473684238
precision 0.9499999525000024
f1 0.9743584247208373
===== RES ====
p 17
tp 17
fp 1
recall 0.999999941176474
precision 0.9444443919753115
f1 0.9714280163267905
===== RES ====
p 19
tp 19
fp 1
recall 0.9999999473684238
precision 0.9499999525000024
f1 0.9743584247208373
===== RES ====
p 20
tp 20
fp 1
recall 0.9999999500000024
precision 0.9523809070294805
f1 0.9756092088045414
===== RES ====
p 20
tp 20
fp 1
recall 0.9999999500000024
precision 0.9523809070294805
f1 0.9756092088045414
===== RES ====
p 19
tp 19
fp 1
recall 0.9999999473684238
precision 0.9499999525000024
f1 0.9743584247208373
===== RES ====
p 18
tp 18
fp 1
recall 0.9999999444444475
precision 0.9473683711911383
f1 0.9729724207453287
===== RES ====
p 14
tp 14
fp 2
recall 0.9999999285714337
precision 0.8749999453125034
f1 0.9333327733336026
===== RES ====
p 16
tp 16
fp 3
recall 0.9999999375000038
precision 0.8421052188365674
f1 0.9142851657145576
===== RES ====
p 16
tp 16
fp 28
recall 0.9999999375000038
precision 0.36363635537190103
f1 0.5333329244447246
===== RES ====
p 19
tp 19
fp 85
recall 0.9999999473684238
precision 0.18269230593565092
f1 0.3089428231874695
===== RES ====
p 20
tp 20
fp 109
recall 0.9999999500000024
precision 0.15503875848807164
f1 0.268456139813716
===== RES ====
p 18
tp 18
fp 104
recall 0.9999999444444475
precision 0.14754098239720506
f1 0.2571426293879425
===== RES ====
p 18
tp 18
fp 47
recall 0.9999999444444475
precision 0.276923072662722
f1 0.4337345896359096
===== RES ====
p 22
tp 22
fp 50
recall 0.9999999545454565
precision 0.30555555131172846
f1 0.468084737890717
=== Result of InvGAN+KD: ===
0.468084737890717
The source-target datasets are: dzy_fz with seed 1000
The F1 score is: 0.468084737890717
The training time is: 90.99111413955688
The inference time is: 0.0002690032124519348
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: fz
tgt: dzy
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 36
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 36
tp 34
fp 1
recall 0.9444444182098773
precision 0.9714285436734702
f1 0.9577459519939139
tgt_res:
===== RES ====
p 124
tp 119
fp 1
recall 0.9596774116155048
precision 0.9916666584027779
f1 0.97540932820504
save pretrained model to: checkpoint/fz/bert/1000/source-encoder.ptdzybest
save pretrained model to: checkpoint/fz/bert/1000/source-classifier.ptdzybest
===== RES ====
p 39
tp 37
fp 7
recall 0.9487179243918481
precision 0.8409090717975212
f1 0.8915657453914821
===== RES ====
p 38
tp 37
fp 5
recall 0.9736841849030479
precision 0.8809523599773248
f1 0.9249994781252696
===== RES ====
p 40
tp 35
fp 2
recall 0.8749999781250006
precision 0.94594592037984
f1 0.9090903862374142
Restore model from: /home/derossi/DADER/main/checkpoint/fz/bert/1000/source-encoder.ptdzybest
Restore model from: /home/derossi/DADER/main/checkpoint/fz/bert/1000/source-classifier.ptdzybest
Pretraining time:  26.056278944015503
=== Training F' and A ===
===== RES ====
p 39
tp 36
fp 1
recall 0.9230768994082847
precision 0.9729729466764069
f1 0.9473678964684084
======== tgt result =======
===== RES ====
p 122
tp 116
fp 0
recall 0.9508196643375437
precision 0.9999999913793104
f1 0.974789408092905
===== RES ====
p 39
tp 37
fp 1
recall 0.9487179243918481
precision 0.9736841849030479
f1 0.9610384361615022
======== tgt result =======
===== RES ====
p 120
tp 116
fp 0
recall 0.9666666586111112
precision 0.9999999913793104
f1 0.9830503392705789
===== RES ====
p 40
tp 36
fp 0
recall 0.8999999775000006
precision 0.999999972222223
f1 0.9473678975071883
===== RES ====
p 40
tp 36
fp 0
recall 0.8999999775000006
precision 0.999999972222223
f1 0.9473678975071883
===== RES ====
p 39
tp 32
fp 0
recall 0.8205127994740309
precision 0.9999999687500011
f1 0.9014079301728575
===== RES ====
p 39
tp 31
fp 0
recall 0.7948717744904674
precision 0.9999999677419364
f1 0.8857137669390509
===== RES ====
p 37
tp 30
fp 0
recall 0.8108107888970058
precision 0.9999999666666678
f1 0.8955218667857493
===== RES ====
p 38
tp 28
fp 0
recall 0.7368420858725767
precision 0.9999999642857155
f1 0.8484843342518884
===== RES ====
p 37
tp 25
fp 0
recall 0.6756756574141715
precision 0.9999999600000016
f1 0.8064511056194335
===== RES ====
p 39
tp 15
fp 0
recall 0.384615374753452
precision 0.9999999333333378
f1 0.5555551337451393
===== RES ====
p 36
tp 9
fp 0
recall 0.24999999305555576
precision 0.9999998888889013
f1 0.3999996622224631
===== RES ====
p 38
tp 7
fp 0
recall 0.18421052146814418
precision 0.9999998571428775
f1 0.3111108345681027
===== RES ====
p 39
tp 3
fp 0
recall 0.0769230749506904
precision 0.9999996666667778
f1 0.1428570034014491
===== RES ====
p 38
tp 1
fp 0
recall 0.026315788781163457
precision 0.9999990000010001
f1 0.05128199868507828
===== RES ====
p 36
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 37
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 40
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 39
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 35
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 37
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 43
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
=== Result of InvGAN+KD: ===
0.0
The source-target datasets are: fz_dzy with seed 1000
The F1 score is: 0.0
The training time is: 84.85309171676636
The inference time is: 0.0002678409218788147
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: dzy
tgt: fz
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 17
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 18
tp 18
fp 4
recall 0.9999999444444475
precision 0.8181817809917372
f1 0.899999460000274
tgt_res:
===== RES ====
p 63
tp 63
fp 12
recall 0.9999999841269844
precision 0.8399999888000002
f1 0.9130429688093437
save pretrained model to: checkpoint/dzy/bert/42/source-encoder.ptfzbest
save pretrained model to: checkpoint/dzy/bert/42/source-classifier.ptfzbest
===== RES ====
p 17
tp 17
fp 8
recall 0.999999941176474
precision 0.679999972800001
f1 0.8095232891159332
===== RES ====
p 20
tp 20
fp 1
recall 0.9999999500000024
precision 0.9523809070294805
f1 0.9756092088045414
tgt_res:
===== RES ====
p 63
tp 63
fp 5
recall 0.9999999841269844
precision 0.9264705746107269
f1 0.9618315471128986
save pretrained model to: checkpoint/dzy/bert/42/source-encoder.ptfzbest
save pretrained model to: checkpoint/dzy/bert/42/source-classifier.ptfzbest
===== RES ====
p 16
tp 16
fp 17
recall 0.9999999375000038
precision 0.484848470156107
f1 0.6530607580177852
Restore model from: /home/derossi/DADER/main/checkpoint/dzy/bert/42/source-encoder.ptfzbest
Restore model from: /home/derossi/DADER/main/checkpoint/dzy/bert/42/source-classifier.ptfzbest
Pretraining time:  26.31361222267151
=== Training F' and A ===
===== RES ====
p 16
tp 16
fp 6
recall 0.9999999375000038
precision 0.7272726942148775
f1 0.8421047313022223
======== tgt result =======
===== RES ====
p 64
tp 64
fp 11
recall 0.9999999843750003
precision 0.8533333219555558
f1 0.9208627992342624
===== RES ====
p 19
tp 19
fp 1
recall 0.9999999473684238
precision 0.9499999525000024
f1 0.9743584247208373
======== tgt result =======
===== RES ====
p 66
tp 66
fp 10
recall 0.9999999848484851
precision 0.8684210412049863
f1 0.9295769541760251
===== RES ====
p 16
tp 16
fp 0
recall 0.9999999375000038
precision 0.9999999375000038
f1 0.9999994375002539
======== tgt result =======
===== RES ====
p 62
tp 62
fp 4
recall 0.999999983870968
precision 0.9393939251606981
f1 0.9687494853518203
===== RES ====
p 19
tp 19
fp 0
recall 0.9999999473684238
precision 0.9999999473684238
f1 0.9999994473686737
======== tgt result =======
===== RES ====
p 62
tp 62
fp 2
recall 0.999999983870968
precision 0.9687499848632816
f1 0.9841264686321515
===== RES ====
p 17
tp 17
fp 1
recall 0.999999941176474
precision 0.9444443919753115
f1 0.9714280163267905
===== RES ====
p 20
tp 20
fp 1
recall 0.9999999500000024
precision 0.9523809070294805
f1 0.9756092088045414
===== RES ====
p 18
tp 18
fp 1
recall 0.9999999444444475
precision 0.9473683711911383
f1 0.9729724207453287
===== RES ====
p 16
tp 16
fp 1
recall 0.9999999375000038
precision 0.9411764152249167
f1 0.969696411386854
===== RES ====
p 19
tp 19
fp 1
recall 0.9999999473684238
precision 0.9499999525000024
f1 0.9743584247208373
===== RES ====
p 19
tp 19
fp 1
recall 0.9999999473684238
precision 0.9499999525000024
f1 0.9743584247208373
===== RES ====
p 19
tp 19
fp 1
recall 0.9999999473684238
precision 0.9499999525000024
f1 0.9743584247208373
===== RES ====
p 18
tp 18
fp 1
recall 0.9999999444444475
precision 0.9473683711911383
f1 0.9729724207453287
===== RES ====
p 20
tp 20
fp 0
recall 0.9999999500000024
precision 0.9999999500000024
f1 0.9999994500002525
======== tgt result =======
===== RES ====
p 61
tp 61
fp 4
recall 0.9999999836065577
precision 0.9384615240236689
f1 0.968253453389021
===== RES ====
p 17
tp 17
fp 1
recall 0.999999941176474
precision 0.9444443919753115
f1 0.9714280163267905
===== RES ====
p 18
tp 18
fp 1
recall 0.9999999444444475
precision 0.9473683711911383
f1 0.9729724207453287
===== RES ====
p 17
tp 17
fp 1
recall 0.999999941176474
precision 0.9444443919753115
f1 0.9714280163267905
===== RES ====
p 19
tp 19
fp 1
recall 0.9999999473684238
precision 0.9499999525000024
f1 0.9743584247208373
===== RES ====
p 21
tp 21
fp 1
recall 0.9999999523809546
precision 0.9545454111570267
f1 0.9767436408872237
===== RES ====
p 14
tp 14
fp 2
recall 0.9999999285714337
precision 0.8749999453125034
f1 0.9333327733336026
===== RES ====
p 19
tp 19
fp 2
recall 0.9999999473684238
precision 0.9047618616780065
f1 0.9499994537502641
===== RES ====
p 22
tp 22
fp 3
recall 0.9999999545454565
precision 0.8799999648000014
f1 0.9361696749663144
=== Result of InvGAN+KD: ===
0.9361696749663144
The source-target datasets are: dzy_fz with seed 42
The F1 score is: 0.9361696749663144
The training time is: 91.70212936401367
The inference time is: 0.00026685744524002075
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: fz
tgt: dzy
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 1.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 32
fp 0
recall 0.8421052409972306
precision 0.9999999687500011
f1 0.9142851918370049
tgt_res:
===== RES ====
p 126
tp 105
fp 0
recall 0.8333333267195768
precision 0.9999999904761906
f1 0.9090904053524941
save pretrained model to: checkpoint/fz/bert/42/source-encoder.ptdzybest
save pretrained model to: checkpoint/fz/bert/42/source-classifier.ptdzybest
===== RES ====
p 39
tp 34
fp 0
recall 0.8717948494411578
precision 0.9999999705882362
f1 0.9315063261402553
tgt_res:
===== RES ====
p 121
tp 108
fp 0
recall 0.8925619760945291
precision 0.9999999907407409
f1 0.9432309344218038
save pretrained model to: checkpoint/fz/bert/42/source-encoder.ptdzybest
save pretrained model to: checkpoint/fz/bert/42/source-classifier.ptdzybest
===== RES ====
p 36
tp 33
fp 0
recall 0.9166666412037044
precision 0.9999999696969707
f1 0.9565212123506078
tgt_res:
===== RES ====
p 122
tp 110
fp 0
recall 0.9016393368718087
precision 0.999999990909091
f1 0.9482753552321292
save pretrained model to: checkpoint/fz/bert/42/source-encoder.ptdzybest
save pretrained model to: checkpoint/fz/bert/42/source-classifier.ptdzybest
===== RES ====
p 38
tp 35
fp 0
recall 0.9210526073407209
precision 0.9999999714285723
f1 0.9589035841623923
tgt_res:
===== RES ====
p 123
tp 111
fp 0
recall 0.9024390170533414
precision 0.9999999909909911
f1 0.948717441924435
save pretrained model to: checkpoint/fz/bert/42/source-encoder.ptdzybest
save pretrained model to: checkpoint/fz/bert/42/source-classifier.ptdzybest
Restore model from: /home/derossi/DADER/main/checkpoint/fz/bert/42/source-encoder.ptdzybest
Restore model from: /home/derossi/DADER/main/checkpoint/fz/bert/42/source-classifier.ptdzybest
Pretraining time:  26.568768978118896
=== Training F' and A ===
===== RES ====
p 36
tp 33
fp 1
recall 0.9166666412037044
precision 0.9705882067474058
f1 0.9428566163267962
======== tgt result =======
===== RES ====
p 126
tp 115
fp 0
recall 0.9126984054547745
precision 0.9999999913043479
f1 0.9543563395949649
===== RES ====
p 38
tp 35
fp 1
recall 0.9210526073407209
precision 0.9722221952160502
f1 0.945945420745334
======== tgt result =======
===== RES ====
p 124
tp 115
fp 0
recall 0.9274193473595214
precision 0.9999999913043479
f1 0.9623425888905096
===== RES ====
p 34
tp 32
fp 1
recall 0.9411764429065753
precision 0.969696940312214
f1 0.9552233521945153
======== tgt result =======
===== RES ====
p 120
tp 109
fp 0
recall 0.9083333257638889
precision 0.9999999908256881
f1 0.9519645583420169
===== RES ====
p 41
tp 38
fp 0
recall 0.9268292456870916
precision 0.9999999736842113
f1 0.9620247928219229
======== tgt result =======
===== RES ====
p 124
tp 111
fp 0
recall 0.895161283103538
precision 0.9999999909909911
f1 0.94468034455436
===== RES ====
p 39
tp 37
fp 0
recall 0.9487179243918481
precision 0.9999999729729737
f1 0.9736836852495646
======== tgt result =======
===== RES ====
p 124
tp 109
fp 0
recall 0.8790322509755464
precision 0.9999999908256881
f1 0.9356218116379816
===== RES ====
p 39
tp 34
fp 0
recall 0.8717948494411578
precision 0.9999999705882362
f1 0.9315063261402553
===== RES ====
p 40
tp 34
fp 0
recall 0.8499999787500006
precision 0.9999999705882362
f1 0.9189183973706123
===== RES ====
p 35
tp 21
fp 0
recall 0.5999999828571434
precision 0.9999999523809546
f1 0.7499995044645774
===== RES ====
p 40
tp 7
fp 0
recall 0.17499999562500013
precision 0.9999998571428775
f1 0.2978720742419337
===== RES ====
p 39
tp 3
fp 0
recall 0.0769230749506904
precision 0.9999996666667778
f1 0.1428570034014491
===== RES ====
p 39
tp 1
fp 0
recall 0.025641024983563465
precision 0.9999990000010001
f1 0.04999994875000255
===== RES ====
p 39
tp 1
fp 0
recall 0.025641024983563465
precision 0.9999990000010001
f1 0.04999994875000255
===== RES ====
p 39
tp 1
fp 0
recall 0.025641024983563465
precision 0.9999990000010001
f1 0.04999994875000255
===== RES ====
p 36
tp 1
fp 0
recall 0.027777777006172864
precision 0.9999990000010001
f1 0.0540539985390826
===== RES ====
p 39
tp 1
fp 0
recall 0.025641024983563465
precision 0.9999990000010001
f1 0.04999994875000255
===== RES ====
p 40
tp 1
fp 0
recall 0.024999999375000016
precision 0.9999990000010001
f1 0.048780437834624664
===== RES ====
p 40
tp 1
fp 0
recall 0.024999999375000016
precision 0.9999990000010001
f1 0.048780437834624664
===== RES ====
p 38
tp 1
fp 0
recall 0.026315788781163457
precision 0.9999990000010001
f1 0.05128199868507828
===== RES ====
p 38
tp 1
fp 0
recall 0.026315788781163457
precision 0.9999990000010001
f1 0.05128199868507828
===== RES ====
p 35
tp 1
fp 0
recall 0.028571427755102068
precision 0.9999990000010001
f1 0.05555549845679327
===== RES ====
p 43
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
=== Result of InvGAN+KD: ===
0.0
The source-target datasets are: fz_dzy with seed 42
The F1 score is: 0.0
The training time is: 86.36240291595459
The inference time is: 0.0002679973840713501
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ri
tgt: ab
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 203
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 205
tp 203
fp 1229
recall 0.9902438976085663
precision 0.14175977643731857
f1 0.2480144415688504
tgt_res:
===== RES ====
p 614
tp 605
fp 3736
recall 0.9853420179391824
precision 0.13936880899807214
f1 0.24419756280130595
save pretrained model to: checkpoint/ri/bert/3000/source-encoder.ptabbest
save pretrained model to: checkpoint/ri/bert/3000/source-classifier.ptabbest
===== RES ====
p 201
tp 199
fp 1310
recall 0.9900497463181605
precision 0.1318754140941846
f1 0.23274833028505823
===== RES ====
p 203
tp 201
fp 1351
recall 0.9901477783736563
precision 0.12951030919490314
f1 0.22905962421881662
===== RES ====
p 202
tp 202
fp 1587
recall 0.999999995049505
precision 0.11291224141257
f1 0.20291292646054582
Restore model from: /home/derossi/DADER/main/checkpoint/ri/bert/3000/source-encoder.ptabbest
Restore model from: /home/derossi/DADER/main/checkpoint/ri/bert/3000/source-classifier.ptabbest
Pretraining time:  35.642674922943115
=== Training F' and A ===
===== RES ====
p 203
tp 201
fp 1252
recall 0.9901477783736563
precision 0.13833448029020337
f1 0.24275340778027232
======== tgt result =======
===== RES ====
p 616
tp 607
fp 3784
recall 0.985389608789952
precision 0.13823730354401337
f1 0.24246033934216918
===== RES ====
p 203
tp 201
fp 1232
recall 0.9901477783736563
precision 0.14026517785047787
f1 0.24572105372042877
======== tgt result =======
===== RES ====
p 611
tp 600
fp 3724
recall 0.9819967250703818
precision 0.13876040699843653
f1 0.2431608771651206
===== RES ====
p 203
tp 194
fp 979
recall 0.9556650199228325
precision 0.16538789414715438
f1 0.28197649224793353
======== tgt result =======
===== RES ====
p 616
tp 593
fp 2978
recall 0.9626623360995741
precision 0.1660599271447606
f1 0.28325745132285013
===== RES ====
p 203
tp 127
fp 322
recall 0.625615760464947
precision 0.2828507788800651
f1 0.3895701221305856
======== tgt result =======
===== RES ====
p 614
tp 393
fp 986
recall 0.6400651455373532
precision 0.2849891223459107
f1 0.39437990443164966
===== RES ====
p 199
tp 34
fp 36
recall 0.17085427049821975
precision 0.4857142787755103
f1 0.25278771719631166
===== RES ====
p 202
tp 4
fp 2
recall 0.019801980099990197
precision 0.6666665555555741
f1 0.03846148206368255
===== RES ====
p 200
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 203
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 204
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 198
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 203
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 204
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 204
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 203
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 203
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 204
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 202
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 204
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 200
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 206
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 206
tp 1
fp 0
recall 0.004854368908473937
precision 0.9999990000010001
f1 0.009661826040281079
=== Result of InvGAN+KD: ===
0.009661826040281079
The source-target datasets are: ri_ab with seed 3000
The F1 score is: 0.009661826040281079
The training time is: 158.66752243041992
The inference time is: 0.00027382373809814453
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ri
tgt: wa1
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 187
fp 1506
recall 0.9689119120781766
precision 0.11045481387450985
f1 0.19830310344869748
tgt_res:
===== RES ====
p 576
tp 564
fp 4513
recall 0.9791666649667246
precision 0.11108922589893849
f1 0.19953988412920104
save pretrained model to: checkpoint/ri/bert/3000/source-encoder.ptwa1best
save pretrained model to: checkpoint/ri/bert/3000/source-classifier.ptwa1best
===== RES ====
p 193
tp 188
fp 1656
recall 0.9740932592015893
precision 0.10195227760197816
f1 0.18458500255452306
===== RES ====
p 193
tp 189
fp 1617
recall 0.9792746063250021
precision 0.10465116273275128
f1 0.1890943726311982
===== RES ====
p 193
tp 191
fp 1778
recall 0.9896373005718274
precision 0.09700355505484837
f1 0.17668808885515958
Restore model from: /home/derossi/DADER/main/checkpoint/ri/bert/3000/source-encoder.ptwa1best
Restore model from: /home/derossi/DADER/main/checkpoint/ri/bert/3000/source-classifier.ptwa1best
Pretraining time:  39.236268043518066
=== Training F' and A ===
===== RES ====
p 193
tp 191
fp 1681
recall 0.9896373005718274
precision 0.10202991447541136
f1 0.18498772382898632
======== tgt result =======
===== RES ====
p 576
tp 571
fp 5056
recall 0.9913194427234038
precision 0.10147503108201972
f1 0.1841042970507015
===== RES ====
p 193
tp 187
fp 1471
recall 0.9689119120781766
precision 0.11278648967865712
f1 0.2020527573439472
======== tgt result =======
===== RES ====
p 576
tp 562
fp 4415
recall 0.9756944427505305
precision 0.11291942935243732
f1 0.20241292402133051
===== RES ====
p 193
tp 183
fp 1315
recall 0.9481865235845258
precision 0.12216288376357617
f1 0.2164397738752376
======== tgt result =======
===== RES ====
p 576
tp 552
fp 3995
recall 0.9583333316695601
precision 0.1213987244069939
f1 0.2154985315428568
===== RES ====
p 193
tp 143
fp 577
recall 0.7409326386480174
precision 0.19861111083526234
f1 0.31325267795262923
======== tgt result =======
===== RES ====
p 576
tp 447
fp 1756
recall 0.7760416653193721
precision 0.2029051292769382
f1 0.32169812383274377
===== RES ====
p 193
tp 88
fp 220
recall 0.4559585468603184
precision 0.28571428478664196
f1 0.3512969301323909
======== tgt result =======
===== RES ====
p 576
tp 289
fp 656
recall 0.5017361102400415
precision 0.30582010549648664
f1 0.38001267817305384
===== RES ====
p 193
tp 13
fp 8
recall 0.06735751260436522
precision 0.6190475895691624
f1 0.12149514896522694
===== RES ====
p 193
tp 1
fp 1
recall 0.005181347123412709
precision 0.499999750000125
f1 0.010256389848814043
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 192
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
=== Result of InvGAN+KD: ===
0.0
The source-target datasets are: ri_wa1 with seed 3000
The F1 score is: 0.0
The training time is: 170.9117968082428
The inference time is: 0.00027154386043548584
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ia
tgt: da
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 442
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 441
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 440
tp 440
fp 833
recall 0.9999999977272728
precision 0.34564021968135095
f1 0.5137182399353427
tgt_res:
===== RES ====
p 1326
tp 1326
fp 2384
recall 0.9999999992458521
precision 0.3574123988254953
f1 0.5266080312214031
save pretrained model to: checkpoint/ia/bert/3000/source-encoder.ptdabest
save pretrained model to: checkpoint/ia/bert/3000/source-classifier.ptdabest
===== RES ====
p 443
tp 443
fp 1765
recall 0.9999999977426637
precision 0.20063405788014763
f1 0.33421322572190465
===== RES ====
p 443
tp 443
fp 1074
recall 0.9999999977426637
precision 0.29202373085562044
f1 0.4520404659951685
Restore model from: /home/derossi/DADER/main/checkpoint/ia/bert/3000/source-encoder.ptdabest
Restore model from: /home/derossi/DADER/main/checkpoint/ia/bert/3000/source-classifier.ptdabest
Pretraining time:  40.73690056800842
=== Training F' and A ===
===== RES ====
p 443
tp 443
fp 748
recall 0.9999999977426637
precision 0.3719563388984413
f1 0.5422272662930657
======== tgt result =======
===== RES ====
p 1331
tp 1331
fp 2136
recall 0.9999999992486851
precision 0.3839053936014118
f1 0.5548141049082677
===== RES ====
p 442
tp 442
fp 880
recall 0.9999999977375565
precision 0.334341905949817
f1 0.5011334107139241
===== RES ====
p 443
tp 443
fp 743
recall 0.9999999977426637
precision 0.373524451624347
f1 0.5438915616063983
======== tgt result =======
===== RES ====
p 1330
tp 1330
fp 2124
recall 0.9999999992481202
precision 0.38506079896205536
f1 0.5560196652165627
===== RES ====
p 442
tp 442
fp 439
recall 0.9999999977375565
precision 0.5017026101002241
f1 0.66817793650709
======== tgt result =======
===== RES ====
p 1329
tp 1329
fp 1264
recall 0.9999999992475544
precision 0.5125337444996013
f1 0.6777150028887214
===== RES ====
p 443
tp 436
fp 152
recall 0.9841986433765267
precision 0.7414965973784071
f1 0.8457803035937367
======== tgt result =======
===== RES ====
p 1328
tp 1302
fp 433
recall 0.9804216860087185
precision 0.7504322762245347
f1 0.8501464230626494
===== RES ====
p 441
tp 356
fp 66
recall 0.8072562339971514
precision 0.8436018937355405
f1 0.8250284670444467
===== RES ====
p 443
tp 218
fp 25
recall 0.49209932168826337
precision 0.8971193378719369
f1 0.635568053766192
===== RES ====
p 444
tp 155
fp 13
recall 0.34909909831283986
precision 0.9226190421272676
f1 0.5065355477490417
===== RES ====
p 442
tp 170
fp 13
recall 0.3846153837452141
precision 0.9289617435575861
f1 0.5439995841231947
===== RES ====
p 443
tp 169
fp 13
recall 0.3814898411253051
precision 0.9285714234693878
f1 0.5407995854646346
===== RES ====
p 443
tp 164
fp 12
recall 0.3702031594352073
precision 0.9318181765237604
f1 0.5298865056937412
===== RES ====
p 442
tp 157
fp 13
recall 0.35520361910587417
precision 0.92352940633218
f1 0.5130714925138763
===== RES ====
p 442
tp 157
fp 12
recall 0.35520361910587417
precision 0.9289940773432304
f1 0.5139112184316342
===== RES ====
p 443
tp 143
fp 11
recall 0.3227990963367966
precision 0.9285714225417441
f1 0.4790615921149824
===== RES ====
p 443
tp 155
fp 10
recall 0.3498871323930313
precision 0.9393939337006428
f1 0.5098680239085038
===== RES ====
p 441
tp 146
fp 10
recall 0.3310657588864722
precision 0.9358974298980934
f1 0.48911184011657494
===== RES ====
p 441
tp 174
fp 11
recall 0.39455782223456276
precision 0.9405405354565377
f1 0.5559101249735172
===== RES ====
p 441
tp 183
fp 13
recall 0.41496598545359187
precision 0.9336734646241149
f1 0.5745678610148336
===== RES ====
p 440
tp 234
fp 18
recall 0.5318181806095041
precision 0.9285714248866214
f1 0.6763001129843793
===== RES ====
p 442
tp 259
fp 19
recall 0.5859728493530026
precision 0.931654672907717
f1 0.719443968387658
===== RES ====
p 444
tp 283
fp 11
recall 0.6373873859518302
precision 0.9625850307395067
f1 0.7669371879542657
=== Result of InvGAN+KD: ===
0.7669371879542657
The source-target datasets are: ia_da with seed 3000
The F1 score is: 0.7669371879542657
The training time is: 168.48460841178894
The inference time is: 0.00026707351207733154
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ia
tgt: ds
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 1068
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 1068
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 1065
tp 1047
fp 1229
recall 0.9830985906261984
precision 0.4600175744903262
f1 0.6267580208682579
tgt_res:
===== RES ====
p 3207
tp 3134
fp 3675
recall 0.9772372931159223
precision 0.46027316779846184
f1 0.6257982865849981
save pretrained model to: checkpoint/ia/bert/3000/source-encoder.ptdsbest
save pretrained model to: checkpoint/ia/bert/3000/source-classifier.ptdsbest
===== RES ====
p 1067
tp 1066
fp 3943
recall 0.999062791940897
precision 0.21281692948436473
f1 0.350888452937653
===== RES ====
p 1067
tp 1066
fp 4159
recall 0.999062791940897
precision 0.2040191387169341
f1 0.3388426934533378
Restore model from: /home/derossi/DADER/main/checkpoint/ia/bert/3000/source-encoder.ptdsbest
Restore model from: /home/derossi/DADER/main/checkpoint/ia/bert/3000/source-classifier.ptdsbest
Pretraining time:  71.49437952041626
=== Training F' and A ===
===== RES ====
p 1069
tp 1061
fp 1706
recall 0.99251636951121
precision 0.38344777723764084
f1 0.5531799939268547
======== tgt result =======
===== RES ====
p 3205
tp 3173
fp 4983
recall 0.9900156003151278
precision 0.3890387444348898
f1 0.5585771848589126
===== RES ====
p 1066
tp 1055
fp 1495
recall 0.9896810497282541
precision 0.4137254900338332
f1 0.583517283005952
======== tgt result =======
===== RES ====
p 3204
tp 3161
fp 4374
recall 0.9865792755971975
precision 0.41950895813941486
f1 0.5886949904706353
===== RES ====
p 1068
tp 1039
fp 1015
recall 0.9728464410366605
precision 0.5058422587605442
f1 0.6655985244620709
======== tgt result =======
===== RES ====
p 3205
tp 3109
fp 3032
recall 0.9700468015694083
precision 0.5062693371590508
f1 0.6653109123515109
===== RES ====
p 1066
tp 977
fp 445
recall 0.9165103180895775
precision 0.6870604777165538
f1 0.785369284525512
======== tgt result =======
===== RES ====
p 3206
tp 2912
fp 1316
recall 0.9082969429481295
precision 0.6887417216914045
f1 0.7834270045313289
===== RES ====
p 1070
tp 694
fp 63
recall 0.6485981302349549
precision 0.9167767491191853
f1 0.7597148942488096
===== RES ====
p 1067
tp 302
fp 10
recall 0.28303655081252427
precision 0.9679487148463183
f1 0.4379981989158476
===== RES ====
p 1069
tp 106
fp 1
recall 0.0991580915817043
precision 0.9906541963490262
f1 0.1802719431213617
===== RES ====
p 1066
tp 80
fp 1
recall 0.07504690424479651
precision 0.9876543087943913
f1 0.13949420153568484
===== RES ====
p 1066
tp 67
fp 1
recall 0.06285178230501709
precision 0.9852941031574397
f1 0.11816567188623606
===== RES ====
p 1069
tp 40
fp 1
recall 0.03741814776668087
precision 0.9756097323022017
f1 0.07207200079708193
===== RES ====
p 1068
tp 24
fp 1
recall 0.022471910091318433
precision 0.9599999616000015
f1 0.04391578321670411
===== RES ====
p 1067
tp 9
fp 0
recall 0.008434864097061982
precision 0.9999998888889013
f1 0.016728607915535442
===== RES ====
p 1068
tp 6
fp 0
recall 0.005617977522829608
precision 0.9999998333333611
f1 0.01117317322598012
===== RES ====
p 1066
tp 2
fp 0
recall 0.001876172606119913
precision 0.99999950000025
f1 0.003745314606743446
===== RES ====
p 1066
tp 2
fp 0
recall 0.001876172606119913
precision 0.99999950000025
f1 0.003745314606743446
===== RES ====
p 1069
tp 2
fp 0
recall 0.0018709073883340433
precision 0.99999950000025
f1 0.003734823529413632
===== RES ====
p 1067
tp 2
fp 0
recall 0.0018744142437915516
precision 0.99999950000025
f1 0.003741811038355472
===== RES ====
p 1068
tp 2
fp 0
recall 0.0018726591742765362
precision 0.99999950000025
f1 0.0037383140186934578
===== RES ====
p 1064
tp 2
fp 0
recall 0.0018796992463536658
precision 0.99999950000025
f1 0.00375234146341651
===== RES ====
p 1068
tp 3
fp 0
recall 0.002808988761414804
precision 0.9999996666667778
f1 0.005602235299352222
===== RES ====
p 1070
tp 3
fp 0
recall 0.002803738315136693
precision 0.9999996666667778
f1 0.0055917931086633624
=== Result of InvGAN+KD: ===
0.0055917931086633624
The source-target datasets are: ia_ds with seed 3000
The F1 score is: 0.0055917931086633624
The training time is: 286.13113808631897
The inference time is: 0.00027266889810562134
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: b2
tgt: fz
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 16
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 20
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 20
tp 20
fp 117
recall 0.9999999500000024
precision 0.14598540039426716
f1 0.2547768444969212
tgt_res:
===== RES ====
p 63
tp 63
fp 389
recall 0.9999999841269844
precision 0.13938053066508732
f1 0.24465997849391802
save pretrained model to: checkpoint/b2/bert/3000/source-encoder.ptfzbest
save pretrained model to: checkpoint/b2/bert/3000/source-classifier.ptfzbest
===== RES ====
p 20
tp 20
fp 126
recall 0.9999999500000024
precision 0.13698630043160068
f1 0.24096364058662165
===== RES ====
p 20
tp 20
fp 50
recall 0.9999999500000024
precision 0.2857142816326531
f1 0.4444440888891511
tgt_res:
===== RES ====
p 62
tp 62
fp 171
recall 0.999999983870968
precision 0.26609441945882223
f1 0.42033864820479894
save pretrained model to: checkpoint/b2/bert/3000/source-encoder.ptfzbest
save pretrained model to: checkpoint/b2/bert/3000/source-classifier.ptfzbest
Restore model from: /home/derossi/DADER/main/checkpoint/b2/bert/3000/source-encoder.ptfzbest
Restore model from: /home/derossi/DADER/main/checkpoint/b2/bert/3000/source-classifier.ptfzbest
Pretraining time:  13.58167290687561
=== Training F' and A ===
===== RES ====
p 19
tp 19
fp 75
recall 0.9999999473684238
precision 0.20212765742417385
f1 0.33628290014902285
======== tgt result =======
===== RES ====
p 61
tp 61
fp 249
recall 0.9999999836065577
precision 0.19677419291363163
f1 0.32884069380512543
===== RES ====
p 19
tp 19
fp 41
recall 0.9999999473684238
precision 0.316666661388889
f1 0.4810122807245139
======== tgt result =======
===== RES ====
p 65
tp 65
fp 148
recall 0.9999999846153849
precision 0.30516431781612996
f1 0.4676255376277245
===== RES ====
p 18
tp 18
fp 21
recall 0.9999999444444475
precision 0.4615384497041424
f1 0.6315784930750841
======== tgt result =======
===== RES ====
p 65
tp 64
fp 88
recall 0.9846153694674559
precision 0.4210526288088643
f1 0.5898613260849455
===== RES ====
p 19
tp 18
fp 3
recall 0.9473683711911383
precision 0.8571428163265326
f1 0.8999994562502784
======== tgt result =======
===== RES ====
p 61
tp 57
fp 4
recall 0.9344262141897343
precision 0.9344262141897343
f1 0.9344257141900019
===== RES ====
p 18
tp 5
fp 0
recall 0.27777776234567986
precision 0.99999980000004
f1 0.43478223062406035
===== RES ====
p 19
tp 1
fp 0
recall 0.05263157617728546
precision 0.9999990000010001
f1 0.09999989500001026
===== RES ====
p 18
tp 1
fp 0
recall 0.055555552469135974
precision 0.9999990000010001
f1 0.10526304709142413
===== RES ====
p 21
tp 1
fp 0
recall 0.04761904535147403
precision 0.9999990000010001
f1 0.09090899586777704
===== RES ====
p 18
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 17
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 17
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 17
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 18
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 17
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 15
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 17
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 18
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 20
tp 1
fp 0
recall 0.04999999750000012
precision 0.9999990000010001
f1 0.09523799546486189
===== RES ====
p 17
tp 2
fp 0
recall 0.11764705190311459
precision 0.99999950000025
f1 0.21052610526326318
===== RES ====
p 19
tp 12
fp 0
recall 0.6315789141274255
precision 0.9999999166666736
f1 0.7741930239336935
===== RES ====
p 22
tp 13
fp 0
recall 0.590909064049588
precision 0.9999999230769291
f1 0.74285663346968
=== Result of InvGAN+KD: ===
0.74285663346968
The source-target datasets are: b2_fz with seed 3000
The F1 score is: 0.74285663346968
The training time is: 66.25797605514526
The inference time is: 0.0002688094973564148
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: b2
tgt: dzy
seed: 42
train_seed: 3000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 40
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 40
tp 40
fp 99
recall 0.9999999750000007
precision 0.2877697821023757
f1 0.4469270222530049
tgt_res:
===== RES ====
p 125
tp 125
fp 303
recall 0.9999999920000001
precision 0.29205607408398115
f1 0.45207921447728
save pretrained model to: checkpoint/b2/bert/3000/source-encoder.ptdzybest
save pretrained model to: checkpoint/b2/bert/3000/source-classifier.ptdzybest
===== RES ====
p 38
tp 38
fp 120
recall 0.9999999736842113
precision 0.2405063275917321
f1 0.3877547855062874
===== RES ====
p 36
tp 36
fp 119
recall 0.999999972222223
precision 0.23225806301768992
f1 0.37696304092565747
Restore model from: /home/derossi/DADER/main/checkpoint/b2/bert/3000/source-encoder.ptdzybest
Restore model from: /home/derossi/DADER/main/checkpoint/b2/bert/3000/source-classifier.ptdzybest
Pretraining time:  11.506626844406128
=== Training F' and A ===
===== RES ====
p 40
tp 40
fp 78
recall 0.9999999750000007
precision 0.33898304797471995
f1 0.5063287293705727
======== tgt result =======
===== RES ====
p 122
tp 122
fp 249
recall 0.9999999918032788
precision 0.3288409694640405
f1 0.49492863162600087
===== RES ====
p 39
tp 39
fp 106
recall 0.999999974358975
precision 0.26896551538644475
f1 0.42391270480886084
===== RES ====
p 39
tp 39
fp 87
recall 0.999999974358975
precision 0.3095238070672714
f1 0.4727269060057821
===== RES ====
p 37
tp 37
fp 89
recall 0.9999999729729737
precision 0.2936507913202318
f1 0.45398737355591523
===== RES ====
p 41
tp 41
fp 40
recall 0.9999999756097567
precision 0.5061728332571255
f1 0.672130690271727
======== tgt result =======
===== RES ====
p 121
tp 121
fp 131
recall 0.9999999917355373
precision 0.4801587282533384
f1 0.6487931238781355
===== RES ====
p 38
tp 35
fp 3
recall 0.9210526073407209
precision 0.9210526073407209
f1 0.9210521073409924
======== tgt result =======
===== RES ====
p 123
tp 115
fp 2
recall 0.9349593419922005
precision 0.9829059745050771
f1 0.9583328256599829
===== RES ====
p 38
tp 33
fp 1
recall 0.868421029778394
precision 0.9705882067474058
f1 0.9166661427471853
===== RES ====
p 38
tp 26
fp 0
recall 0.6842105083102499
precision 0.9999999615384629
f1 0.8124994921877863
===== RES ====
p 39
tp 14
fp 0
recall 0.3589743497698885
precision 0.9999999285714337
f1 0.5283014781063661
===== RES ====
p 39
tp 4
fp 0
recall 0.10256409993425386
precision 0.9999997500000625
f1 0.18604633423484415
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 40
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 39
tp 3
fp 0
recall 0.0769230749506904
precision 0.9999996666667778
f1 0.1428570034014491
===== RES ====
p 40
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 42
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 40
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 36
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 40
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 37
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 39
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 43
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
=== Result of InvGAN+KD: ===
0.0
The source-target datasets are: b2_dzy with seed 3000
The F1 score is: 0.0
The training time is: 65.61970233917236
The inference time is: 0.0002708807587623596
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ri
tgt: ab
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 202
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 202
tp 197
fp 1119
recall 0.9752475199245172
precision 0.14969604851846804
f1 0.2595518110944776
tgt_res:
===== RES ====
p 615
tp 607
fp 3426
recall 0.9869918683138343
precision 0.15050830643429003
f1 0.2611873778455207
save pretrained model to: checkpoint/ri/bert/1000/source-encoder.ptabbest
save pretrained model to: checkpoint/ri/bert/1000/source-classifier.ptabbest
===== RES ====
p 204
tp 204
fp 1484
recall 0.9999999950980393
precision 0.12085308049712494
f1 0.21564462767472367
===== RES ====
p 204
tp 191
fp 908
recall 0.9362745052143406
precision 0.17379435834959564
f1 0.2931693440454154
tgt_res:
===== RES ====
p 613
tp 571
fp 2798
recall 0.9314845009274315
precision 0.16948649445844866
f1 0.2867902968762336
save pretrained model to: checkpoint/ri/bert/1000/source-encoder.ptabbest
save pretrained model to: checkpoint/ri/bert/1000/source-classifier.ptabbest
===== RES ====
p 203
tp 196
fp 1062
recall 0.9655172366230678
precision 0.15580286156136497
f1 0.26830913749194973
Restore model from: /home/derossi/DADER/main/checkpoint/ri/bert/1000/source-encoder.ptabbest
Restore model from: /home/derossi/DADER/main/checkpoint/ri/bert/1000/source-classifier.ptabbest
Pretraining time:  40.694411277770996
=== Training F' and A ===
===== RES ====
p 203
tp 199
fp 1136
recall 0.9802955616734209
precision 0.14906367030032683
f1 0.2587774038169909
======== tgt result =======
===== RES ====
p 612
tp 595
fp 3407
recall 0.9722222206336238
precision 0.1486756621317652
f1 0.2579104763407521
===== RES ====
p 200
tp 171
fp 735
recall 0.8549999957250001
precision 0.18874172164598044
f1 0.30922212632423046
======== tgt result =======
===== RES ====
p 613
tp 545
fp 2313
recall 0.8890701453685642
precision 0.19069279209562884
f1 0.31403024773610827
===== RES ====
p 204
tp 64
fp 117
recall 0.3137254886582084
precision 0.3535911582674522
f1 0.33246703252562426
======== tgt result =======
===== RES ====
p 614
tp 208
fp 392
recall 0.33876221443198334
precision 0.3466666660888889
f1 0.3426683627646397
===== RES ====
p 204
tp 31
fp 34
recall 0.1519607835688197
precision 0.47692306958579894
f1 0.23048290316665096
===== RES ====
p 201
tp 22
fp 18
recall 0.109452735773867
precision 0.5499999862500004
f1 0.1825723357384372
===== RES ====
p 203
tp 8
fp 4
recall 0.03940886680094154
precision 0.6666666111111158
f1 0.07441849856152646
===== RES ====
p 204
tp 2
fp 0
recall 0.009803921520569012
precision 0.99999950000025
f1 0.01941745631068932
===== RES ====
p 204
tp 2
fp 0
recall 0.009803921520569012
precision 0.99999950000025
f1 0.01941745631068932
===== RES ====
p 205
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 203
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 204
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 203
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 199
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 200
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 204
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 203
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 205
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 201
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 205
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 203
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 206
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
=== Result of InvGAN+KD: ===
0.0
The source-target datasets are: ri_ab with seed 1000
The F1 score is: 0.0
The training time is: 152.0484139919281
The inference time is: 0.00027086585760116577
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ri
tgt: wa1
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 192
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 193
fp 1831
recall 0.9999999948186529
precision 0.09535573117818393
f1 0.1741089974087367
tgt_res:
===== RES ====
p 576
tp 576
fp 5512
recall 0.9999999982638889
precision 0.09461235215265894
f1 0.17286898968004968
save pretrained model to: checkpoint/ri/bert/1000/source-encoder.ptwa1best
save pretrained model to: checkpoint/ri/bert/1000/source-classifier.ptwa1best
===== RES ====
p 193
tp 193
fp 1800
recall 0.9999999948186529
precision 0.09683893622837986
f1 0.1765780639189206
tgt_res:
===== RES ====
p 576
tp 575
fp 5431
recall 0.9982638871557918
precision 0.09573759572165541
f1 0.17471877065704167
save pretrained model to: checkpoint/ri/bert/1000/source-encoder.ptwa1best
save pretrained model to: checkpoint/ri/bert/1000/source-classifier.ptwa1best
===== RES ====
p 193
tp 193
fp 1830
recall 0.9999999948186529
precision 0.0954028669820055
f1 0.17418756645766062
===== RES ====
p 193
tp 193
fp 1843
recall 0.9999999948186529
precision 0.09479371311650603
f1 0.1731716675980886
Restore model from: /home/derossi/DADER/main/checkpoint/ri/bert/1000/source-encoder.ptwa1best
Restore model from: /home/derossi/DADER/main/checkpoint/ri/bert/1000/source-classifier.ptwa1best
Pretraining time:  46.7586932182312
=== Training F' and A ===
===== RES ====
p 192
tp 192
fp 1820
recall 0.9999999947916667
precision 0.09542743534024481
f1 0.17422851592729896
======== tgt result =======
===== RES ====
p 576
tp 576
fp 5483
recall 0.9999999982638889
precision 0.0950651922602632
f1 0.17362455880354893
===== RES ====
p 193
tp 193
fp 1794
recall 0.9999999948186529
precision 0.09713135375081461
f1 0.1770640586328373
======== tgt result =======
===== RES ====
p 576
tp 575
fp 5426
recall 0.9982638871557918
precision 0.09581736375673765
f1 0.17485159625072574
===== RES ====
p 193
tp 192
fp 1753
recall 0.9948186476952402
precision 0.09871465290554517
f1 0.17960694503546318
======== tgt result =======
===== RES ====
p 576
tp 573
fp 5272
recall 0.9947916649395978
precision 0.09803250639896792
f1 0.1784767093887116
===== RES ====
p 193
tp 189
fp 1677
recall 0.9792746063250021
precision 0.10128617357916067
f1 0.1835840941305466
======== tgt result =======
===== RES ====
p 576
tp 571
fp 5045
recall 0.9913194427234038
precision 0.10167378915568485
f1 0.18443135574852748
===== RES ====
p 193
tp 188
fp 1564
recall 0.9740932592015893
precision 0.10730593601181167
f1 0.19331601640925525
======== tgt result =======
===== RES ====
p 576
tp 557
fp 4705
recall 0.9670138872100453
precision 0.10585328770318257
f1 0.19081859562866937
===== RES ====
p 193
tp 171
fp 1047
recall 0.8860103581035733
precision 0.14039408855468463
f1 0.24238105337602872
======== tgt result =======
===== RES ====
p 576
tp 531
fp 3176
recall 0.9218749983995226
precision 0.14324251412375438
f1 0.24795680654493146
===== RES ====
p 193
tp 127
fp 428
recall 0.6580310846734141
precision 0.22882882841652463
f1 0.33957180871314807
======== tgt result =======
===== RES ====
p 576
tp 408
fp 1378
recall 0.708333332103588
precision 0.2284434489202444
f1 0.34546957165030334
===== RES ====
p 193
tp 45
fp 59
recall 0.23316062055357192
precision 0.43269230353180477
f1 0.30302984588942444
===== RES ====
p 193
tp 32
fp 46
recall 0.1658031079492067
precision 0.4102564049967128
f1 0.2361619499196929
===== RES ====
p 193
tp 42
fp 103
recall 0.2176165791833338
precision 0.2896551704161712
f1 0.24852021867328103
===== RES ====
p 193
tp 54
fp 333
recall 0.2797927446642863
precision 0.13953488336037498
f1 0.1862064518500479
===== RES ====
p 193
tp 28
fp 76
recall 0.14507771945555586
precision 0.26923076664201184
f1 0.1885517321826213
===== RES ====
p 193
tp 5
fp 2
recall 0.025906735617063546
precision 0.7142856122449125
f1 0.04999993195008261
===== RES ====
p 193
tp 1
fp 0
recall 0.005181347123412709
precision 0.9999990000010001
f1 0.010309267988096612
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
=== Result of InvGAN+KD: ===
0.0
The source-target datasets are: ri_wa1 with seed 1000
The F1 score is: 0.0
The training time is: 184.50623321533203
The inference time is: 0.0002667456865310669
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ia
tgt: da
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 443
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 444
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 442
tp 442
fp 1130
recall 0.9999999977375565
precision 0.28117048328169814
f1 0.43892716441342666
tgt_res:
===== RES ====
p 1329
tp 1329
fp 3410
recall 0.9999999992475544
precision 0.2804389111035157
f1 0.43803525433006874
save pretrained model to: checkpoint/ia/bert/1000/source-encoder.ptdabest
save pretrained model to: checkpoint/ia/bert/1000/source-classifier.ptdabest
===== RES ====
p 440
tp 440
fp 1805
recall 0.9999999977272728
precision 0.19599109122673003
f1 0.32774646687288156
===== RES ====
p 442
tp 442
fp 1505
recall 0.9999999977375565
precision 0.22701592181457836
f1 0.3700289990847547
Restore model from: /home/derossi/DADER/main/checkpoint/ia/bert/1000/source-encoder.ptdabest
Restore model from: /home/derossi/DADER/main/checkpoint/ia/bert/1000/source-classifier.ptdabest
Pretraining time:  40.01579403877258
=== Training F' and A ===
===== RES ====
p 443
tp 443
fp 1054
recall 0.9999999977426637
precision 0.29592518350305597
f1 0.45670067804416575
======== tgt result =======
===== RES ====
p 1327
tp 1327
fp 3205
recall 0.9999999992464205
precision 0.29280670779064283
f1 0.45297797340806734
===== RES ====
p 441
tp 441
fp 1043
recall 0.9999999977324263
precision 0.2971698111205055
f1 0.45818146448952823
======== tgt result =======
===== RES ====
p 1327
tp 1327
fp 3164
recall 0.9999999992464205
precision 0.2954798485202672
f1 0.4561701530470093
===== RES ====
p 443
tp 443
fp 881
recall 0.9999999977426637
precision 0.33459214476239263
f1 0.5014144511173944
======== tgt result =======
===== RES ====
p 1328
tp 1328
fp 2627
recall 0.9999999992469879
precision 0.33577749675454427
f1 0.5027442761008704
===== RES ====
p 441
tp 441
fp 638
recall 0.9999999977324263
precision 0.4087117697787657
f1 0.5802627452210333
======== tgt result =======
===== RES ====
p 1329
tp 1329
fp 1888
recall 0.9999999992475544
precision 0.4131178114973212
f1 0.5846894232038594
===== RES ====
p 443
tp 443
fp 374
recall 0.9999999977426637
precision 0.5422276615150212
f1 0.7031741461114066
======== tgt result =======
===== RES ====
p 1330
tp 1329
fp 1129
recall 0.9992481195494374
precision 0.5406834822861337
f1 0.7016890899015458
===== RES ====
p 444
tp 444
fp 212
recall 0.9999999977477477
precision 0.676829267260931
f1 0.8072722443771465
======== tgt result =======
===== RES ====
p 1328
tp 1326
fp 606
recall 0.9984939751517364
precision 0.6863354033714619
f1 0.8134964491801578
===== RES ====
p 442
tp 439
fp 118
recall 0.9932126674361704
precision 0.7881508064844689
f1 0.8788783837453833
======== tgt result =======
===== RES ====
p 1331
tp 1322
fp 335
recall 0.9932381660456512
precision 0.7978273984322103
f1 0.8848723299915932
===== RES ====
p 444
tp 434
fp 95
recall 0.9774774752759516
precision 0.8204158774661325
f1 0.8920858329176157
======== tgt result =======
===== RES ====
p 1327
tp 1296
fp 248
recall 0.9766390346822614
precision 0.8393782377983301
f1 0.9028208188421959
===== RES ====
p 439
tp 428
fp 74
recall 0.9749430501709726
precision 0.8525896397358772
f1 0.9096700635386205
======== tgt result =======
===== RES ====
p 1327
tp 1278
fp 193
recall 0.9630746036450077
precision 0.8687967363230477
f1 0.9135091504214603
===== RES ====
p 444
tp 404
fp 48
recall 0.9099099078605632
precision 0.8938053077570679
f1 0.9017852123129366
===== RES ====
p 442
tp 317
fp 19
recall 0.7171945685131345
precision 0.943452378144487
f1 0.8149095328939455
===== RES ====
p 443
tp 201
fp 14
recall 0.4537246039419309
precision 0.9348837165819363
f1 0.6109418074161757
===== RES ====
p 441
tp 99
fp 4
recall 0.22448979540932018
precision 0.9611650392119899
f1 0.3639702799188886
===== RES ====
p 442
tp 36
fp 0
recall 0.08144796361663356
precision 0.999999972222223
f1 0.15062747514937036
===== RES ====
p 442
tp 9
fp 0
recall 0.02036199090415839
precision 0.9999998888889013
f1 0.039911268912183155
===== RES ====
p 442
tp 3
fp 0
recall 0.006787330301386131
precision 0.9999996666667778
f1 0.013483132614577797
===== RES ====
p 443
tp 2
fp 0
recall 0.0045146726760391135
precision 0.99999950000025
f1 0.00898875505618427
===== RES ====
p 442
tp 1
fp 0
recall 0.002262443433795377
precision 0.9999990000010001
f1 0.004514668161366449
===== RES ====
p 444
tp 1
fp 0
recall 0.002252252247179612
precision 0.9999990000010001
f1 0.004494377517990173
===== RES ====
p 444
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 444
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
=== Result of InvGAN+KD: ===
0.0
The source-target datasets are: ia_da with seed 1000
The F1 score is: 0.0
The training time is: 210.14495420455933
The inference time is: 0.0002749338746070862
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ia
tgt: ds
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 1065
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 1070
tp 5
fp 0
recall 0.004672897191894488
precision 0.99999980000004
f1 0.009302316305037136
tgt_res:
===== RES ====
p 3205
tp 6
fp 1
recall 0.0018720748824112093
precision 0.857142734693895
f1 0.003735985685881969
save pretrained model to: checkpoint/ia/bert/1000/source-encoder.ptdsbest
save pretrained model to: checkpoint/ia/bert/1000/source-classifier.ptdsbest
===== RES ====
p 1067
tp 1066
fp 3887
recall 0.999062791940897
precision 0.2152230970694078
f1 0.3541525321476225
tgt_res:
===== RES ====
p 3205
tp 3205
fp 11622
recall 0.9999999996879876
precision 0.2161597086250651
f1 0.35547885584538846
save pretrained model to: checkpoint/ia/bert/1000/source-encoder.ptdsbest
save pretrained model to: checkpoint/ia/bert/1000/source-classifier.ptdsbest
===== RES ====
p 1069
tp 1068
fp 3968
recall 0.9990645453703791
precision 0.2120730738260379
f1 0.3498768608797017
===== RES ====
p 1066
tp 1063
fp 3566
recall 0.9971857401527338
precision 0.22963923088579838
f1 0.3733096165629953
tgt_res:
===== RES ====
p 3205
tp 3203
fp 10730
recall 0.9993759747271839
precision 0.22988588242087948
f1 0.37378893616520137
save pretrained model to: checkpoint/ia/bert/1000/source-encoder.ptdsbest
save pretrained model to: checkpoint/ia/bert/1000/source-classifier.ptdsbest
Restore model from: /home/derossi/DADER/main/checkpoint/ia/bert/1000/source-encoder.ptdsbest
Restore model from: /home/derossi/DADER/main/checkpoint/ia/bert/1000/source-classifier.ptdsbest
Pretraining time:  104.91313028335571
=== Training F' and A ===
===== RES ====
p 1067
tp 1066
fp 3929
recall 0.999062791940897
precision 0.213413413370688
f1 0.3516988190222748
======== tgt result =======
===== RES ====
p 3206
tp 3205
fp 11850
recall 0.9996880845291054
precision 0.2128860843432158
f1 0.35102101270619035
===== RES ====
p 1068
tp 1067
fp 3566
recall 0.999063669476532
precision 0.23030433839190495
f1 0.3743199900721832
======== tgt result =======
===== RES ====
p 3207
tp 3206
fp 10720
recall 0.999688181789932
precision 0.23021686053208265
f1 0.3742482219018929
===== RES ====
p 1068
tp 1065
fp 3017
recall 0.9971910103022555
precision 0.26090151879938717
f1 0.41359190410387137
======== tgt result =======
===== RES ====
p 3205
tp 3200
fp 8930
recall 0.9984399372859782
precision 0.2638087386427198
f1 0.4173456099675941
===== RES ====
p 1067
tp 1062
fp 2405
recall 0.9953139634533139
precision 0.3063167002289251
f1 0.4684601604025227
======== tgt result =======
===== RES ====
p 3205
tp 3193
fp 7156
recall 0.9962558499231652
precision 0.30853222530596847
f1 0.4711520661634981
===== RES ====
p 1068
tp 1058
fp 1513
recall 0.9906367031922876
precision 0.41151302978937654
f1 0.5814780131153682
======== tgt result =======
===== RES ====
p 3206
tp 3174
fp 4526
recall 0.9900187146007428
precision 0.41220779215425873
f1 0.582064503186603
===== RES ====
p 1067
tp 1034
fp 750
recall 0.9690721640402322
precision 0.5795964122311679
f1 0.7253590540896391
======== tgt result =======
===== RES ====
p 3207
tp 3082
fp 2264
recall 0.9610227624069153
precision 0.5765057986201821
f1 0.7206823324599584
===== RES ====
p 1066
tp 990
fp 351
recall 0.928705440029357
precision 0.7382550330065212
f1 0.8226002536621938
======== tgt result =======
===== RES ====
p 3207
tp 2947
fp 995
recall 0.9189273461431471
precision 0.7475900556195865
f1 0.8244504772186987
===== RES ====
p 1069
tp 920
fp 158
recall 0.8606173986336599
precision 0.8534322812120293
f1 0.857009280300638
======== tgt result =======
===== RES ====
p 3207
tp 2726
fp 421
recall 0.8500155906298673
precision 0.8662217982630372
f1 0.8580416779302885
===== RES ====
p 1068
tp 826
fp 65
recall 0.7734082389762095
precision 0.9270482593411355
f1 0.8432868947474059
===== RES ====
p 1066
tp 726
fp 36
recall 0.6810506560215284
precision 0.9527559042614752
f1 0.7943102350600776
===== RES ====
p 1068
tp 627
fp 22
recall 0.5870786511356941
precision 0.9661016934266538
f1 0.7303431515225262
===== RES ====
p 1066
tp 499
fp 13
recall 0.46810506522691825
precision 0.9746093730964661
f1 0.6324456951736392
===== RES ====
p 1070
tp 399
fp 8
recall 0.37289719591318016
precision 0.9803439779352728
f1 0.540283960206187
===== RES ====
p 1068
tp 295
fp 5
recall 0.2762172282057891
precision 0.9833333300555556
f1 0.4312862066642877
===== RES ====
p 1068
tp 234
fp 3
recall 0.21910112339035473
precision 0.9873417679858997
f1 0.35862039185121686
===== RES ====
p 1067
tp 164
fp 2
recall 0.15370196799090724
precision 0.9879518012773988
f1 0.26601760921792433
===== RES ====
p 1068
tp 140
fp 2
recall 0.13108614219935752
precision 0.9859154860146797
f1 0.23140475112920575
===== RES ====
p 1067
tp 124
fp 2
recall 0.1162136831150762
precision 0.9841269763164526
f1 0.2078791066217105
===== RES ====
p 1068
tp 109
fp 1
recall 0.10205992499807122
precision 0.9909090819008266
f1 0.18505925311829885
===== RES ====
p 1066
tp 112
fp 2
recall 0.10506566594271513
precision 0.982456131732841
f1 0.18983033359969978
===== RES ====
p 1070
tp 125
fp 1
recall 0.11682242979736221
precision 0.9920634841899724
f1 0.2090299114804656
=== Result of InvGAN+KD: ===
0.2090299114804656
The source-target datasets are: ia_ds with seed 1000
The F1 score is: 0.2090299114804656
The training time is: 363.3614044189453
The inference time is: 0.000272408127784729
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: b2
tgt: fz
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 17
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 16
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 21
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 18
tp 18
fp 22
recall 0.9999999444444475
precision 0.4499999887500003
f1 0.6206892057077821
tgt_res:
===== RES ====
p 62
tp 62
fp 82
recall 0.999999983870968
precision 0.4305555525655865
f1 0.6019413209541997
save pretrained model to: checkpoint/b2/bert/1000/source-encoder.ptfzbest
save pretrained model to: checkpoint/b2/bert/1000/source-classifier.ptfzbest
===== RES ====
p 16
tp 16
fp 93
recall 0.9999999375000038
precision 0.1467889894790001
f1 0.2559997726721859
Restore model from: /home/derossi/DADER/main/checkpoint/b2/bert/1000/source-encoder.ptfzbest
Restore model from: /home/derossi/DADER/main/checkpoint/b2/bert/1000/source-classifier.ptfzbest
Pretraining time:  11.55567455291748
=== Training F' and A ===
===== RES ====
p 19
tp 19
fp 18
recall 0.9999999473684238
precision 0.5135134996347703
f1 0.6785709559951915
======== tgt result =======
===== RES ====
p 62
tp 61
fp 72
recall 0.9838709518730492
precision 0.4586466130928826
f1 0.6256405855098326
===== RES ====
p 17
tp 17
fp 13
recall 0.999999941176474
precision 0.5666666477777784
f1 0.723403762788885
======== tgt result =======
===== RES ====
p 64
tp 63
fp 48
recall 0.9843749846191409
precision 0.5675675624543463
f1 0.7199995278370329
===== RES ====
p 22
tp 22
fp 4
recall 0.9999999545454565
precision 0.8461538136094686
f1 0.9166661319447147
======== tgt result =======
===== RES ====
p 65
tp 63
fp 7
recall 0.9692307543195269
precision 0.8999999871428573
f1 0.9333328201923111
===== RES ====
p 16
tp 10
fp 0
recall 0.6249999609375024
precision 0.99999990000001
f1 0.7692302366866823
===== RES ====
p 20
tp 2
fp 0
recall 0.09999999500000024
precision 0.99999950000025
f1 0.18181800000009088
===== RES ====
p 20
tp 1
fp 0
recall 0.04999999750000012
precision 0.9999990000010001
f1 0.09523799546486189
===== RES ====
p 22
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 19
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 20
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 19
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 16
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 17
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 17
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 20
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 19
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 16
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 20
tp 2
fp 0
recall 0.09999999500000024
precision 0.99999950000025
f1 0.18181800000009088
===== RES ====
p 17
tp 8
fp 1
recall 0.47058820761245834
precision 0.8888887901234679
f1 0.6153841153849448
===== RES ====
p 19
tp 18
fp 16
recall 0.9473683711911383
precision 0.5294117491349486
f1 0.6792447974371196
===== RES ====
p 18
tp 18
fp 59
recall 0.9999999444444475
precision 0.2337662307303087
f1 0.3789470532966399
===== RES ====
p 22
tp 22
fp 57
recall 0.9999999545454565
precision 0.27848100913315177
f1 0.435643214979184
=== Result of InvGAN+KD: ===
0.435643214979184
The source-target datasets are: b2_fz with seed 1000
The F1 score is: 0.435643214979184
The training time is: 65.7611997127533
The inference time is: 0.00026632845401763916
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: b2
tgt: dzy
seed: 42
train_seed: 1000
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 39
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 37
tp 37
fp 104
recall 0.9999999729729737
precision 0.26241134565665714
f1 0.41573000309330094
tgt_res:
===== RES ====
p 122
tp 122
fp 300
recall 0.9999999918032788
precision 0.28909952538128075
f1 0.4485290621758744
save pretrained model to: checkpoint/b2/bert/1000/source-encoder.ptdzybest
save pretrained model to: checkpoint/b2/bert/1000/source-classifier.ptdzybest
===== RES ====
p 38
tp 38
fp 81
recall 0.9999999736842113
precision 0.3193277284090107
f1 0.4840760600432786
tgt_res:
===== RES ====
p 122
tp 122
fp 264
recall 0.9999999918032788
precision 0.31606217534698916
f1 0.48031459377546387
save pretrained model to: checkpoint/b2/bert/1000/source-encoder.ptdzybest
save pretrained model to: checkpoint/b2/bert/1000/source-classifier.ptdzybest
Restore model from: /home/derossi/DADER/main/checkpoint/b2/bert/1000/source-encoder.ptdzybest
Restore model from: /home/derossi/DADER/main/checkpoint/b2/bert/1000/source-classifier.ptdzybest
Pretraining time:  13.344129085540771
=== Training F' and A ===
===== RES ====
p 37
tp 37
fp 78
recall 0.9999999729729737
precision 0.3217391276370511
f1 0.48684173052312857
======== tgt result =======
===== RES ====
p 124
tp 124
fp 252
recall 0.999999991935484
precision 0.3297872331654595
f1 0.49599962502427947
===== RES ====
p 40
tp 39
fp 61
recall 0.9749999756250006
precision 0.3899999961
f1 0.5571424410207048
======== tgt result =======
===== RES ====
p 123
tp 123
fp 208
recall 0.9999999918699188
precision 0.3716012073365522
f1 0.5418498228282093
===== RES ====
p 41
tp 40
fp 30
recall 0.9756097323022017
precision 0.5714285632653062
f1 0.7207202418637852
======== tgt result =======
===== RES ====
p 122
tp 121
fp 85
recall 0.9918032705589896
precision 0.5873786379253464
f1 0.7378044063432462
===== RES ====
p 38
tp 36
fp 5
recall 0.9473683961218844
precision 0.8780487590719815
f1 0.9113918827113782
======== tgt result =======
===== RES ====
p 121
tp 118
fp 12
recall 0.9752066035106892
precision 0.9076923007100592
f1 0.9402385369758689
===== RES ====
p 39
tp 35
fp 1
recall 0.8974358744247213
precision 0.9722221952160502
f1 0.9333328092447122
======== tgt result =======
===== RES ====
p 123
tp 115
fp 0
recall 0.9349593419922005
precision 0.9999999913043479
f1 0.9663860470661434
===== RES ====
p 37
tp 31
fp 0
recall 0.8378378151935726
precision 0.9999999677419364
f1 0.9117641829587481
===== RES ====
p 37
tp 22
fp 0
recall 0.5945945785244708
precision 0.9999999545454565
f1 0.7457622189029062
===== RES ====
p 41
tp 9
fp 0
recall 0.21951218976799539
precision 0.9999998888889013
f1 0.35999969040022634
===== RES ====
p 40
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 41
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 37
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 37
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 37
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 36
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 37
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 40
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 43
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
=== Result of InvGAN+KD: ===
0.0
The source-target datasets are: b2_dzy with seed 1000
The F1 score is: 0.0
The training time is: 66.5645649433136
The inference time is: 0.0002679601311683655
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ri
tgt: ab
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 204
tp 69
fp 168
recall 0.3382352924596309
precision 0.29113923927789354
f1 0.31292467144941477
tgt_res:
===== RES ====
p 615
tp 205
fp 524
recall 0.33333333279132793
precision 0.28120713267324127
f1 0.30505902695371134
save pretrained model to: checkpoint/ri/bert/42/source-encoder.ptabbest
save pretrained model to: checkpoint/ri/bert/42/source-classifier.ptabbest
===== RES ====
p 204
tp 202
fp 1343
recall 0.9901960735774702
precision 0.13074433648495512
f1 0.23098893031828371
===== RES ====
p 203
tp 202
fp 1354
recall 0.995073886723774
precision 0.12982005133044983
f1 0.22967574780898092
===== RES ====
p 203
tp 201
fp 1284
recall 0.9901477783736563
precision 0.1353535352623882
f1 0.23815144688971698
===== RES ====
p 203
tp 193
fp 1109
recall 0.9507389115727147
precision 0.14823348682931375
f1 0.25647817159478564
Restore model from: /home/derossi/DADER/main/checkpoint/ri/bert/42/source-encoder.ptabbest
Restore model from: /home/derossi/DADER/main/checkpoint/ri/bert/42/source-classifier.ptabbest
Pretraining time:  38.525267362594604
=== Training F' and A ===
===== RES ====
p 205
tp 63
fp 122
recall 0.30731707167162403
precision 0.3405405386997809
f1 0.3230764227358125
======== tgt result =======
===== RES ====
p 616
tp 171
fp 392
recall 0.2775974021467575
precision 0.3037300172225044
f1 0.290075836397051
===== RES ====
p 204
tp 30
fp 43
recall 0.1470588228085352
precision 0.4109588984800151
f1 0.21660610846033768
===== RES ====
p 204
tp 1
fp 0
recall 0.004901960760284506
precision 0.9999990000010001
f1 0.009756087757287424
===== RES ====
p 205
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 199
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 205
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 202
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 205
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 201
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 202
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 203
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 204
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 202
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 202
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 204
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 203
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 203
tp 6
fp 1
recall 0.02955665010070616
precision 0.857142734693895
f1 0.0571427921542594
===== RES ====
p 204
tp 5
fp 0
recall 0.02450980380142253
precision 0.99999980000004
f1 0.047846842792097534
===== RES ====
p 203
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 199
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 206
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
=== Result of InvGAN+KD: ===
0.0
The source-target datasets are: ri_ab with seed 42
The F1 score is: 0.0
The training time is: 139.02690172195435
The inference time is: 0.0002655908465385437
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ri
tgt: wa1
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 193
tp 147
fp 863
recall 0.7616580271416683
precision 0.14554455431134203
f1 0.2443887576379283
tgt_res:
===== RES ====
p 576
tp 456
fp 2520
recall 0.7916666652922454
precision 0.15322580640012573
f1 0.25675648488128316
save pretrained model to: checkpoint/ri/bert/42/source-encoder.ptwa1best
save pretrained model to: checkpoint/ri/bert/42/source-classifier.ptwa1best
===== RES ====
p 193
tp 193
fp 1813
recall 0.9999999948186529
precision 0.09621136585433132
f1 0.1755341735003991
===== RES ====
p 193
tp 193
fp 1844
recall 0.9999999948186529
precision 0.09474717717489092
f1 0.17309401213511535
===== RES ====
p 193
tp 193
fp 1847
recall 0.9999999948186529
precision 0.0946078430908785
f1 0.17286146306175118
===== RES ====
p 193
tp 193
fp 1839
recall 0.9999999948186529
precision 0.09498031491388763
f1 0.17348298747667482
Restore model from: /home/derossi/DADER/main/checkpoint/ri/bert/42/source-encoder.ptwa1best
Restore model from: /home/derossi/DADER/main/checkpoint/ri/bert/42/source-classifier.ptwa1best
Pretraining time:  37.05047249794006
=== Training F' and A ===
===== RES ====
p 193
tp 154
fp 911
recall 0.7979274570055572
precision 0.1446009388313606
f1 0.24483280821150671
======== tgt result =======
===== RES ====
p 576
tp 472
fp 2685
recall 0.8194444430217979
precision 0.14950902751045012
f1 0.2528794602861663
===== RES ====
p 193
tp 139
fp 790
recall 0.7202072501543666
precision 0.1496232506462613
f1 0.24777155071508675
======== tgt result =======
===== RES ====
p 576
tp 438
fp 2368
recall 0.7604166653464989
precision 0.15609408404985955
f1 0.25901804958133345
===== RES ====
p 193
tp 96
fp 402
recall 0.4974093238476201
precision 0.19277108395025888
f1 0.27785777316433163
======== tgt result =======
===== RES ====
p 576
tp 306
fp 1238
recall 0.531249999077691
precision 0.19818652836905015
f1 0.2886788492547253
===== RES ====
p 192
tp 12
fp 27
recall 0.06249999967447917
precision 0.3076922998027616
f1 0.10389582234291435
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 192
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 193
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
=== Result of InvGAN+KD: ===
0.0
The source-target datasets are: ri_wa1 with seed 42
The F1 score is: 0.0
The training time is: 156.85858869552612
The inference time is: 0.0002675577998161316
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ia
tgt: da
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 440
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 443
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 444
tp 444
fp 604
recall 0.9999999977477477
precision 0.4236641217331449
f1 0.595173843878996
tgt_res:
===== RES ====
p 1327
tp 1326
fp 1787
recall 0.9992464197443508
precision 0.42595566963509296
f1 0.5972968779319757
save pretrained model to: checkpoint/ia/bert/42/source-encoder.ptdabest
save pretrained model to: checkpoint/ia/bert/42/source-classifier.ptdabest
===== RES ====
p 444
tp 444
fp 567
recall 0.9999999977477477
precision 0.4391691390314845
f1 0.6103088534413261
tgt_res:
===== RES ====
p 1329
tp 1328
fp 1651
recall 0.9992475538004156
precision 0.44578717675535845
f1 0.6165269639624213
save pretrained model to: checkpoint/ia/bert/42/source-encoder.ptdabest
save pretrained model to: checkpoint/ia/bert/42/source-classifier.ptdabest
===== RES ====
p 442
tp 442
fp 664
recall 0.9999999977375565
precision 0.3996383359858604
f1 0.5710590227820419
Restore model from: /home/derossi/DADER/main/checkpoint/ia/bert/42/source-encoder.ptdabest
Restore model from: /home/derossi/DADER/main/checkpoint/ia/bert/42/source-classifier.ptdabest
Pretraining time:  49.28770470619202
=== Training F' and A ===
===== RES ====
p 441
tp 441
fp 612
recall 0.9999999977324263
precision 0.41880341840569474
f1 0.5903610288949883
======== tgt result =======
===== RES ====
p 1326
tp 1325
fp 1794
recall 0.9992458514334495
precision 0.42481564590419507
f1 0.5961750591528155
===== RES ====
p 443
tp 443
fp 603
recall 0.9999999977426637
precision 0.42351816403105336
f1 0.5950298028265713
======== tgt result =======
===== RES ====
p 1326
tp 1325
fp 1785
recall 0.9992458514334495
precision 0.42604501594017846
f1 0.5973846121589094
===== RES ====
p 442
tp 442
fp 438
recall 0.9999999977375565
precision 0.5022727267019628
f1 0.6686833662793259
======== tgt result =======
===== RES ====
p 1327
tp 1325
fp 1230
recall 0.9984928402422811
precision 0.5185909978400818
f1 0.6826373652404928
===== RES ====
p 442
tp 442
fp 239
recall 0.9999999977375565
precision 0.6490455203391402
f1 0.7871767251632188
======== tgt result =======
===== RES ====
p 1324
tp 1321
fp 693
recall 0.997734138219234
precision 0.655908639197662
f1 0.7914914322148415
===== RES ====
p 441
tp 439
fp 123
recall 0.9954648503504199
precision 0.7811387886456606
f1 0.8753733838964453
======== tgt result =======
===== RES ====
p 1326
tp 1312
fp 345
recall 0.9894419298722157
precision 0.7917923954183509
f1 0.8796508632603939
===== RES ====
p 443
tp 432
fp 77
recall 0.9751692980244485
precision 0.848722984580112
f1 0.907562525706879
======== tgt result =======
===== RES ====
p 1329
tp 1292
fp 211
recall 0.9721595177034164
precision 0.859614104551155
f1 0.9124288797744528
===== RES ====
p 443
tp 402
fp 60
recall 0.9074492078838619
precision 0.8701298682464722
f1 0.8883972883126038
===== RES ====
p 442
tp 382
fp 52
recall 0.864253391709834
precision 0.8801843297691605
f1 0.8721456167722472
===== RES ====
p 442
tp 341
fp 34
recall 0.7714932109242235
precision 0.9093333309084445
f1 0.834760823228843
===== RES ====
p 444
tp 310
fp 28
recall 0.6981981966256797
precision 0.9171597606001191
f1 0.7928383818397737
===== RES ====
p 441
tp 286
fp 17
recall 0.648526075626925
precision 0.9438943863237809
f1 0.7688167194367986
===== RES ====
p 441
tp 248
fp 14
recall 0.5623582753688021
precision 0.9465648818833402
f1 0.7055471833255832
===== RES ====
p 442
tp 220
fp 13
recall 0.49773755543498294
precision 0.9442060045313048
f1 0.651851397856006
===== RES ====
p 443
tp 208
fp 9
recall 0.4695259583080678
precision 0.9585253412049524
f1 0.6303025870205108
===== RES ====
p 441
tp 222
fp 11
recall 0.503401359402718
precision 0.9527896954815893
f1 0.6587532548629931
===== RES ====
p 444
tp 246
fp 13
recall 0.5540540528061846
precision 0.9498069461397415
f1 0.6998572851246191
===== RES ====
p 444
tp 301
fp 19
recall 0.6779279264010633
precision 0.9406249970605469
f1 0.7879576262920692
===== RES ====
p 442
tp 322
fp 19
recall 0.7285067856821114
precision 0.9442815221575322
f1 0.8224771562826608
===== RES ====
p 443
tp 359
fp 23
recall 0.8103837453490209
precision 0.9397905734560457
f1 0.8703025309270059
===== RES ====
p 442
tp 365
fp 24
recall 0.8257918533353126
precision 0.9383033394902228
f1 0.87845918704386
===== RES ====
p 444
tp 376
fp 18
recall 0.8468468449395341
precision 0.9543147183900642
f1 0.8973742013092234
=== Result of InvGAN+KD: ===
0.8973742013092234
The source-target datasets are: ia_da with seed 42
The F1 score is: 0.8973742013092234
The training time is: 185.33246302604675
The inference time is: 0.0002666488289833069
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: ia
tgt: ds
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 1065
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 1065
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 1067
tp 1062
fp 2372
recall 0.9953139634533139
precision 0.3092603377084274
f1 0.47189477248170403
tgt_res:
===== RES ====
p 3206
tp 3193
fp 7034
recall 0.9959451026213522
precision 0.312212770087786
f1 0.4753960483420792
save pretrained model to: checkpoint/ia/bert/42/source-encoder.ptdsbest
save pretrained model to: checkpoint/ia/bert/42/source-classifier.ptdsbest
===== RES ====
p 1068
tp 1063
fp 2893
recall 0.995318351127979
precision 0.2687057633294475
f1 0.42316845486167526
===== RES ====
p 1067
tp 1056
fp 2736
recall 0.9896907207219392
precision 0.27848101258478875
f1 0.4346569939786811
Restore model from: /home/derossi/DADER/main/checkpoint/ia/bert/42/source-encoder.ptdsbest
Restore model from: /home/derossi/DADER/main/checkpoint/ia/bert/42/source-classifier.ptdsbest
Pretraining time:  71.66581583023071
=== Training F' and A ===
===== RES ====
p 1068
tp 1058
fp 1923
recall 0.9906367031922876
precision 0.3549144581164326
f1 0.5225977837407253
======== tgt result =======
===== RES ====
p 3207
tp 3180
fp 5687
recall 0.9915809164354284
precision 0.35863313405225744
f1 0.5267513076516259
===== RES ====
p 1066
tp 1060
fp 2197
recall 0.9943714812435539
precision 0.3254528706400206
f1 0.4903998132653412
===== RES ====
p 1068
tp 1058
fp 1920
recall 0.9906367031922876
precision 0.35527199450796776
f1 0.5229852760215459
======== tgt result =======
===== RES ====
p 3205
tp 3182
fp 5692
recall 0.9928237126387446
precision 0.3585756141133
f1 0.5268644167362315
===== RES ====
p 1070
tp 1059
fp 1568
recall 0.9897196252432526
precision 0.4031214311369922
f1 0.5728965318426703
======== tgt result =======
===== RES ====
p 3206
tp 3170
fp 4607
recall 0.988771053964825
precision 0.40761218973799507
f1 0.5772553454017947
===== RES ====
p 1068
tp 1031
fp 804
recall 0.9653558043395544
precision 0.5618528607292355
f1 0.7102992243901496
======== tgt result =======
===== RES ====
p 3205
tp 3087
fp 2386
recall 0.9631825270005671
precision 0.5640416589504765
f1 0.7114537861202181
===== RES ====
p 1070
tp 980
fp 298
recall 0.9158878496113196
precision 0.7668231605893402
f1 0.8347524844736514
======== tgt result =======
===== RES ====
p 3205
tp 2866
fp 932
recall 0.8942277688317543
precision 0.7546076880582918
f1 0.8185058577712583
===== RES ====
p 1068
tp 829
fp 89
recall 0.7762172277376242
precision 0.9030501079487472
f1 0.8348434093633196
======== tgt result =======
===== RES ====
p 3204
tp 2486
fp 324
recall 0.7759051183595802
precision 0.8846975085819582
f1 0.8267371141095023
===== RES ====
p 1068
tp 656
fp 28
recall 0.6142322091627038
precision 0.9590643260832393
f1 0.7488579706535415
===== RES ====
p 1069
tp 450
fp 7
recall 0.42095416237515976
precision 0.9846827111932545
f1 0.5897767749290561
===== RES ====
p 1068
tp 296
fp 4
recall 0.27715355779292733
precision 0.9866666633777778
f1 0.43274819496624284
===== RES ====
p 1068
tp 146
fp 1
recall 0.13670411972218713
precision 0.9931972721551207
f1 0.24032900501229584
===== RES ====
p 1066
tp 62
fp 1
recall 0.0581613507897173
precision 0.9841269685059212
f1 0.10983160390739595
===== RES ====
p 1068
tp 26
fp 1
recall 0.02434456926559497
precision 0.9629629272976693
f1 0.047488536289115264
===== RES ====
p 1067
tp 16
fp 1
recall 0.014995313950332413
precision 0.9411764152249167
f1 0.029520264275094892
===== RES ====
p 1069
tp 13
fp 1
recall 0.012160898024171283
precision 0.9285713622449028
f1 0.024007361324072236
===== RES ====
p 1068
tp 11
fp 0
recall 0.010299625458520948
precision 0.9999999090909174
f1 0.020389229085748726
===== RES ====
p 1067
tp 11
fp 0
recall 0.010309278340853534
precision 0.9999999090909174
f1 0.020408143027544626
===== RES ====
p 1069
tp 14
fp 1
recall 0.013096351718338305
precision 0.9333332711111153
f1 0.025830230962636532
===== RES ====
p 1065
tp 22
fp 1
recall 0.020657276975908658
precision 0.9565216975425348
f1 0.040441135010651066
===== RES ====
p 1067
tp 54
fp 1
recall 0.050609184582371895
precision 0.9818181639669425
f1 0.09625659108711514
===== RES ====
p 1070
tp 59
fp 0
recall 0.055140186864354956
precision 0.9999999830508478
f1 0.10451717268165533
=== Result of InvGAN+KD: ===
0.10451717268165533
The source-target datasets are: ia_ds with seed 42
The F1 score is: 0.10451717268165533
The training time is: 325.1016855239868
The inference time is: 0.0002736970782279968
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: b2
tgt: fz
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 18
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 18
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 18
tp 12
fp 0
recall 0.6666666296296316
precision 0.9999999166666736
f1 0.7999994666669555
tgt_res:
===== RES ====
p 62
tp 36
fp 1
recall 0.5806451519250783
precision 0.9729729466764069
f1 0.727272244465151
save pretrained model to: checkpoint/b2/bert/42/source-encoder.ptfzbest
save pretrained model to: checkpoint/b2/bert/42/source-classifier.ptfzbest
===== RES ====
p 19
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 21
tp 21
fp 89
recall 0.9999999523809546
precision 0.19090908917355373
f1 0.3206104129132195
Restore model from: /home/derossi/DADER/main/checkpoint/b2/bert/42/source-encoder.ptfzbest
Restore model from: /home/derossi/DADER/main/checkpoint/b2/bert/42/source-classifier.ptfzbest
Pretraining time:  14.265705585479736
=== Training F' and A ===
===== RES ====
p 18
tp 11
fp 0
recall 0.6111110771604957
precision 0.9999999090909174
f1 0.7586201664687818
======== tgt result =======
===== RES ====
p 63
tp 23
fp 2
recall 0.3650793592844546
precision 0.9199999632000014
f1 0.5227268540808911
===== RES ====
p 19
tp 15
fp 0
recall 0.7894736426592819
precision 0.9999999333333378
f1 0.8823523961940494
======== tgt result =======
===== RES ====
p 59
tp 38
fp 2
recall 0.6440677856937663
precision 0.9499999762500007
f1 0.7676762705849359
===== RES ====
p 19
tp 10
fp 0
recall 0.5263157617728547
precision 0.99999990000001
f1 0.6896546730086162
===== RES ====
p 17
tp 7
fp 0
recall 0.4117646816609011
precision 0.9999998571428775
f1 0.58333287152806
===== RES ====
p 19
tp 1
fp 0
recall 0.05263157617728546
precision 0.9999990000010001
f1 0.09999989500001026
===== RES ====
p 18
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 18
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 16
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 18
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 19
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 17
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 19
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 18
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 19
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 20
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 20
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 19
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 18
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 20
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 18
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 22
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
=== Result of InvGAN+KD: ===
0.0
The source-target datasets are: b2_fz with seed 42
The F1 score is: 0.0
The training time is: 64.99545478820801
The inference time is: 0.0002690330147743225
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
=== Argument Setting ===
src: b2
tgt: dzy
seed: 42
train_seed: 42
model_type: bert
max_seq_length: 256
batch_size: 32
pre_epochs: 5
num_epochs: 20
AD weight: 1.0
KD weight: 5.0
temperature: 20
=== Processing datasets ===
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
=== Training F and M ===
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 36
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 37
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 36
fp 4
recall 0.9473683961218844
precision 0.8999999775000006
f1 0.9230763997372864
tgt_res:
===== RES ====
p 128
tp 120
fp 5
recall 0.9374999926757813
precision 0.9599999923200001
f1 0.9486160933621345
save pretrained model to: checkpoint/b2/bert/42/source-encoder.ptdzybest
save pretrained model to: checkpoint/b2/bert/42/source-classifier.ptdzybest
===== RES ====
p 40
tp 39
fp 78
recall 0.9749999756250006
precision 0.3333333304843305
f1 0.49681490056420463
Restore model from: /home/derossi/DADER/main/checkpoint/b2/bert/42/source-encoder.ptdzybest
Restore model from: /home/derossi/DADER/main/checkpoint/b2/bert/42/source-classifier.ptdzybest
Pretraining time:  11.582116842269897
=== Training F' and A ===
===== RES ====
p 39
tp 36
fp 5
recall 0.9230768994082847
precision 0.8780487590719815
f1 0.8999994778127781
======== tgt result =======
===== RES ====
p 121
tp 113
fp 7
recall 0.9338842898026092
precision 0.9416666588194446
f1 0.9377588283262253
===== RES ====
p 38
tp 32
fp 1
recall 0.8421052409972306
precision 0.969696940312214
f1 0.90140792779238
======== tgt result =======
===== RES ====
p 123
tp 113
fp 0
recall 0.9186991795227709
precision 0.9999999911504426
f1 0.9576266114265745
===== RES ====
p 41
tp 31
fp 1
recall 0.7560975425342064
precision 0.9687499697265636
f1 0.8493145528244554
===== RES ====
p 40
tp 8
fp 0
recall 0.19999999500000012
precision 0.9999998750000157
f1 0.33333304166688027
===== RES ====
p 39
tp 2
fp 0
recall 0.05128204996712693
precision 0.99999950000025
f1 0.09756087804882928
===== RES ====
p 35
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 36
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 37
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 37
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 34
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 40
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 36
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 35
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 39
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 40
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 39
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 38
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 37
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
===== RES ====
p 43
tp 0
fp 0
recall 0.0
precision 0.0
f1 0.0
=== Result of InvGAN+KD: ===
0.0
The source-target datasets are: b2_dzy with seed 42
The F1 score is: 0.0
The training time is: 64.90069723129272
The inference time is: 0.0002638474106788635
Source Encoder trainable parameters: 0
Source Encoder non-trainable parameters: 109482240
Target Encoder trainable parameters: 109482240
Target Encoder non-trainable parameters: 0
Source Classifier trainable parameters: 0
Source Classifier non-trainable parameters: 1538
Discriminator trainable parameters: 11805697
Discriminator non-trainable parameters: 0
