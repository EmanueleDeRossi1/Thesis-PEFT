program: train.py
project: finetune_task_divergence
method: random
metric: 
    name: target_validation/f1
    goal: maximize


parameters:
    learning_rate:
        min: 0.00001
        max: 0.00005
    weight_decay:
        min: 0.01
        max: 0.1
    n_epochs:
        values: [15, 20, 25]
    batch_size:
        values: [16, 32]


            # lora_alpha:
            #     values: [8, 16, 32, 64]
            # lora_r:
            #     values: [4, 8, 16, 32, 64]
            # lora_dropout:
            #     values: [0.1, 0.2, 0.3, 0.4, 0.5]
            # learning_rate:
            #     min: 1e-5
            #     max: 5e-5
            # weight_decay:
            #     min: 0.01
            #     max: 0.1
            # n_epochs:
            #     values: [15, 20, 25]
            # batch_size:
            #     values: [16, 32]
