model_name: "finetune_task_divergence"

common_parameters:
    dataset_dir: "./data"
    # maybe you should use roberta instead of bert?
    pretrained_model_name: "bert-base-uncased"
    padding: "max_length"
    max_seq_length: 256
    accelerator: "gpu"
    devices: 1
    num_classes: 2
    random_seed: 42
    source_folder: None
    target_folder: None
    num_instances: 100 # Change to None to train on all source data

models:
    finetune_task_divergence:
        default_parameters:
            learning_rate: 3e-5
            weight_decay: 1e-2
            n_epochs: 10
            batch_size: 32
    lora:
        default_parameters:
            lora_alpha: 32
            lora_r: 8
            lora_dropout: 0.1
            learning_rate: 3e-5
            weight_decay: 1e-2
            n_epochs: 10
            batch_size: 32


# AND REMEMBER THAT YOU SHOULD ALWAYS TRAIN ON THE SAME GPU, CHNAGE .sh FILE BEFORE CALCULATING INF & TRAINING TIME